{
 "cells": [
  {
   "source": [
    "## 读入分析数据"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T09:54:03.520380Z",
     "start_time": "2020-11-02T09:54:03.516392Z"
    }
   },
   "outputs": [],
   "source": [
    "# numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# sklearn preprocessing for dealing with categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, StandardScaler\n",
    "from sklearn.impute import SimpleImputer as Imputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# File system manangement\n",
    "import os\n",
    "\n",
    "import joblib\n",
    "import requests\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T09:54:37.378241Z",
     "start_time": "2020-11-02T09:54:37.370262Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['application_test.csv', 'application_train.csv', 'bureau.csv', 'bureau_balance.csv', 'credit_card_balance.csv', 'HomeCredit_columns_description.csv', 'installments_payments.csv', 'POS_CASH_balance.csv', 'previous_application.csv', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "# List files available\n",
    "print(os.listdir(\"./input/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T09:55:00.329493Z",
     "start_time": "2020-11-02T09:54:56.606423Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data shape:  (307511, 122)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   SK_ID_CURR  TARGET NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR  \\\n",
       "0      100002       1         Cash loans           M            N   \n",
       "1      100003       0         Cash loans           F            N   \n",
       "2      100004       0    Revolving loans           M            Y   \n",
       "3      100006       0         Cash loans           F            N   \n",
       "4      100007       0         Cash loans           M            N   \n",
       "\n",
       "  FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
       "0               Y             0          202500.0    406597.5      24700.5   \n",
       "1               N             0          270000.0   1293502.5      35698.5   \n",
       "2               Y             0           67500.0    135000.0       6750.0   \n",
       "3               Y             0          135000.0    312682.5      29686.5   \n",
       "4               Y             0          121500.0    513000.0      21865.5   \n",
       "\n",
       "   ...  FLAG_DOCUMENT_18 FLAG_DOCUMENT_19 FLAG_DOCUMENT_20 FLAG_DOCUMENT_21  \\\n",
       "0  ...                 0                0                0                0   \n",
       "1  ...                 0                0                0                0   \n",
       "2  ...                 0                0                0                0   \n",
       "3  ...                 0                0                0                0   \n",
       "4  ...                 0                0                0                0   \n",
       "\n",
       "  AMT_REQ_CREDIT_BUREAU_HOUR AMT_REQ_CREDIT_BUREAU_DAY  \\\n",
       "0                        0.0                       0.0   \n",
       "1                        0.0                       0.0   \n",
       "2                        0.0                       0.0   \n",
       "3                        NaN                       NaN   \n",
       "4                        0.0                       0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_WEEK  AMT_REQ_CREDIT_BUREAU_MON  \\\n",
       "0                         0.0                        0.0   \n",
       "1                         0.0                        0.0   \n",
       "2                         0.0                        0.0   \n",
       "3                         NaN                        NaN   \n",
       "4                         0.0                        0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_QRT  AMT_REQ_CREDIT_BUREAU_YEAR  \n",
       "0                        0.0                         1.0  \n",
       "1                        0.0                         0.0  \n",
       "2                        0.0                         0.0  \n",
       "3                        NaN                         NaN  \n",
       "4                        0.0                         0.0  \n",
       "\n",
       "[5 rows x 122 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SK_ID_CURR</th>\n      <th>TARGET</th>\n      <th>NAME_CONTRACT_TYPE</th>\n      <th>CODE_GENDER</th>\n      <th>FLAG_OWN_CAR</th>\n      <th>FLAG_OWN_REALTY</th>\n      <th>CNT_CHILDREN</th>\n      <th>AMT_INCOME_TOTAL</th>\n      <th>AMT_CREDIT</th>\n      <th>AMT_ANNUITY</th>\n      <th>...</th>\n      <th>FLAG_DOCUMENT_18</th>\n      <th>FLAG_DOCUMENT_19</th>\n      <th>FLAG_DOCUMENT_20</th>\n      <th>FLAG_DOCUMENT_21</th>\n      <th>AMT_REQ_CREDIT_BUREAU_HOUR</th>\n      <th>AMT_REQ_CREDIT_BUREAU_DAY</th>\n      <th>AMT_REQ_CREDIT_BUREAU_WEEK</th>\n      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n      <th>AMT_REQ_CREDIT_BUREAU_YEAR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100002</td>\n      <td>1</td>\n      <td>Cash loans</td>\n      <td>M</td>\n      <td>N</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>202500.0</td>\n      <td>406597.5</td>\n      <td>24700.5</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>100003</td>\n      <td>0</td>\n      <td>Cash loans</td>\n      <td>F</td>\n      <td>N</td>\n      <td>N</td>\n      <td>0</td>\n      <td>270000.0</td>\n      <td>1293502.5</td>\n      <td>35698.5</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>100004</td>\n      <td>0</td>\n      <td>Revolving loans</td>\n      <td>M</td>\n      <td>Y</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>67500.0</td>\n      <td>135000.0</td>\n      <td>6750.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>100006</td>\n      <td>0</td>\n      <td>Cash loans</td>\n      <td>F</td>\n      <td>N</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>135000.0</td>\n      <td>312682.5</td>\n      <td>29686.5</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>100007</td>\n      <td>0</td>\n      <td>Cash loans</td>\n      <td>M</td>\n      <td>N</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>121500.0</td>\n      <td>513000.0</td>\n      <td>21865.5</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 122 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Training data\n",
    "app_train = pd.read_csv('./input/application_train.csv')\n",
    "app_test = pd.read_csv('./input/application_test.csv')\n",
    "bureau = pd.read_csv('./input/bureau.csv')\n",
    "bb = pd.read_csv('./input/bureau_balance.csv')\n",
    "prev = pd.read_csv('./input/previous_application.csv')\n",
    "cc = pd.read_csv('./input/credit_card_balance.csv')\n",
    "ins = pd.read_csv('./input/installments_payments.csv')\n",
    "pos = pd.read_csv('./input/POS_CASH_balance.csv')\n",
    "\n",
    "\n",
    "dic={}\n",
    "dic['bur'] = bureau\n",
    "dic['bb'] = bb\n",
    "dic['pre'] = prev\n",
    "dic['cc'] = cc\n",
    "dic['ins'] = ins\n",
    "dic['pos'] = pos\n",
    "\n",
    "print('Training data shape: ', app_train.shape)\n",
    "app_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Features shape:  (307511, 121)\nTesting Features shape:  (48744, 121)\n"
     ]
    }
   ],
   "source": [
    "train_labels = app_train['TARGET']\n",
    "\n",
    "app_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n",
    "print('Training Features shape: ', app_train.shape)\n",
    "print('Testing Features shape: ', app_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# numeric_columns = []\n",
    "# category_columns = []\n",
    "\n",
    "# for col in app_train:\n",
    "\n",
    "#     if app_train[col].isna().sum() > 0.7 * app_train.shape[0]:\n",
    "#         continue\n",
    "\n",
    "#     if app_train[col].dtype == ('object') or app_train[col].dtype == ('bool'):\n",
    "#         category_columns.append(col)\n",
    "#     else:\n",
    "#         numeric_columns.append(col)\n",
    "\n",
    "# print(\"Number of categorical feature:\", len(category_columns))\n",
    "# print(\"Number of numerical feature:\", len(numeric_columns))\n",
    "# print(\"Number of missing feature:\", app_train.shape[1] - len(category_columns) - len(numeric_columns))"
   ]
  },
  {
   "source": [
    "## 预处理"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if ((df[col].dtype == 'object') or (df[col].dtype == 'bool'))]\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    " \n",
    "class appTrainTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, X, y=None):\n",
    "        # DAYS_EMPLOYED_anom\n",
    "        X['DAYS_EMPLOYED_anom'] = (X['DAYS_EMPLOYED'] == 365243)\n",
    "        X['DAYS_EMPLOYED'].replace(365243, np.nan, inplace = True)\n",
    "\n",
    "        # Some simple new features (percentages)\n",
    "        X['DAYS_EMPLOYED_PERC'] = X['DAYS_EMPLOYED'] / X['DAYS_BIRTH']\n",
    "        X['INCOME_CREDIT_PERC'] = X['AMT_INCOME_TOTAL'] / X['AMT_CREDIT']\n",
    "        X['INCOME_PER_PERSON'] = X['AMT_INCOME_TOTAL'] / X['CNT_FAM_MEMBERS']\n",
    "        X['ANNUITY_INCOME_PERC'] = X['AMT_ANNUITY'] / X['AMT_INCOME_TOTAL']\n",
    "        X['PAYMENT_RATE'] = X['AMT_ANNUITY'] / X['AMT_CREDIT']\n",
    "        X.replace(np.inf, np.nan, inplace = True)\n",
    "\n",
    "        return X\n",
    "\n",
    "class dropTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "        self.drop = []\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        drop = []\n",
    "        for col in X:\n",
    "            if X[col].isna().sum() > self.rate * X.shape[0]:\n",
    "                drop.append(col)\n",
    "        self.drop = drop\n",
    "        return self\n",
    " \n",
    "    def transform(self, X_copy, y=None):\n",
    "        X_copy.drop(self.drop, axis = 1, inplace = True)\n",
    "        return X_copy\n",
    "\n",
    "class bureauTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, bureauinfo, bbinfo, y=None):\n",
    "        # app_train treatment\n",
    "        bb_copy, bb_cat = one_hot_encoder(bbinfo, True)\n",
    "        bureau_copy, bureau_cat = one_hot_encoder(bureauinfo, True)\n",
    "\n",
    "        # bb Treatment\n",
    "        bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
    "        for col in bb_cat:\n",
    "            bb_aggregations[col] = ['mean']\n",
    "        bb_agg = bb_copy.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "        bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "        bureau_copy = bureau_copy.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "        bureau_copy.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n",
    "\n",
    "        ## bureau Treatment\n",
    "        # Bureau and bureau_balance numeric features\n",
    "        num_aggregations = {\n",
    "            'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
    "            'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "            'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "            'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "            'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "            'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "            'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "            'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "            'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "            'AMT_ANNUITY': ['max', 'mean'],\n",
    "            'CNT_CREDIT_PROLONG': ['sum'],\n",
    "            'MONTHS_BALANCE_MIN': ['min'],\n",
    "            'MONTHS_BALANCE_MAX': ['max'],\n",
    "            'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
    "        }\n",
    "\n",
    "        # Bureau and bureau_balance categorical features\n",
    "        cat_aggregations = {}\n",
    "        for cat in bureau_cat: \n",
    "            cat_aggregations[cat] = ['mean']\n",
    "        for cat in bb_cat: \n",
    "            cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "\n",
    "        bureau_agg = bureau_copy.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "        bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "\n",
    "        # Bureau: Active credits - using only numerical aggregations\n",
    "        active = bureau_copy[bureau_copy['CREDIT_ACTIVE_Active'] == 1]\n",
    "        active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "        active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "        bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "\n",
    "        # Bureau: Closed credits - using only numerical aggregations\n",
    "        closed = bureau_copy[bureau_copy['CREDIT_ACTIVE_Closed'] == 1]\n",
    "        closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "        closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "        bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "        return bureau_agg\n",
    "\n",
    "class previousTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, previnfo, y=None):\n",
    "\n",
    "        prev, cat_cols = one_hot_encoder(previnfo, nan_as_category= True)\n",
    "        # Days 365.243 values -> nan\n",
    "        prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "        prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "        prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "        prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "        prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "\n",
    "        # Add feature: value ask / value received percentage\n",
    "        prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "        # Previous applications numeric features\n",
    "        num_aggregations = {\n",
    "            'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "            'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
    "            'AMT_CREDIT': ['min', 'max', 'mean'],\n",
    "            'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "            'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "            'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "            'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "            'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "            'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "            'CNT_PAYMENT': ['mean', 'sum'],\n",
    "        }\n",
    "        # Previous applications categorical features\n",
    "        cat_aggregations = {}\n",
    "        for cat in cat_cols:\n",
    "            cat_aggregations[cat] = ['mean']\n",
    "        \n",
    "        prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "        prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "        # Previous Applications: Approved Applications - only numerical features\n",
    "        approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "        approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "        approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "        prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "        # Previous Applications: Refused Applications - only numerical features\n",
    "        refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "        refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "        refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "        prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "        return prev_agg\n",
    "\n",
    "class posTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, posinfo, y=None):\n",
    "\n",
    "        pos, cat_cols = one_hot_encoder(posinfo, True)\n",
    "        # Features\n",
    "        aggregations = {\n",
    "            'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
    "            'SK_DPD': ['max', 'mean'],\n",
    "            'SK_DPD_DEF': ['max', 'mean']\n",
    "        }\n",
    "        for cat in cat_cols:\n",
    "            aggregations[cat] = ['mean']\n",
    "        \n",
    "        pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "        pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "        # Count pos cash accounts\n",
    "        pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
    "\n",
    "        return pos_agg\n",
    "    \n",
    "class installmentsTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, insinfo, y=None):\n",
    "        ins, cat_cols = one_hot_encoder(insinfo, True)\n",
    "\n",
    "        # Percentage and difference paid in each installment (amount paid and installment value)\n",
    "        ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "        ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "        # Days past due and days before due (no negative values)\n",
    "        ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "        ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "        ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "        ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "        # Features: Perform aggregations\n",
    "        aggregations = {\n",
    "            'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "            'DPD': ['max', 'mean', 'sum'],\n",
    "            'DBD': ['max', 'mean', 'sum'],\n",
    "            'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "            'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "            'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
    "            'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "            'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n",
    "        }\n",
    "        for cat in cat_cols:\n",
    "            aggregations[cat] = ['mean']\n",
    "        ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "        ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "        # Count installments accounts\n",
    "        ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "        return ins_agg\n",
    "\n",
    "class ccTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, ccinfo, y=None):\n",
    "        cc, cat_cols = one_hot_encoder(ccinfo, True)\n",
    "        # General aggregations\n",
    "        cc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n",
    "        cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "        cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "        # Count credit card lines\n",
    "        cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "        return cc_agg\n",
    "\n",
    "class joinTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, dic):\n",
    "        self.dic = dic\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, X, y=None):\n",
    "        bur = bureauTransformer()\n",
    "        pre = previousTransformer()\n",
    "        pos = posTransformer()\n",
    "        cc = ccTransformer()\n",
    "        ins = installmentsTransformer()\n",
    "        app = appTrainTransformer()\n",
    "\n",
    "        X = X.join(bur.transform(self.dic['bur'], self.dic['bb']), how='left', on='SK_ID_CURR')\n",
    "        X = X.join(pre.transform(self.dic['pre']), how='left', on='SK_ID_CURR')\n",
    "        X = X.join(pos.transform(self.dic['pos']), how='left', on='SK_ID_CURR')\n",
    "        X = X.join(cc.transform(self.dic['cc']), how='left', on='SK_ID_CURR')\n",
    "        X = X.join(ins.transform(self.dic['ins']), how='left', on='SK_ID_CURR')\n",
    "        X = app.transform(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of categorical feature: 17\nNumber of numerical feature: 484\n"
     ]
    }
   ],
   "source": [
    "j = joinTransformer(dic)\n",
    "g = dropTransformer(0.7)\n",
    "\n",
    "X = app_train.copy()\n",
    "X = j.fit_transform(X)\n",
    "X = g.fit_transform(X)\n",
    "\n",
    "\n",
    "numeric_columns = []\n",
    "category_columns = []\n",
    "\n",
    "for col in X:\n",
    "    \n",
    "    if X[col].dtype == ('object') or X[col].dtype == ('bool'):\n",
    "        category_columns.append(col)\n",
    "    else:\n",
    "        numeric_columns.append(col)\n",
    "\n",
    "print(\"Number of categorical feature:\", len(category_columns))\n",
    "print(\"Number of numerical feature:\", len(numeric_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('impute_nan', Imputer(missing_values=np.nan,strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "category_transformer = Pipeline(steps=[\n",
    "    ('impute', Imputer(strategy='constant', fill_value='missing')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocesser = ColumnTransformer(transformers=[\n",
    "    ('numeric', numeric_transformer, numeric_columns),\n",
    "    ('category', category_transformer, category_columns)\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_pre = Pipeline(steps=[\n",
    "    ('join', joinTransformer(dic)),\n",
    "    ('drop', dropTransformer(0.7)),\n",
    "    ('preprocesser', preprocesser)\n",
    "])"
   ]
  },
  {
   "source": [
    "## 建造Pipeline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "class kfoldlgb(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.clf = []\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        kf = KFold(n_splits=self.k)\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            gbm = lgb.LGBMClassifier(\n",
    "                nthread=4,\n",
    "                n_estimators=10000,\n",
    "                learning_rate=0.02,\n",
    "                num_leaves=34,\n",
    "                colsample_bytree=0.9497036,\n",
    "                subsample=0.8715623,\n",
    "                max_depth=8,\n",
    "                reg_alpha=0.041545473,\n",
    "                reg_lambda=0.0735294,\n",
    "                min_split_gain=0.0222415,\n",
    "                min_child_weight=39.3259775,\n",
    "                silent=-1,\n",
    "                verbose=-1, )\n",
    "            gbm.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], eval_metric= 'auc', verbose= 50, early_stopping_rounds= 200)\n",
    "            self.clf.append(gbm)\n",
    "        return self\n",
    " \n",
    "    def predict_proba(self, X, y=None):\n",
    "        for i in range(self.k):\n",
    "            tmp = self.clf[i].predict_proba(X)\n",
    "            if i!=0:\n",
    "                result = result + tmp/self.k\n",
    "            else:\n",
    "                result = tmp/self.k\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_pre_train = pipeline_pre.fit_transform(app_train)\n",
    "app_pre_test = pipeline_pre.transform(app_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "868015\ttraining's binary_logloss: 0.205722\tvalid_1's auc: 0.786795\tvalid_1's binary_logloss: 0.240044\n",
      "[1350]\ttraining's auc: 0.870254\ttraining's binary_logloss: 0.204721\tvalid_1's auc: 0.786841\tvalid_1's binary_logloss: 0.24002\n",
      "[1400]\ttraining's auc: 0.872241\ttraining's binary_logloss: 0.203827\tvalid_1's auc: 0.786776\tvalid_1's binary_logloss: 0.240019\n",
      "[1450]\ttraining's auc: 0.874323\ttraining's binary_logloss: 0.202876\tvalid_1's auc: 0.78682\tvalid_1's binary_logloss: 0.240009\n",
      "Early stopping, best iteration is:\n",
      "[1293]\ttraining's auc: 0.867774\ttraining's binary_logloss: 0.205835\tvalid_1's auc: 0.786864\tvalid_1's binary_logloss: 0.240025\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's auc: 0.75184\ttraining's binary_logloss: 0.255076\tvalid_1's auc: 0.730084\tvalid_1's binary_logloss: 0.257219\n",
      "[100]\ttraining's auc: 0.771247\ttraining's binary_logloss: 0.246017\tvalid_1's auc: 0.745528\tvalid_1's binary_logloss: 0.250293\n",
      "[150]\ttraining's auc: 0.784564\ttraining's binary_logloss: 0.240503\tvalid_1's auc: 0.755205\tvalid_1's binary_logloss: 0.246602\n",
      "[200]\ttraining's auc: 0.793973\ttraining's binary_logloss: 0.236657\tvalid_1's auc: 0.761366\tvalid_1's binary_logloss: 0.24448\n",
      "[250]\ttraining's auc: 0.801137\ttraining's binary_logloss: 0.233674\tvalid_1's auc: 0.765249\tvalid_1's binary_logloss: 0.243095\n",
      "[300]\ttraining's auc: 0.806965\ttraining's binary_logloss: 0.231247\tvalid_1's auc: 0.767987\tvalid_1's binary_logloss: 0.242144\n",
      "[350]\ttraining's auc: 0.812071\ttraining's binary_logloss: 0.229118\tvalid_1's auc: 0.770037\tvalid_1's binary_logloss: 0.241412\n",
      "[400]\ttraining's auc: 0.816348\ttraining's binary_logloss: 0.227338\tvalid_1's auc: 0.771784\tvalid_1's binary_logloss: 0.240857\n",
      "[450]\ttraining's auc: 0.820258\ttraining's binary_logloss: 0.225773\tvalid_1's auc: 0.773002\tvalid_1's binary_logloss: 0.240461\n",
      "[500]\ttraining's auc: 0.823962\ttraining's binary_logloss: 0.224301\tvalid_1's auc: 0.774387\tvalid_1's binary_logloss: 0.240028\n",
      "[550]\ttraining's auc: 0.827685\ttraining's binary_logloss: 0.222828\tvalid_1's auc: 0.775286\tvalid_1's binary_logloss: 0.239733\n",
      "[600]\ttraining's auc: 0.831206\ttraining's binary_logloss: 0.221436\tvalid_1's auc: 0.77608\tvalid_1's binary_logloss: 0.239484\n",
      "[650]\ttraining's auc: 0.834462\ttraining's binary_logloss: 0.220152\tvalid_1's auc: 0.776415\tvalid_1's binary_logloss: 0.239342\n",
      "[700]\ttraining's auc: 0.837742\ttraining's binary_logloss: 0.218838\tvalid_1's auc: 0.776918\tvalid_1's binary_logloss: 0.239172\n",
      "[750]\ttraining's auc: 0.840697\ttraining's binary_logloss: 0.217657\tvalid_1's auc: 0.777216\tvalid_1's binary_logloss: 0.239026\n",
      "[800]\ttraining's auc: 0.843574\ttraining's binary_logloss: 0.216488\tvalid_1's auc: 0.777459\tvalid_1's binary_logloss: 0.238935\n",
      "[850]\ttraining's auc: 0.846464\ttraining's binary_logloss: 0.215338\tvalid_1's auc: 0.77768\tvalid_1's binary_logloss: 0.238854\n",
      "[900]\ttraining's auc: 0.849303\ttraining's binary_logloss: 0.214181\tvalid_1's auc: 0.777671\tvalid_1's binary_logloss: 0.238815\n",
      "[950]\ttraining's auc: 0.851873\ttraining's binary_logloss: 0.213122\tvalid_1's auc: 0.77777\tvalid_1's binary_logloss: 0.238765\n",
      "[1000]\ttraining's auc: 0.854302\ttraining's binary_logloss: 0.212099\tvalid_1's auc: 0.777769\tvalid_1's binary_logloss: 0.238747\n",
      "[1050]\ttraining's auc: 0.85679\ttraining's binary_logloss: 0.21105\tvalid_1's auc: 0.777953\tvalid_1's binary_logloss: 0.238668\n",
      "[1100]\ttraining's auc: 0.859203\ttraining's binary_logloss: 0.210034\tvalid_1's auc: 0.777996\tvalid_1's binary_logloss: 0.238656\n",
      "[1150]\ttraining's auc: 0.861632\ttraining's binary_logloss: 0.208986\tvalid_1's auc: 0.777888\tvalid_1's binary_logloss: 0.238693\n",
      "[1200]\ttraining's auc: 0.863988\ttraining's binary_logloss: 0.207977\tvalid_1's auc: 0.777888\tvalid_1's binary_logloss: 0.238708\n",
      "[1250]\ttraining's auc: 0.86626\ttraining's binary_logloss: 0.206991\tvalid_1's auc: 0.778037\tvalid_1's binary_logloss: 0.238646\n",
      "Early stopping, best iteration is:\n",
      "[1079]\ttraining's auc: 0.858177\ttraining's binary_logloss: 0.210465\tvalid_1's auc: 0.778098\tvalid_1's binary_logloss: 0.238625\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's auc: 0.75131\ttraining's binary_logloss: 0.255095\tvalid_1's auc: 0.739523\tvalid_1's binary_logloss: 0.25771\n",
      "[100]\ttraining's auc: 0.77058\ttraining's binary_logloss: 0.246058\tvalid_1's auc: 0.75489\tvalid_1's binary_logloss: 0.250161\n",
      "[150]\ttraining's auc: 0.783784\ttraining's binary_logloss: 0.240554\tvalid_1's auc: 0.764655\tvalid_1's binary_logloss: 0.246156\n",
      "[200]\ttraining's auc: 0.793094\ttraining's binary_logloss: 0.236745\tvalid_1's auc: 0.77075\tvalid_1's binary_logloss: 0.243705\n",
      "[250]\ttraining's auc: 0.800188\ttraining's binary_logloss: 0.233775\tvalid_1's auc: 0.77503\tvalid_1's binary_logloss: 0.241979\n",
      "[300]\ttraining's auc: 0.806172\ttraining's binary_logloss: 0.231318\tvalid_1's auc: 0.777722\tvalid_1's binary_logloss: 0.240837\n",
      "[350]\ttraining's auc: 0.811147\ttraining's binary_logloss: 0.229254\tvalid_1's auc: 0.779713\tvalid_1's binary_logloss: 0.24001\n",
      "[400]\ttraining's auc: 0.815568\ttraining's binary_logloss: 0.227439\tvalid_1's auc: 0.78127\tvalid_1's binary_logloss: 0.239414\n",
      "[450]\ttraining's auc: 0.81965\ttraining's binary_logloss: 0.225785\tvalid_1's auc: 0.782566\tvalid_1's binary_logloss: 0.238897\n",
      "[500]\ttraining's auc: 0.823459\ttraining's binary_logloss: 0.224262\tvalid_1's auc: 0.78353\tvalid_1's binary_logloss: 0.238494\n",
      "[550]\ttraining's auc: 0.827128\ttraining's binary_logloss: 0.222824\tvalid_1's auc: 0.78474\tvalid_1's binary_logloss: 0.238078\n",
      "[600]\ttraining's auc: 0.830626\ttraining's binary_logloss: 0.221452\tvalid_1's auc: 0.785535\tvalid_1's binary_logloss: 0.237787\n",
      "[650]\ttraining's auc: 0.833721\ttraining's binary_logloss: 0.220215\tvalid_1's auc: 0.786236\tvalid_1's binary_logloss: 0.237538\n",
      "[700]\ttraining's auc: 0.836844\ttraining's binary_logloss: 0.218985\tvalid_1's auc: 0.786752\tvalid_1's binary_logloss: 0.237325\n",
      "[750]\ttraining's auc: 0.839703\ttraining's binary_logloss: 0.217827\tvalid_1's auc: 0.787197\tvalid_1's binary_logloss: 0.237143\n",
      "[800]\ttraining's auc: 0.842784\ttraining's binary_logloss: 0.216621\tvalid_1's auc: 0.787673\tvalid_1's binary_logloss: 0.236978\n",
      "[850]\ttraining's auc: 0.845632\ttraining's binary_logloss: 0.215464\tvalid_1's auc: 0.788047\tvalid_1's binary_logloss: 0.236855\n",
      "[900]\ttraining's auc: 0.84848\ttraining's binary_logloss: 0.214302\tvalid_1's auc: 0.788364\tvalid_1's binary_logloss: 0.23672\n",
      "[950]\ttraining's auc: 0.851227\ttraining's binary_logloss: 0.213193\tvalid_1's auc: 0.78873\tvalid_1's binary_logloss: 0.236593\n",
      "[1000]\ttraining's auc: 0.853888\ttraining's binary_logloss: 0.212088\tvalid_1's auc: 0.789193\tvalid_1's binary_logloss: 0.236429\n",
      "[1050]\ttraining's auc: 0.856474\ttraining's binary_logloss: 0.21102\tvalid_1's auc: 0.789378\tvalid_1's binary_logloss: 0.236347\n",
      "[1100]\ttraining's auc: 0.858817\ttraining's binary_logloss: 0.21002\tvalid_1's auc: 0.789549\tvalid_1's binary_logloss: 0.236298\n",
      "[1150]\ttraining's auc: 0.861301\ttraining's binary_logloss: 0.208989\tvalid_1's auc: 0.789907\tvalid_1's binary_logloss: 0.236205\n",
      "[1200]\ttraining's auc: 0.863466\ttraining's binary_logloss: 0.208039\tvalid_1's auc: 0.790088\tvalid_1's binary_logloss: 0.236137\n",
      "[1250]\ttraining's auc: 0.865698\ttraining's binary_logloss: 0.207068\tvalid_1's auc: 0.790168\tvalid_1's binary_logloss: 0.236103\n",
      "[1300]\ttraining's auc: 0.867871\ttraining's binary_logloss: 0.206124\tvalid_1's auc: 0.790402\tvalid_1's binary_logloss: 0.236031\n",
      "[1350]\ttraining's auc: 0.870058\ttraining's binary_logloss: 0.20516\tvalid_1's auc: 0.79057\tvalid_1's binary_logloss: 0.23598\n",
      "[1400]\ttraining's auc: 0.87226\ttraining's binary_logloss: 0.204178\tvalid_1's auc: 0.790664\tvalid_1's binary_logloss: 0.235954\n",
      "[1450]\ttraining's auc: 0.874235\ttraining's binary_logloss: 0.203301\tvalid_1's auc: 0.790656\tvalid_1's binary_logloss: 0.23597\n",
      "[1500]\ttraining's auc: 0.876411\ttraining's binary_logloss: 0.202294\tvalid_1's auc: 0.790799\tvalid_1's binary_logloss: 0.23594\n",
      "[1550]\ttraining's auc: 0.878441\ttraining's binary_logloss: 0.201374\tvalid_1's auc: 0.790736\tvalid_1's binary_logloss: 0.235966\n",
      "[1600]\ttraining's auc: 0.880473\ttraining's binary_logloss: 0.200427\tvalid_1's auc: 0.790644\tvalid_1's binary_logloss: 0.235996\n",
      "[1650]\ttraining's auc: 0.882325\ttraining's binary_logloss: 0.199573\tvalid_1's auc: 0.79058\tvalid_1's binary_logloss: 0.236003\n",
      "Early stopping, best iteration is:\n",
      "[1477]\ttraining's auc: 0.875424\ttraining's binary_logloss: 0.202744\tvalid_1's auc: 0.790813\tvalid_1's binary_logloss: 0.235933\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's auc: 0.750536\ttraining's binary_logloss: 0.255538\tvalid_1's auc: 0.74758\tvalid_1's binary_logloss: 0.253733\n",
      "[100]\ttraining's auc: 0.76994\ttraining's binary_logloss: 0.24651\tvalid_1's auc: 0.76077\tvalid_1's binary_logloss: 0.246412\n",
      "[150]\ttraining's auc: 0.78343\ttraining's binary_logloss: 0.240992\tvalid_1's auc: 0.769376\tvalid_1's binary_logloss: 0.242531\n",
      "[200]\ttraining's auc: 0.792792\ttraining's binary_logloss: 0.237135\tvalid_1's auc: 0.77469\tvalid_1's binary_logloss: 0.240166\n",
      "[250]\ttraining's auc: 0.79985\ttraining's binary_logloss: 0.234159\tvalid_1's auc: 0.778408\tvalid_1's binary_logloss: 0.238571\n",
      "[300]\ttraining's auc: 0.805942\ttraining's binary_logloss: 0.231677\tvalid_1's auc: 0.781108\tvalid_1's binary_logloss: 0.237462\n",
      "[350]\ttraining's auc: 0.810932\ttraining's binary_logloss: 0.229622\tvalid_1's auc: 0.78265\tvalid_1's binary_logloss: 0.23678\n",
      "[400]\ttraining's auc: 0.815345\ttraining's binary_logloss: 0.227805\tvalid_1's auc: 0.78391\tvalid_1's binary_logloss: 0.236243\n",
      "[450]\ttraining's auc: 0.819514\ttraining's binary_logloss: 0.226134\tvalid_1's auc: 0.784859\tvalid_1's binary_logloss: 0.235874\n",
      "[500]\ttraining's auc: 0.823416\ttraining's binary_logloss: 0.224575\tvalid_1's auc: 0.785897\tvalid_1's binary_logloss: 0.235523\n",
      "[550]\ttraining's auc: 0.82718\ttraining's binary_logloss: 0.223087\tvalid_1's auc: 0.786566\tvalid_1's binary_logloss: 0.235288\n",
      "[600]\ttraining's auc: 0.830599\ttraining's binary_logloss: 0.221736\tvalid_1's auc: 0.78729\tvalid_1's binary_logloss: 0.235042\n",
      "[650]\ttraining's auc: 0.833942\ttraining's binary_logloss: 0.220388\tvalid_1's auc: 0.787814\tvalid_1's binary_logloss: 0.234845\n",
      "[700]\ttraining's auc: 0.837033\ttraining's binary_logloss: 0.219147\tvalid_1's auc: 0.788005\tvalid_1's binary_logloss: 0.23477\n",
      "[750]\ttraining's auc: 0.839977\ttraining's binary_logloss: 0.217951\tvalid_1's auc: 0.788185\tvalid_1's binary_logloss: 0.2347\n",
      "[800]\ttraining's auc: 0.842831\ttraining's binary_logloss: 0.216821\tvalid_1's auc: 0.788434\tvalid_1's binary_logloss: 0.234619\n",
      "[850]\ttraining's auc: 0.845582\ttraining's binary_logloss: 0.215723\tvalid_1's auc: 0.788643\tvalid_1's binary_logloss: 0.234554\n",
      "[900]\ttraining's auc: 0.848323\ttraining's binary_logloss: 0.214588\tvalid_1's auc: 0.789015\tvalid_1's binary_logloss: 0.234439\n",
      "[950]\ttraining's auc: 0.850838\ttraining's binary_logloss: 0.21353\tvalid_1's auc: 0.789191\tvalid_1's binary_logloss: 0.234374\n",
      "[1000]\ttraining's auc: 0.853299\ttraining's binary_logloss: 0.212509\tvalid_1's auc: 0.789332\tvalid_1's binary_logloss: 0.234325\n",
      "[1050]\ttraining's auc: 0.855804\ttraining's binary_logloss: 0.211472\tvalid_1's auc: 0.789501\tvalid_1's binary_logloss: 0.234278\n",
      "[1100]\ttraining's auc: 0.85805\ttraining's binary_logloss: 0.21051\tvalid_1's auc: 0.789638\tvalid_1's binary_logloss: 0.234256\n",
      "[1150]\ttraining's auc: 0.860308\ttraining's binary_logloss: 0.209542\tvalid_1's auc: 0.789809\tvalid_1's binary_logloss: 0.234205\n",
      "[1200]\ttraining's auc: 0.862517\ttraining's binary_logloss: 0.20859\tvalid_1's auc: 0.789831\tvalid_1's binary_logloss: 0.234191\n",
      "[1250]\ttraining's auc: 0.864886\ttraining's binary_logloss: 0.207597\tvalid_1's auc: 0.789709\tvalid_1's binary_logloss: 0.234211\n",
      "[1300]\ttraining's auc: 0.867135\ttraining's binary_logloss: 0.206629\tvalid_1's auc: 0.789703\tvalid_1's binary_logloss: 0.2342\n",
      "[1350]\ttraining's auc: 0.869223\ttraining's binary_logloss: 0.205718\tvalid_1's auc: 0.789659\tvalid_1's binary_logloss: 0.234209\n",
      "Early stopping, best iteration is:\n",
      "[1173]\ttraining's auc: 0.861367\ttraining's binary_logloss: 0.209096\tvalid_1's auc: 0.789909\tvalid_1's binary_logloss: 0.23418\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's auc: 0.749746\ttraining's binary_logloss: 0.255626\tvalid_1's auc: 0.749419\tvalid_1's binary_logloss: 0.253846\n",
      "[100]\ttraining's auc: 0.769748\ttraining's binary_logloss: 0.246565\tvalid_1's auc: 0.764165\tvalid_1's binary_logloss: 0.245987\n",
      "[150]\ttraining's auc: 0.783446\ttraining's binary_logloss: 0.241032\tvalid_1's auc: 0.773684\tvalid_1's binary_logloss: 0.241699\n",
      "[200]\ttraining's auc: 0.792814\ttraining's binary_logloss: 0.237198\tvalid_1's auc: 0.778797\tvalid_1's binary_logloss: 0.239245\n",
      "[250]\ttraining's auc: 0.799779\ttraining's binary_logloss: 0.234273\tvalid_1's auc: 0.782109\tvalid_1's binary_logloss: 0.237599\n",
      "[300]\ttraining's auc: 0.805513\ttraining's binary_logloss: 0.231853\tvalid_1's auc: 0.784824\tvalid_1's binary_logloss: 0.236416\n",
      "[350]\ttraining's auc: 0.810546\ttraining's binary_logloss: 0.229775\tvalid_1's auc: 0.786776\tvalid_1's binary_logloss: 0.235522\n",
      "[400]\ttraining's auc: 0.815016\ttraining's binary_logloss: 0.227945\tvalid_1's auc: 0.788321\tvalid_1's binary_logloss: 0.234842\n",
      "[450]\ttraining's auc: 0.819081\ttraining's binary_logloss: 0.226282\tvalid_1's auc: 0.789398\tvalid_1's binary_logloss: 0.234347\n",
      "[500]\ttraining's auc: 0.822949\ttraining's binary_logloss: 0.224734\tvalid_1's auc: 0.79038\tvalid_1's binary_logloss: 0.23394\n",
      "[550]\ttraining's auc: 0.826699\ttraining's binary_logloss: 0.223289\tvalid_1's auc: 0.791253\tvalid_1's binary_logloss: 0.233632\n",
      "[600]\ttraining's auc: 0.829988\ttraining's binary_logloss: 0.221958\tvalid_1's auc: 0.791906\tvalid_1's binary_logloss: 0.23336\n",
      "[650]\ttraining's auc: 0.833393\ttraining's binary_logloss: 0.220624\tvalid_1's auc: 0.792776\tvalid_1's binary_logloss: 0.233048\n",
      "[700]\ttraining's auc: 0.836635\ttraining's binary_logloss: 0.219358\tvalid_1's auc: 0.793276\tvalid_1's binary_logloss: 0.232845\n",
      "[750]\ttraining's auc: 0.839577\ttraining's binary_logloss: 0.218211\tvalid_1's auc: 0.793664\tvalid_1's binary_logloss: 0.232689\n",
      "[800]\ttraining's auc: 0.842269\ttraining's binary_logloss: 0.217143\tvalid_1's auc: 0.793963\tvalid_1's binary_logloss: 0.232563\n",
      "[850]\ttraining's auc: 0.84514\ttraining's binary_logloss: 0.216011\tvalid_1's auc: 0.794234\tvalid_1's binary_logloss: 0.23244\n",
      "[900]\ttraining's auc: 0.847899\ttraining's binary_logloss: 0.214916\tvalid_1's auc: 0.79437\tvalid_1's binary_logloss: 0.232358\n",
      "[950]\ttraining's auc: 0.850444\ttraining's binary_logloss: 0.213881\tvalid_1's auc: 0.794498\tvalid_1's binary_logloss: 0.232281\n",
      "[1000]\ttraining's auc: 0.853071\ttraining's binary_logloss: 0.212815\tvalid_1's auc: 0.794624\tvalid_1's binary_logloss: 0.232203\n",
      "[1050]\ttraining's auc: 0.8554\ttraining's binary_logloss: 0.211842\tvalid_1's auc: 0.79469\tvalid_1's binary_logloss: 0.232123\n",
      "[1100]\ttraining's auc: 0.857769\ttraining's binary_logloss: 0.210837\tvalid_1's auc: 0.794749\tvalid_1's binary_logloss: 0.23206\n",
      "[1150]\ttraining's auc: 0.860139\ttraining's binary_logloss: 0.209829\tvalid_1's auc: 0.79484\tvalid_1's binary_logloss: 0.232015\n",
      "[1200]\ttraining's auc: 0.862536\ttraining's binary_logloss: 0.208809\tvalid_1's auc: 0.794916\tvalid_1's binary_logloss: 0.231974\n",
      "[1250]\ttraining's auc: 0.864716\ttraining's binary_logloss: 0.207847\tvalid_1's auc: 0.795006\tvalid_1's binary_logloss: 0.231915\n",
      "[1300]\ttraining's auc: 0.866946\ttraining's binary_logloss: 0.206878\tvalid_1's auc: 0.795055\tvalid_1's binary_logloss: 0.231887\n",
      "[1350]\ttraining's auc: 0.869223\ttraining's binary_logloss: 0.205893\tvalid_1's auc: 0.795185\tvalid_1's binary_logloss: 0.231844\n",
      "[1400]\ttraining's auc: 0.871204\ttraining's binary_logloss: 0.205007\tvalid_1's auc: 0.795255\tvalid_1's binary_logloss: 0.231812\n",
      "[1450]\ttraining's auc: 0.873305\ttraining's binary_logloss: 0.204067\tvalid_1's auc: 0.795272\tvalid_1's binary_logloss: 0.231809\n",
      "[1500]\ttraining's auc: 0.87547\ttraining's binary_logloss: 0.203128\tvalid_1's auc: 0.795334\tvalid_1's binary_logloss: 0.231769\n",
      "[1550]\ttraining's auc: 0.877676\ttraining's binary_logloss: 0.20215\tvalid_1's auc: 0.795143\tvalid_1's binary_logloss: 0.231805\n",
      "[1600]\ttraining's auc: 0.879583\ttraining's binary_logloss: 0.201262\tvalid_1's auc: 0.795164\tvalid_1's binary_logloss: 0.231787\n",
      "[1650]\ttraining's auc: 0.881579\ttraining's binary_logloss: 0.200328\tvalid_1's auc: 0.795077\tvalid_1's binary_logloss: 0.231801\n",
      "Early stopping, best iteration is:\n",
      "[1471]\ttraining's auc: 0.874171\ttraining's binary_logloss: 0.203688\tvalid_1's auc: 0.795364\tvalid_1's binary_logloss: 0.231775\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's auc: 0.750865\ttraining's binary_logloss: 0.255492\tvalid_1's auc: 0.739023\tvalid_1's binary_logloss: 0.254026\n",
      "[100]\ttraining's auc: 0.770432\ttraining's binary_logloss: 0.24643\tvalid_1's auc: 0.756075\tvalid_1's binary_logloss: 0.246502\n",
      "[150]\ttraining's auc: 0.784001\ttraining's binary_logloss: 0.240899\tvalid_1's auc: 0.767172\tvalid_1's binary_logloss: 0.24237\n",
      "[200]\ttraining's auc: 0.793365\ttraining's binary_logloss: 0.237047\tvalid_1's auc: 0.773285\tvalid_1's binary_logloss: 0.239967\n",
      "[250]\ttraining's auc: 0.800476\ttraining's binary_logloss: 0.234063\tvalid_1's auc: 0.777426\tvalid_1's binary_logloss: 0.238332\n",
      "[300]\ttraining's auc: 0.806249\ttraining's binary_logloss: 0.231658\tvalid_1's auc: 0.780228\tvalid_1's binary_logloss: 0.237264\n",
      "[350]\ttraining's auc: 0.811223\ttraining's binary_logloss: 0.229581\tvalid_1's auc: 0.782324\tvalid_1's binary_logloss: 0.236486\n",
      "[400]\ttraining's auc: 0.815678\ttraining's binary_logloss: 0.227761\tvalid_1's auc: 0.783731\tvalid_1's binary_logloss: 0.235961\n",
      "[450]\ttraining's auc: 0.819861\ttraining's binary_logloss: 0.226077\tvalid_1's auc: 0.785262\tvalid_1's binary_logloss: 0.235449\n",
      "[500]\ttraining's auc: 0.823632\ttraining's binary_logloss: 0.224543\tvalid_1's auc: 0.786426\tvalid_1's binary_logloss: 0.23508\n",
      "[550]\ttraining's auc: 0.827451\ttraining's binary_logloss: 0.223034\tvalid_1's auc: 0.787334\tvalid_1's binary_logloss: 0.234798\n",
      "[600]\ttraining's auc: 0.83091\ttraining's binary_logloss: 0.221687\tvalid_1's auc: 0.78801\tvalid_1's binary_logloss: 0.234559\n",
      "[650]\ttraining's auc: 0.834143\ttraining's binary_logloss: 0.220425\tvalid_1's auc: 0.788467\tvalid_1's binary_logloss: 0.234393\n",
      "[700]\ttraining's auc: 0.837145\ttraining's binary_logloss: 0.219227\tvalid_1's auc: 0.788858\tvalid_1's binary_logloss: 0.234238\n",
      "[750]\ttraining's auc: 0.840318\ttraining's binary_logloss: 0.217973\tvalid_1's auc: 0.789221\tvalid_1's binary_logloss: 0.23408\n",
      "[800]\ttraining's auc: 0.843202\ttraining's binary_logloss: 0.216777\tvalid_1's auc: 0.789685\tvalid_1's binary_logloss: 0.23391\n",
      "[850]\ttraining's auc: 0.845964\ttraining's binary_logloss: 0.215669\tvalid_1's auc: 0.789822\tvalid_1's binary_logloss: 0.233846\n",
      "[900]\ttraining's auc: 0.848496\ttraining's binary_logloss: 0.214628\tvalid_1's auc: 0.790042\tvalid_1's binary_logloss: 0.233769\n",
      "[950]\ttraining's auc: 0.851125\ttraining's binary_logloss: 0.213567\tvalid_1's auc: 0.790344\tvalid_1's binary_logloss: 0.233672\n",
      "[1000]\ttraining's auc: 0.853602\ttraining's binary_logloss: 0.212537\tvalid_1's auc: 0.7905\tvalid_1's binary_logloss: 0.23359\n",
      "[1050]\ttraining's auc: 0.856215\ttraining's binary_logloss: 0.211486\tvalid_1's auc: 0.790559\tvalid_1's binary_logloss: 0.233553\n",
      "[1100]\ttraining's auc: 0.858619\ttraining's binary_logloss: 0.210488\tvalid_1's auc: 0.790816\tvalid_1's binary_logloss: 0.233473\n",
      "[1150]\ttraining's auc: 0.860866\ttraining's binary_logloss: 0.209529\tvalid_1's auc: 0.791005\tvalid_1's binary_logloss: 0.233394\n",
      "[1200]\ttraining's auc: 0.863164\ttraining's binary_logloss: 0.208563\tvalid_1's auc: 0.790926\tvalid_1's binary_logloss: 0.233405\n",
      "[1250]\ttraining's auc: 0.865495\ttraining's binary_logloss: 0.207558\tvalid_1's auc: 0.790878\tvalid_1's binary_logloss: 0.23339\n",
      "[1300]\ttraining's auc: 0.867709\ttraining's binary_logloss: 0.206599\tvalid_1's auc: 0.790873\tvalid_1's binary_logloss: 0.233373\n",
      "[1350]\ttraining's auc: 0.869923\ttraining's binary_logloss: 0.20563\tvalid_1's auc: 0.790876\tvalid_1's binary_logloss: 0.233354\n",
      "Early stopping, best iteration is:\n",
      "[1158]\ttraining's auc: 0.861291\ttraining's binary_logloss: 0.209357\tvalid_1's auc: 0.791029\tvalid_1's binary_logloss: 0.233384\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "kfoldlgb(k=10)"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "# 使用sklearn的pipelines训练一个模型\n",
    "clf = kfoldlgb(10)\n",
    "clf.fit(app_pre_train, train_labels)"
   ]
  },
  {
   "source": [
    "## Kaggle 输出 （用于上传） "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = clf.predict_proba(app_pre_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_pred = Y[:, 1]\n",
    "submit = app_test[['SK_ID_CURR']]\n",
    "submit['TARGET'] = log_reg_pred\n",
    "submit.to_csv('baseline.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "558a930abb17a758a0551b39992238abdc330d323f5c283a9ea65ae5bef6161a"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}