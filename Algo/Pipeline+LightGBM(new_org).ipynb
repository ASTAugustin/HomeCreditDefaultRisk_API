{
 "cells": [
  {
   "source": [
    "## 读入分析数据"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T09:54:03.520380Z",
     "start_time": "2020-11-02T09:54:03.516392Z"
    }
   },
   "outputs": [],
   "source": [
    "# numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# sklearn preprocessing for dealing with categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, StandardScaler\n",
    "from sklearn.impute import SimpleImputer as Imputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# File system manangement\n",
    "import os\n",
    "\n",
    "import joblib\n",
    "import requests\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T09:54:37.378241Z",
     "start_time": "2020-11-02T09:54:37.370262Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['application_test.csv', 'application_train.csv', 'bureau.csv', 'bureau_balance.csv', 'credit_card_balance.csv', 'HomeCredit_columns_description.csv', 'installments_payments.csv', 'POS_CASH_balance.csv', 'previous_application.csv', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "# List files available\n",
    "print(os.listdir(\"./input/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T09:55:00.329493Z",
     "start_time": "2020-11-02T09:54:56.606423Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data shape:  (307511, 122)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   SK_ID_CURR  TARGET NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR  \\\n",
       "0      100002       1         Cash loans           M            N   \n",
       "1      100003       0         Cash loans           F            N   \n",
       "2      100004       0    Revolving loans           M            Y   \n",
       "3      100006       0         Cash loans           F            N   \n",
       "4      100007       0         Cash loans           M            N   \n",
       "\n",
       "  FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
       "0               Y             0          202500.0    406597.5      24700.5   \n",
       "1               N             0          270000.0   1293502.5      35698.5   \n",
       "2               Y             0           67500.0    135000.0       6750.0   \n",
       "3               Y             0          135000.0    312682.5      29686.5   \n",
       "4               Y             0          121500.0    513000.0      21865.5   \n",
       "\n",
       "   ...  FLAG_DOCUMENT_18 FLAG_DOCUMENT_19 FLAG_DOCUMENT_20 FLAG_DOCUMENT_21  \\\n",
       "0  ...                 0                0                0                0   \n",
       "1  ...                 0                0                0                0   \n",
       "2  ...                 0                0                0                0   \n",
       "3  ...                 0                0                0                0   \n",
       "4  ...                 0                0                0                0   \n",
       "\n",
       "  AMT_REQ_CREDIT_BUREAU_HOUR AMT_REQ_CREDIT_BUREAU_DAY  \\\n",
       "0                        0.0                       0.0   \n",
       "1                        0.0                       0.0   \n",
       "2                        0.0                       0.0   \n",
       "3                        NaN                       NaN   \n",
       "4                        0.0                       0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_WEEK  AMT_REQ_CREDIT_BUREAU_MON  \\\n",
       "0                         0.0                        0.0   \n",
       "1                         0.0                        0.0   \n",
       "2                         0.0                        0.0   \n",
       "3                         NaN                        NaN   \n",
       "4                         0.0                        0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_QRT  AMT_REQ_CREDIT_BUREAU_YEAR  \n",
       "0                        0.0                         1.0  \n",
       "1                        0.0                         0.0  \n",
       "2                        0.0                         0.0  \n",
       "3                        NaN                         NaN  \n",
       "4                        0.0                         0.0  \n",
       "\n",
       "[5 rows x 122 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SK_ID_CURR</th>\n      <th>TARGET</th>\n      <th>NAME_CONTRACT_TYPE</th>\n      <th>CODE_GENDER</th>\n      <th>FLAG_OWN_CAR</th>\n      <th>FLAG_OWN_REALTY</th>\n      <th>CNT_CHILDREN</th>\n      <th>AMT_INCOME_TOTAL</th>\n      <th>AMT_CREDIT</th>\n      <th>AMT_ANNUITY</th>\n      <th>...</th>\n      <th>FLAG_DOCUMENT_18</th>\n      <th>FLAG_DOCUMENT_19</th>\n      <th>FLAG_DOCUMENT_20</th>\n      <th>FLAG_DOCUMENT_21</th>\n      <th>AMT_REQ_CREDIT_BUREAU_HOUR</th>\n      <th>AMT_REQ_CREDIT_BUREAU_DAY</th>\n      <th>AMT_REQ_CREDIT_BUREAU_WEEK</th>\n      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n      <th>AMT_REQ_CREDIT_BUREAU_YEAR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100002</td>\n      <td>1</td>\n      <td>Cash loans</td>\n      <td>M</td>\n      <td>N</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>202500.0</td>\n      <td>406597.5</td>\n      <td>24700.5</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>100003</td>\n      <td>0</td>\n      <td>Cash loans</td>\n      <td>F</td>\n      <td>N</td>\n      <td>N</td>\n      <td>0</td>\n      <td>270000.0</td>\n      <td>1293502.5</td>\n      <td>35698.5</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>100004</td>\n      <td>0</td>\n      <td>Revolving loans</td>\n      <td>M</td>\n      <td>Y</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>67500.0</td>\n      <td>135000.0</td>\n      <td>6750.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>100006</td>\n      <td>0</td>\n      <td>Cash loans</td>\n      <td>F</td>\n      <td>N</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>135000.0</td>\n      <td>312682.5</td>\n      <td>29686.5</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>100007</td>\n      <td>0</td>\n      <td>Cash loans</td>\n      <td>M</td>\n      <td>N</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>121500.0</td>\n      <td>513000.0</td>\n      <td>21865.5</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 122 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# Training data\n",
    "app_train = pd.read_csv('./input/application_train.csv')\n",
    "app_test = pd.read_csv('./input/application_test.csv')\n",
    "bureau = pd.read_csv('./input/bureau.csv')\n",
    "bb = pd.read_csv('./input/bureau_balance.csv')\n",
    "prev = pd.read_csv('./input/previous_application.csv')\n",
    "cc = pd.read_csv('./input/credit_card_balance.csv')\n",
    "ins = pd.read_csv('./input/installments_payments.csv')\n",
    "pos = pd.read_csv('./input/POS_CASH_balance.csv')\n",
    "\n",
    "\n",
    "dic={}\n",
    "dic['bur'] = bureau\n",
    "dic['bb'] = bb\n",
    "dic['pre'] = prev\n",
    "dic['cc'] = cc\n",
    "dic['ins'] = ins\n",
    "dic['pos'] = pos\n",
    "\n",
    "print('Training data shape: ', app_train.shape)\n",
    "app_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Features shape:  (307511, 121)\n",
      "Testing Features shape:  (48744, 121)\n"
     ]
    }
   ],
   "source": [
    "train_labels = app_train['TARGET']\n",
    "\n",
    "app_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n",
    "print('Training Features shape: ', app_train.shape)\n",
    "print('Testing Features shape: ', app_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# numeric_columns = []\n",
    "# category_columns = []\n",
    "\n",
    "# for col in app_train:\n",
    "\n",
    "#     if app_train[col].isna().sum() > 0.7 * app_train.shape[0]:\n",
    "#         continue\n",
    "\n",
    "#     if app_train[col].dtype == ('object') or app_train[col].dtype == ('bool'):\n",
    "#         category_columns.append(col)\n",
    "#     else:\n",
    "#         numeric_columns.append(col)\n",
    "\n",
    "# print(\"Number of categorical feature:\", len(category_columns))\n",
    "# print(\"Number of numerical feature:\", len(numeric_columns))\n",
    "# print(\"Number of missing feature:\", app_train.shape[1] - len(category_columns) - len(numeric_columns))"
   ]
  },
  {
   "source": [
    "## 预处理"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if ((df[col].dtype == 'object') or (df[col].dtype == 'bool'))]\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    " \n",
    "class appTrainTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, X, y=None):\n",
    "        # DAYS_EMPLOYED_anom\n",
    "        X['DAYS_EMPLOYED_anom'] = (X['DAYS_EMPLOYED'] == 365243)\n",
    "        X['DAYS_EMPLOYED'].replace(365243, np.nan, inplace = True)\n",
    "\n",
    "        # Some simple new features (percentages)\n",
    "        X['DAYS_EMPLOYED_PERC'] = X['DAYS_EMPLOYED'] / X['DAYS_BIRTH']\n",
    "        X['INCOME_CREDIT_PERC'] = X['AMT_INCOME_TOTAL'] / X['AMT_CREDIT']\n",
    "        X['INCOME_PER_PERSON'] = X['AMT_INCOME_TOTAL'] / X['CNT_FAM_MEMBERS']\n",
    "        X['ANNUITY_INCOME_PERC'] = X['AMT_ANNUITY'] / X['AMT_INCOME_TOTAL']\n",
    "        X['PAYMENT_RATE'] = X['AMT_ANNUITY'] / X['AMT_CREDIT']\n",
    "        X.replace(np.inf, np.nan, inplace = True)\n",
    "\n",
    "        return X\n",
    "\n",
    "class dropTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "        self.drop = []\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        drop = []\n",
    "        for col in X:\n",
    "            if X[col].isna().sum() > self.rate * X.shape[0]:\n",
    "                drop.append(col)\n",
    "        self.drop = drop\n",
    "        return self\n",
    " \n",
    "    def transform(self, X_copy, y=None):\n",
    "        X_copy.drop(self.drop, axis = 1, inplace = True)\n",
    "        return X_copy\n",
    "\n",
    "class bureauTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, bureauinfo, bbinfo, y=None):\n",
    "        # app_train treatment\n",
    "        bb_copy, bb_cat = one_hot_encoder(bbinfo, True)\n",
    "        bureau_copy, bureau_cat = one_hot_encoder(bureauinfo, True)\n",
    "\n",
    "        # bb Treatment\n",
    "        bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
    "        for col in bb_cat:\n",
    "            bb_aggregations[col] = ['mean']\n",
    "        bb_agg = bb_copy.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "        bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "        bureau_copy = bureau_copy.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "        bureau_copy.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n",
    "\n",
    "        ## bureau Treatment\n",
    "        # Bureau and bureau_balance numeric features\n",
    "        num_aggregations = {\n",
    "            'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
    "            'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "            'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "            'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "            'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "            'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "            'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "            'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "            'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "            'AMT_ANNUITY': ['max', 'mean'],\n",
    "            'CNT_CREDIT_PROLONG': ['sum'],\n",
    "            'MONTHS_BALANCE_MIN': ['min'],\n",
    "            'MONTHS_BALANCE_MAX': ['max'],\n",
    "            'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
    "        }\n",
    "\n",
    "        # Bureau and bureau_balance categorical features\n",
    "        cat_aggregations = {}\n",
    "        for cat in bureau_cat: \n",
    "            cat_aggregations[cat] = ['mean']\n",
    "        for cat in bb_cat: \n",
    "            cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "\n",
    "        bureau_agg = bureau_copy.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "        bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "\n",
    "        # Bureau: Active credits - using only numerical aggregations\n",
    "        active = bureau_copy[bureau_copy['CREDIT_ACTIVE_Active'] == 1]\n",
    "        active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "        active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "        bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "\n",
    "        # Bureau: Closed credits - using only numerical aggregations\n",
    "        closed = bureau_copy[bureau_copy['CREDIT_ACTIVE_Closed'] == 1]\n",
    "        closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "        closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "        bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "        return bureau_agg\n",
    "\n",
    "class previousTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, previnfo, y=None):\n",
    "\n",
    "        prev, cat_cols = one_hot_encoder(previnfo, nan_as_category= True)\n",
    "        # Days 365.243 values -> nan\n",
    "        prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "        prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "        prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "        prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "        prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "\n",
    "        # Add feature: value ask / value received percentage\n",
    "        prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "        # Previous applications numeric features\n",
    "        num_aggregations = {\n",
    "            'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "            'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
    "            'AMT_CREDIT': ['min', 'max', 'mean'],\n",
    "            'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "            'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "            'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "            'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "            'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "            'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "            'CNT_PAYMENT': ['mean', 'sum'],\n",
    "        }\n",
    "        # Previous applications categorical features\n",
    "        cat_aggregations = {}\n",
    "        for cat in cat_cols:\n",
    "            cat_aggregations[cat] = ['mean']\n",
    "        \n",
    "        prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "        prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "        # Previous Applications: Approved Applications - only numerical features\n",
    "        approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "        approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "        approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "        prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "        # Previous Applications: Refused Applications - only numerical features\n",
    "        refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "        refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "        refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "        prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "        return prev_agg\n",
    "\n",
    "class posTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, posinfo, y=None):\n",
    "\n",
    "        pos, cat_cols = one_hot_encoder(posinfo, True)\n",
    "        # Features\n",
    "        aggregations = {\n",
    "            'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
    "            'SK_DPD': ['max', 'mean'],\n",
    "            'SK_DPD_DEF': ['max', 'mean']\n",
    "        }\n",
    "        for cat in cat_cols:\n",
    "            aggregations[cat] = ['mean']\n",
    "        \n",
    "        pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "        pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "        # Count pos cash accounts\n",
    "        pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
    "\n",
    "        return pos_agg\n",
    "    \n",
    "class installmentsTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, insinfo, y=None):\n",
    "        ins, cat_cols = one_hot_encoder(insinfo, True)\n",
    "\n",
    "        # Percentage and difference paid in each installment (amount paid and installment value)\n",
    "        ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "        ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "        # Days past due and days before due (no negative values)\n",
    "        ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "        ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "        ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "        ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "        # Features: Perform aggregations\n",
    "        aggregations = {\n",
    "            'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "            'DPD': ['max', 'mean', 'sum'],\n",
    "            'DBD': ['max', 'mean', 'sum'],\n",
    "            'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "            'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "            'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
    "            'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "            'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n",
    "        }\n",
    "        for cat in cat_cols:\n",
    "            aggregations[cat] = ['mean']\n",
    "        ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "        ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "        # Count installments accounts\n",
    "        ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "        return ins_agg\n",
    "\n",
    "class ccTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, ccinfo, y=None):\n",
    "        cc, cat_cols = one_hot_encoder(ccinfo, True)\n",
    "        # General aggregations\n",
    "        cc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n",
    "        cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "        cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "        # Count credit card lines\n",
    "        cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "        return cc_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bur = bureauTransformer().transform(dic['bur'], dic['bb'])\n",
    "pre = previousTransformer().transform(dic['pre'])\n",
    "pos = posTransformer().transform(dic['pos'])\n",
    "cc = ccTransformer().transform(dic['cc'])\n",
    "ins = installmentsTransformer().transform(dic['ins'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join(X):\n",
    "    X = X.join(bur, how='left', on='SK_ID_CURR')\n",
    "    X = X.join(pre, how='left', on='SK_ID_CURR')\n",
    "    X = X.join(pos, how='left', on='SK_ID_CURR')\n",
    "    X = X.join(cc, how='left', on='SK_ID_CURR')\n",
    "    X = X.join(ins, how='left', on='SK_ID_CURR')\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of categorical feature: 17\nNumber of numerical feature: 484\n"
     ]
    }
   ],
   "source": [
    "X = app_train.copy()\n",
    "X = join(X)\n",
    "X = appTrainTransformer().transform(X)\n",
    "X = dropTransformer(0.7).fit_transform(X)\n",
    "\n",
    "\n",
    "numeric_columns = []\n",
    "category_columns = []\n",
    "\n",
    "for col in X:\n",
    "    \n",
    "    if X[col].dtype == ('object') or X[col].dtype == ('bool'):\n",
    "        category_columns.append(col)\n",
    "    else:\n",
    "        numeric_columns.append(col)\n",
    "\n",
    "print(\"Number of categorical feature:\", len(category_columns))\n",
    "print(\"Number of numerical feature:\", len(numeric_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('impute_nan', Imputer(missing_values=np.nan,strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "category_transformer = Pipeline(steps=[\n",
    "    ('impute', Imputer(strategy='constant', fill_value='missing')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocesser = ColumnTransformer(transformers=[\n",
    "    ('numeric', numeric_transformer, numeric_columns),\n",
    "    ('category', category_transformer, category_columns)\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_pre = Pipeline(steps=[\n",
    "    ('app', appTrainTransformer()),\n",
    "    ('drop', dropTransformer(0.7)),\n",
    "    ('preprocesser', preprocesser)\n",
    "])"
   ]
  },
  {
   "source": [
    "## 建造Pipeline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "class kfoldlgb(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.clf = []\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        kf = KFold(n_splits=self.k)\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            gbm = lgb.LGBMClassifier(\n",
    "                nthread=4,\n",
    "                n_estimators=10000,\n",
    "                learning_rate=0.02,\n",
    "                num_leaves=34,\n",
    "                colsample_bytree=0.9497036,\n",
    "                subsample=0.8715623,\n",
    "                max_depth=8,\n",
    "                reg_alpha=0.041545473,\n",
    "                reg_lambda=0.0735294,\n",
    "                min_split_gain=0.0222415,\n",
    "                min_child_weight=39.3259775,\n",
    "                silent=-1,\n",
    "                verbose=-1, )\n",
    "            gbm.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], eval_metric= 'auc', verbose= 50, early_stopping_rounds= 200)\n",
    "            self.clf.append(gbm)\n",
    "        return self\n",
    " \n",
    "    def predict_proba(self, X, y=None):\n",
    "        for i in range(self.k):\n",
    "            tmp = self.clf[i].predict_proba(X)\n",
    "            if i!=0:\n",
    "                result = result + tmp/self.k\n",
    "            else:\n",
    "                result = tmp/self.k\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_av_train = app_train.copy()\n",
    "app_av_train = join(app_av_train)\n",
    "\n",
    "app_av_test = app_test.copy()\n",
    "app_av_test = join(app_av_test)\n",
    "\n",
    "app_pre_train = pipeline_pre.fit_transform(app_av_train)\n",
    "app_pre_test = pipeline_pre.transform(app_av_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "'s auc: 0.785707\tvalid_1's binary_logloss: 0.236001\n",
      "[700]\ttraining's auc: 0.842068\ttraining's binary_logloss: 0.217485\tvalid_1's auc: 0.786066\tvalid_1's binary_logloss: 0.235874\n",
      "[750]\ttraining's auc: 0.845233\ttraining's binary_logloss: 0.216199\tvalid_1's auc: 0.7865\tvalid_1's binary_logloss: 0.235729\n",
      "[800]\ttraining's auc: 0.848409\ttraining's binary_logloss: 0.214914\tvalid_1's auc: 0.786714\tvalid_1's binary_logloss: 0.235652\n",
      "[850]\ttraining's auc: 0.851555\ttraining's binary_logloss: 0.213621\tvalid_1's auc: 0.787054\tvalid_1's binary_logloss: 0.235564\n",
      "[900]\ttraining's auc: 0.854484\ttraining's binary_logloss: 0.212419\tvalid_1's auc: 0.787316\tvalid_1's binary_logloss: 0.235485\n",
      "[950]\ttraining's auc: 0.857353\ttraining's binary_logloss: 0.211203\tvalid_1's auc: 0.787543\tvalid_1's binary_logloss: 0.235403\n",
      "[1000]\ttraining's auc: 0.860081\ttraining's binary_logloss: 0.210046\tvalid_1's auc: 0.787707\tvalid_1's binary_logloss: 0.23535\n",
      "[1050]\ttraining's auc: 0.862836\ttraining's binary_logloss: 0.20887\tvalid_1's auc: 0.787939\tvalid_1's binary_logloss: 0.235288\n",
      "[1100]\ttraining's auc: 0.86534\ttraining's binary_logloss: 0.207794\tvalid_1's auc: 0.788043\tvalid_1's binary_logloss: 0.235254\n",
      "[1150]\ttraining's auc: 0.8681\ttraining's binary_logloss: 0.206614\tvalid_1's auc: 0.788301\tvalid_1's binary_logloss: 0.235184\n",
      "[1200]\ttraining's auc: 0.870662\ttraining's binary_logloss: 0.205517\tvalid_1's auc: 0.78829\tvalid_1's binary_logloss: 0.235207\n",
      "[1250]\ttraining's auc: 0.873057\ttraining's binary_logloss: 0.204462\tvalid_1's auc: 0.788359\tvalid_1's binary_logloss: 0.235193\n",
      "[1300]\ttraining's auc: 0.875498\ttraining's binary_logloss: 0.203394\tvalid_1's auc: 0.788326\tvalid_1's binary_logloss: 0.2352\n",
      "[1350]\ttraining's auc: 0.877852\ttraining's binary_logloss: 0.202344\tvalid_1's auc: 0.788314\tvalid_1's binary_logloss: 0.235186\n",
      "[1400]\ttraining's auc: 0.880219\ttraining's binary_logloss: 0.201296\tvalid_1's auc: 0.788407\tvalid_1's binary_logloss: 0.235166\n",
      "[1450]\ttraining's auc: 0.882347\ttraining's binary_logloss: 0.200307\tvalid_1's auc: 0.788362\tvalid_1's binary_logloss: 0.235187\n",
      "[1500]\ttraining's auc: 0.884499\ttraining's binary_logloss: 0.199316\tvalid_1's auc: 0.788348\tvalid_1's binary_logloss: 0.235195\n",
      "[1550]\ttraining's auc: 0.886535\ttraining's binary_logloss: 0.198333\tvalid_1's auc: 0.788372\tvalid_1's binary_logloss: 0.235203\n",
      "Early stopping, best iteration is:\n",
      "[1379]\ttraining's auc: 0.879196\ttraining's binary_logloss: 0.201748\tvalid_1's auc: 0.788388\tvalid_1's binary_logloss: 0.235161\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's auc: 0.75267\ttraining's binary_logloss: 0.253844\tvalid_1's auc: 0.741554\tvalid_1's binary_logloss: 0.261307\n",
      "[100]\ttraining's auc: 0.771823\ttraining's binary_logloss: 0.244661\tvalid_1's auc: 0.757392\tvalid_1's binary_logloss: 0.253623\n",
      "[150]\ttraining's auc: 0.78524\ttraining's binary_logloss: 0.239093\tvalid_1's auc: 0.767682\tvalid_1's binary_logloss: 0.249447\n",
      "[200]\ttraining's auc: 0.794798\ttraining's binary_logloss: 0.235168\tvalid_1's auc: 0.773835\tvalid_1's binary_logloss: 0.246979\n",
      "[250]\ttraining's auc: 0.802228\ttraining's binary_logloss: 0.23209\tvalid_1's auc: 0.777754\tvalid_1's binary_logloss: 0.245362\n",
      "[300]\ttraining's auc: 0.808372\ttraining's binary_logloss: 0.22956\tvalid_1's auc: 0.780506\tvalid_1's binary_logloss: 0.244211\n",
      "[350]\ttraining's auc: 0.813701\ttraining's binary_logloss: 0.227386\tvalid_1's auc: 0.782679\tvalid_1's binary_logloss: 0.243363\n",
      "[400]\ttraining's auc: 0.818441\ttraining's binary_logloss: 0.225466\tvalid_1's auc: 0.784249\tvalid_1's binary_logloss: 0.242722\n",
      "[450]\ttraining's auc: 0.822633\ttraining's binary_logloss: 0.22375\tvalid_1's auc: 0.785552\tvalid_1's binary_logloss: 0.242252\n",
      "[500]\ttraining's auc: 0.826748\ttraining's binary_logloss: 0.222134\tvalid_1's auc: 0.786704\tvalid_1's binary_logloss: 0.24183\n",
      "[550]\ttraining's auc: 0.830571\ttraining's binary_logloss: 0.220618\tvalid_1's auc: 0.78761\tvalid_1's binary_logloss: 0.241493\n",
      "[600]\ttraining's auc: 0.834056\ttraining's binary_logloss: 0.219239\tvalid_1's auc: 0.788322\tvalid_1's binary_logloss: 0.241218\n",
      "[650]\ttraining's auc: 0.83746\ttraining's binary_logloss: 0.217874\tvalid_1's auc: 0.788854\tvalid_1's binary_logloss: 0.241031\n",
      "[700]\ttraining's auc: 0.840842\ttraining's binary_logloss: 0.216551\tvalid_1's auc: 0.789125\tvalid_1's binary_logloss: 0.240909\n",
      "[750]\ttraining's auc: 0.843933\ttraining's binary_logloss: 0.215308\tvalid_1's auc: 0.789376\tvalid_1's binary_logloss: 0.240818\n",
      "[800]\ttraining's auc: 0.846821\ttraining's binary_logloss: 0.214135\tvalid_1's auc: 0.789579\tvalid_1's binary_logloss: 0.240728\n",
      "[850]\ttraining's auc: 0.849838\ttraining's binary_logloss: 0.212914\tvalid_1's auc: 0.789901\tvalid_1's binary_logloss: 0.240617\n",
      "[900]\ttraining's auc: 0.852913\ttraining's binary_logloss: 0.211697\tvalid_1's auc: 0.790211\tvalid_1's binary_logloss: 0.240503\n",
      "[950]\ttraining's auc: 0.855697\ttraining's binary_logloss: 0.210531\tvalid_1's auc: 0.790457\tvalid_1's binary_logloss: 0.240414\n",
      "[1000]\ttraining's auc: 0.858402\ttraining's binary_logloss: 0.209399\tvalid_1's auc: 0.790637\tvalid_1's binary_logloss: 0.24035\n",
      "[1050]\ttraining's auc: 0.861128\ttraining's binary_logloss: 0.208222\tvalid_1's auc: 0.790915\tvalid_1's binary_logloss: 0.240249\n",
      "[1100]\ttraining's auc: 0.863669\ttraining's binary_logloss: 0.207129\tvalid_1's auc: 0.791026\tvalid_1's binary_logloss: 0.240193\n",
      "[1150]\ttraining's auc: 0.866286\ttraining's binary_logloss: 0.206023\tvalid_1's auc: 0.791291\tvalid_1's binary_logloss: 0.24009\n",
      "[1200]\ttraining's auc: 0.868904\ttraining's binary_logloss: 0.204896\tvalid_1's auc: 0.791487\tvalid_1's binary_logloss: 0.240023\n",
      "[1250]\ttraining's auc: 0.871166\ttraining's binary_logloss: 0.203898\tvalid_1's auc: 0.791504\tvalid_1's binary_logloss: 0.240008\n",
      "[1300]\ttraining's auc: 0.87351\ttraining's binary_logloss: 0.202853\tvalid_1's auc: 0.791651\tvalid_1's binary_logloss: 0.239973\n",
      "[1350]\ttraining's auc: 0.875831\ttraining's binary_logloss: 0.201798\tvalid_1's auc: 0.791749\tvalid_1's binary_logloss: 0.239938\n",
      "[1400]\ttraining's auc: 0.878156\ttraining's binary_logloss: 0.200742\tvalid_1's auc: 0.7918\tvalid_1's binary_logloss: 0.239927\n",
      "[1450]\ttraining's auc: 0.88036\ttraining's binary_logloss: 0.199734\tvalid_1's auc: 0.79189\tvalid_1's binary_logloss: 0.239892\n",
      "[1500]\ttraining's auc: 0.882424\ttraining's binary_logloss: 0.198757\tvalid_1's auc: 0.791892\tvalid_1's binary_logloss: 0.239894\n",
      "[1550]\ttraining's auc: 0.884568\ttraining's binary_logloss: 0.197757\tvalid_1's auc: 0.791968\tvalid_1's binary_logloss: 0.239871\n",
      "[1600]\ttraining's auc: 0.886472\ttraining's binary_logloss: 0.19683\tvalid_1's auc: 0.791926\tvalid_1's binary_logloss: 0.23989\n",
      "[1650]\ttraining's auc: 0.888447\ttraining's binary_logloss: 0.195874\tvalid_1's auc: 0.792021\tvalid_1's binary_logloss: 0.239873\n",
      "[1700]\ttraining's auc: 0.890548\ttraining's binary_logloss: 0.194872\tvalid_1's auc: 0.792054\tvalid_1's binary_logloss: 0.239872\n",
      "[1750]\ttraining's auc: 0.892542\ttraining's binary_logloss: 0.193909\tvalid_1's auc: 0.792061\tvalid_1's binary_logloss: 0.239872\n",
      "[1800]\ttraining's auc: 0.894511\ttraining's binary_logloss: 0.192933\tvalid_1's auc: 0.792121\tvalid_1's binary_logloss: 0.239866\n",
      "[1850]\ttraining's auc: 0.896495\ttraining's binary_logloss: 0.191965\tvalid_1's auc: 0.792089\tvalid_1's binary_logloss: 0.239876\n",
      "[1900]\ttraining's auc: 0.898507\ttraining's binary_logloss: 0.190996\tvalid_1's auc: 0.792092\tvalid_1's binary_logloss: 0.23988\n",
      "[1950]\ttraining's auc: 0.900308\ttraining's binary_logloss: 0.19011\tvalid_1's auc: 0.79214\tvalid_1's binary_logloss: 0.239886\n",
      "Early stopping, best iteration is:\n",
      "[1759]\ttraining's auc: 0.892873\ttraining's binary_logloss: 0.193745\tvalid_1's auc: 0.792138\tvalid_1's binary_logloss: 0.239854\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's auc: 0.753365\ttraining's binary_logloss: 0.254407\tvalid_1's auc: 0.732597\tvalid_1's binary_logloss: 0.258841\n",
      "[100]\ttraining's auc: 0.77339\ttraining's binary_logloss: 0.245115\tvalid_1's auc: 0.748507\tvalid_1's binary_logloss: 0.251697\n",
      "[150]\ttraining's auc: 0.786933\ttraining's binary_logloss: 0.239457\tvalid_1's auc: 0.758445\tvalid_1's binary_logloss: 0.247908\n",
      "[200]\ttraining's auc: 0.796478\ttraining's binary_logloss: 0.23551\tvalid_1's auc: 0.764788\tvalid_1's binary_logloss: 0.245658\n",
      "[250]\ttraining's auc: 0.803864\ttraining's binary_logloss: 0.232436\tvalid_1's auc: 0.768783\tvalid_1's binary_logloss: 0.2442\n",
      "[300]\ttraining's auc: 0.809928\ttraining's binary_logloss: 0.229862\tvalid_1's auc: 0.771818\tvalid_1's binary_logloss: 0.243101\n",
      "[350]\ttraining's auc: 0.815223\ttraining's binary_logloss: 0.227649\tvalid_1's auc: 0.774195\tvalid_1's binary_logloss: 0.242282\n",
      "[400]\ttraining's auc: 0.819942\ttraining's binary_logloss: 0.2257\tvalid_1's auc: 0.776134\tvalid_1's binary_logloss: 0.241628\n",
      "[450]\ttraining's auc: 0.824208\ttraining's binary_logloss: 0.223987\tvalid_1's auc: 0.77753\tvalid_1's binary_logloss: 0.241155\n",
      "[500]\ttraining's auc: 0.828051\ttraining's binary_logloss: 0.22242\tvalid_1's auc: 0.77848\tvalid_1's binary_logloss: 0.240802\n",
      "[550]\ttraining's auc: 0.831939\ttraining's binary_logloss: 0.220848\tvalid_1's auc: 0.779468\tvalid_1's binary_logloss: 0.240489\n",
      "[600]\ttraining's auc: 0.835385\ttraining's binary_logloss: 0.219434\tvalid_1's auc: 0.780214\tvalid_1's binary_logloss: 0.240226\n",
      "[650]\ttraining's auc: 0.838957\ttraining's binary_logloss: 0.218021\tvalid_1's auc: 0.780955\tvalid_1's binary_logloss: 0.239967\n",
      "[700]\ttraining's auc: 0.842168\ttraining's binary_logloss: 0.216727\tvalid_1's auc: 0.781363\tvalid_1's binary_logloss: 0.239808\n",
      "[750]\ttraining's auc: 0.845484\ttraining's binary_logloss: 0.215372\tvalid_1's auc: 0.781959\tvalid_1's binary_logloss: 0.23961\n",
      "[800]\ttraining's auc: 0.848511\ttraining's binary_logloss: 0.214106\tvalid_1's auc: 0.782306\tvalid_1's binary_logloss: 0.239483\n",
      "[850]\ttraining's auc: 0.851515\ttraining's binary_logloss: 0.212833\tvalid_1's auc: 0.782625\tvalid_1's binary_logloss: 0.239351\n",
      "[900]\ttraining's auc: 0.854586\ttraining's binary_logloss: 0.211586\tvalid_1's auc: 0.78277\tvalid_1's binary_logloss: 0.239283\n",
      "[950]\ttraining's auc: 0.857333\ttraining's binary_logloss: 0.21043\tvalid_1's auc: 0.782888\tvalid_1's binary_logloss: 0.239258\n",
      "[1000]\ttraining's auc: 0.860045\ttraining's binary_logloss: 0.209276\tvalid_1's auc: 0.783003\tvalid_1's binary_logloss: 0.239208\n",
      "[1050]\ttraining's auc: 0.862629\ttraining's binary_logloss: 0.20816\tvalid_1's auc: 0.783231\tvalid_1's binary_logloss: 0.239122\n",
      "[1100]\ttraining's auc: 0.865275\ttraining's binary_logloss: 0.207013\tvalid_1's auc: 0.783242\tvalid_1's binary_logloss: 0.239099\n",
      "[1150]\ttraining's auc: 0.867836\ttraining's binary_logloss: 0.205909\tvalid_1's auc: 0.783204\tvalid_1's binary_logloss: 0.239093\n",
      "[1200]\ttraining's auc: 0.870424\ttraining's binary_logloss: 0.204759\tvalid_1's auc: 0.783257\tvalid_1's binary_logloss: 0.239084\n",
      "[1250]\ttraining's auc: 0.872793\ttraining's binary_logloss: 0.203683\tvalid_1's auc: 0.783263\tvalid_1's binary_logloss: 0.239085\n",
      "[1300]\ttraining's auc: 0.875158\ttraining's binary_logloss: 0.202599\tvalid_1's auc: 0.78325\tvalid_1's binary_logloss: 0.23909\n",
      "[1350]\ttraining's auc: 0.877433\ttraining's binary_logloss: 0.20155\tvalid_1's auc: 0.783307\tvalid_1's binary_logloss: 0.239069\n",
      "[1400]\ttraining's auc: 0.879523\ttraining's binary_logloss: 0.200591\tvalid_1's auc: 0.783364\tvalid_1's binary_logloss: 0.23906\n",
      "[1450]\ttraining's auc: 0.881706\ttraining's binary_logloss: 0.199602\tvalid_1's auc: 0.783305\tvalid_1's binary_logloss: 0.239074\n",
      "[1500]\ttraining's auc: 0.883862\ttraining's binary_logloss: 0.198587\tvalid_1's auc: 0.783259\tvalid_1's binary_logloss: 0.239092\n",
      "[1550]\ttraining's auc: 0.885978\ttraining's binary_logloss: 0.197575\tvalid_1's auc: 0.783167\tvalid_1's binary_logloss: 0.239107\n",
      "[1600]\ttraining's auc: 0.888062\ttraining's binary_logloss: 0.196598\tvalid_1's auc: 0.783255\tvalid_1's binary_logloss: 0.239077\n",
      "Early stopping, best iteration is:\n",
      "[1414]\ttraining's auc: 0.880152\ttraining's binary_logloss: 0.200324\tvalid_1's auc: 0.783413\tvalid_1's binary_logloss: 0.239044\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's auc: 0.753181\ttraining's binary_logloss: 0.255247\tvalid_1's auc: 0.744788\tvalid_1's binary_logloss: 0.255721\n",
      "[100]\ttraining's auc: 0.771977\ttraining's binary_logloss: 0.246033\tvalid_1's auc: 0.758313\tvalid_1's binary_logloss: 0.248331\n",
      "[150]\ttraining's auc: 0.785543\ttraining's binary_logloss: 0.240382\tvalid_1's auc: 0.767099\tvalid_1's binary_logloss: 0.24445\n",
      "[200]\ttraining's auc: 0.795026\ttraining's binary_logloss: 0.236398\tvalid_1's auc: 0.772803\tvalid_1's binary_logloss: 0.242003\n",
      "[250]\ttraining's auc: 0.802437\ttraining's binary_logloss: 0.233292\tvalid_1's auc: 0.776752\tvalid_1's binary_logloss: 0.240346\n",
      "[300]\ttraining's auc: 0.808609\ttraining's binary_logloss: 0.230705\tvalid_1's auc: 0.779329\tvalid_1's binary_logloss: 0.239251\n",
      "[350]\ttraining's auc: 0.813947\ttraining's binary_logloss: 0.228487\tvalid_1's auc: 0.781517\tvalid_1's binary_logloss: 0.238379\n",
      "[400]\ttraining's auc: 0.818606\ttraining's binary_logloss: 0.226575\tvalid_1's auc: 0.78301\tvalid_1's binary_logloss: 0.237791\n",
      "[450]\ttraining's auc: 0.822893\ttraining's binary_logloss: 0.224815\tvalid_1's auc: 0.784221\tvalid_1's binary_logloss: 0.237333\n",
      "[500]\ttraining's auc: 0.82679\ttraining's binary_logloss: 0.223194\tvalid_1's auc: 0.78513\tvalid_1's binary_logloss: 0.23697\n",
      "[550]\ttraining's auc: 0.830609\ttraining's binary_logloss: 0.221699\tvalid_1's auc: 0.785913\tvalid_1's binary_logloss: 0.236691\n",
      "[600]\ttraining's auc: 0.834162\ttraining's binary_logloss: 0.220268\tvalid_1's auc: 0.786713\tvalid_1's binary_logloss: 0.236409\n",
      "[650]\ttraining's auc: 0.837307\ttraining's binary_logloss: 0.219002\tvalid_1's auc: 0.787325\tvalid_1's binary_logloss: 0.236215\n",
      "[700]\ttraining's auc: 0.84069\ttraining's binary_logloss: 0.217664\tvalid_1's auc: 0.787825\tvalid_1's binary_logloss: 0.236038\n",
      "[750]\ttraining's auc: 0.843852\ttraining's binary_logloss: 0.21639\tvalid_1's auc: 0.78825\tvalid_1's binary_logloss: 0.235901\n",
      "[800]\ttraining's auc: 0.846963\ttraining's binary_logloss: 0.215126\tvalid_1's auc: 0.788625\tvalid_1's binary_logloss: 0.235776\n",
      "[850]\ttraining's auc: 0.849838\ttraining's binary_logloss: 0.213943\tvalid_1's auc: 0.788823\tvalid_1's binary_logloss: 0.235704\n",
      "[900]\ttraining's auc: 0.852619\ttraining's binary_logloss: 0.212792\tvalid_1's auc: 0.789025\tvalid_1's binary_logloss: 0.235641\n",
      "[950]\ttraining's auc: 0.855309\ttraining's binary_logloss: 0.211665\tvalid_1's auc: 0.789157\tvalid_1's binary_logloss: 0.235601\n",
      "[1000]\ttraining's auc: 0.858092\ttraining's binary_logloss: 0.21049\tvalid_1's auc: 0.789296\tvalid_1's binary_logloss: 0.235548\n",
      "[1050]\ttraining's auc: 0.860632\ttraining's binary_logloss: 0.209414\tvalid_1's auc: 0.789401\tvalid_1's binary_logloss: 0.235505\n",
      "[1100]\ttraining's auc: 0.863071\ttraining's binary_logloss: 0.208363\tvalid_1's auc: 0.789417\tvalid_1's binary_logloss: 0.23547\n",
      "[1150]\ttraining's auc: 0.865704\ttraining's binary_logloss: 0.207225\tvalid_1's auc: 0.789531\tvalid_1's binary_logloss: 0.235438\n",
      "[1200]\ttraining's auc: 0.868188\ttraining's binary_logloss: 0.20616\tvalid_1's auc: 0.789586\tvalid_1's binary_logloss: 0.235426\n",
      "[1250]\ttraining's auc: 0.870512\ttraining's binary_logloss: 0.205151\tvalid_1's auc: 0.789615\tvalid_1's binary_logloss: 0.235417\n",
      "[1300]\ttraining's auc: 0.872911\ttraining's binary_logloss: 0.204094\tvalid_1's auc: 0.789605\tvalid_1's binary_logloss: 0.235425\n",
      "[1350]\ttraining's auc: 0.875135\ttraining's binary_logloss: 0.203089\tvalid_1's auc: 0.789652\tvalid_1's binary_logloss: 0.235399\n",
      "[1400]\ttraining's auc: 0.877327\ttraining's binary_logloss: 0.202097\tvalid_1's auc: 0.78961\tvalid_1's binary_logloss: 0.235417\n",
      "[1450]\ttraining's auc: 0.879605\ttraining's binary_logloss: 0.201054\tvalid_1's auc: 0.789622\tvalid_1's binary_logloss: 0.235417\n",
      "[1500]\ttraining's auc: 0.881774\ttraining's binary_logloss: 0.200053\tvalid_1's auc: 0.789594\tvalid_1's binary_logloss: 0.235433\n",
      "Early stopping, best iteration is:\n",
      "[1334]\ttraining's auc: 0.874434\ttraining's binary_logloss: 0.203412\tvalid_1's auc: 0.789675\tvalid_1's binary_logloss: 0.235392\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's auc: 0.751397\ttraining's binary_logloss: 0.255764\tvalid_1's auc: 0.743223\tvalid_1's binary_logloss: 0.254061\n",
      "[100]\ttraining's auc: 0.770943\ttraining's binary_logloss: 0.246594\tvalid_1's auc: 0.758156\tvalid_1's binary_logloss: 0.246577\n",
      "[150]\ttraining's auc: 0.785109\ttraining's binary_logloss: 0.240904\tvalid_1's auc: 0.768696\tvalid_1's binary_logloss: 0.242372\n",
      "[200]\ttraining's auc: 0.794661\ttraining's binary_logloss: 0.236936\tvalid_1's auc: 0.774718\tvalid_1's binary_logloss: 0.23989\n",
      "[250]\ttraining's auc: 0.802082\ttraining's binary_logloss: 0.233854\tvalid_1's auc: 0.778541\tvalid_1's binary_logloss: 0.238278\n",
      "[300]\ttraining's auc: 0.808189\ttraining's binary_logloss: 0.231309\tvalid_1's auc: 0.781412\tvalid_1's binary_logloss: 0.237151\n",
      "[350]\ttraining's auc: 0.813425\ttraining's binary_logloss: 0.229122\tvalid_1's auc: 0.783566\tvalid_1's binary_logloss: 0.236304\n",
      "[400]\ttraining's auc: 0.818163\ttraining's binary_logloss: 0.227169\tvalid_1's auc: 0.785044\tvalid_1's binary_logloss: 0.2357\n",
      "[450]\ttraining's auc: 0.822647\ttraining's binary_logloss: 0.225368\tvalid_1's auc: 0.786315\tvalid_1's binary_logloss: 0.235229\n",
      "[500]\ttraining's auc: 0.826553\ttraining's binary_logloss: 0.223774\tvalid_1's auc: 0.787196\tvalid_1's binary_logloss: 0.234876\n",
      "[550]\ttraining's auc: 0.830316\ttraining's binary_logloss: 0.22229\tvalid_1's auc: 0.787945\tvalid_1's binary_logloss: 0.234602\n",
      "[600]\ttraining's auc: 0.834078\ttraining's binary_logloss: 0.220805\tvalid_1's auc: 0.788454\tvalid_1's binary_logloss: 0.234386\n",
      "[650]\ttraining's auc: 0.837552\ttraining's binary_logloss: 0.219433\tvalid_1's auc: 0.788832\tvalid_1's binary_logloss: 0.234225\n",
      "[700]\ttraining's auc: 0.840914\ttraining's binary_logloss: 0.2181\tvalid_1's auc: 0.789182\tvalid_1's binary_logloss: 0.234075\n",
      "[750]\ttraining's auc: 0.844037\ttraining's binary_logloss: 0.216856\tvalid_1's auc: 0.789566\tvalid_1's binary_logloss: 0.233946\n",
      "[800]\ttraining's auc: 0.846944\ttraining's binary_logloss: 0.215667\tvalid_1's auc: 0.78987\tvalid_1's binary_logloss: 0.233812\n",
      "[850]\ttraining's auc: 0.849955\ttraining's binary_logloss: 0.21445\tvalid_1's auc: 0.790133\tvalid_1's binary_logloss: 0.23371\n",
      "[900]\ttraining's auc: 0.852842\ttraining's binary_logloss: 0.213297\tvalid_1's auc: 0.790311\tvalid_1's binary_logloss: 0.23365\n",
      "[950]\ttraining's auc: 0.855662\ttraining's binary_logloss: 0.212146\tvalid_1's auc: 0.790507\tvalid_1's binary_logloss: 0.233579\n",
      "[1000]\ttraining's auc: 0.85838\ttraining's binary_logloss: 0.211022\tvalid_1's auc: 0.790546\tvalid_1's binary_logloss: 0.233565\n",
      "[1050]\ttraining's auc: 0.861006\ttraining's binary_logloss: 0.209895\tvalid_1's auc: 0.790787\tvalid_1's binary_logloss: 0.233485\n",
      "[1100]\ttraining's auc: 0.863457\ttraining's binary_logloss: 0.208835\tvalid_1's auc: 0.790866\tvalid_1's binary_logloss: 0.233441\n",
      "[1150]\ttraining's auc: 0.865958\ttraining's binary_logloss: 0.207758\tvalid_1's auc: 0.79098\tvalid_1's binary_logloss: 0.233372\n",
      "[1200]\ttraining's auc: 0.868184\ttraining's binary_logloss: 0.206766\tvalid_1's auc: 0.791029\tvalid_1's binary_logloss: 0.233341\n",
      "[1250]\ttraining's auc: 0.870514\ttraining's binary_logloss: 0.205732\tvalid_1's auc: 0.791079\tvalid_1's binary_logloss: 0.233313\n",
      "[1300]\ttraining's auc: 0.872934\ttraining's binary_logloss: 0.204661\tvalid_1's auc: 0.791244\tvalid_1's binary_logloss: 0.233262\n",
      "[1350]\ttraining's auc: 0.875281\ttraining's binary_logloss: 0.203625\tvalid_1's auc: 0.791329\tvalid_1's binary_logloss: 0.233229\n",
      "[1400]\ttraining's auc: 0.877519\ttraining's binary_logloss: 0.202597\tvalid_1's auc: 0.791394\tvalid_1's binary_logloss: 0.233199\n",
      "[1450]\ttraining's auc: 0.879729\ttraining's binary_logloss: 0.201563\tvalid_1's auc: 0.791409\tvalid_1's binary_logloss: 0.233171\n",
      "[1500]\ttraining's auc: 0.881908\ttraining's binary_logloss: 0.200537\tvalid_1's auc: 0.791394\tvalid_1's binary_logloss: 0.233197\n",
      "[1550]\ttraining's auc: 0.884155\ttraining's binary_logloss: 0.19949\tvalid_1's auc: 0.791331\tvalid_1's binary_logloss: 0.23323\n",
      "Early stopping, best iteration is:\n",
      "[1382]\ttraining's auc: 0.876722\ttraining's binary_logloss: 0.20295\tvalid_1's auc: 0.791439\tvalid_1's binary_logloss: 0.233189\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "kfoldlgb(k=5)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# 使用sklearn的pipelines训练一个模型\n",
    "clf = kfoldlgb(5)\n",
    "clf.fit(app_pre_train, train_labels)"
   ]
  },
  {
   "source": [
    "## Kaggle 输出 （用于上传） "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = clf.predict_proba(app_pre_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_pred = Y[:, 1]\n",
    "submit = app_test[['SK_ID_CURR']]\n",
    "submit['TARGET'] = log_reg_pred\n",
    "submit.to_csv('baseline.csv', index = False)"
   ]
  },
  {
   "source": [
    "## Pipeline Output"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['./preprocess_model.joblib']"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "joblib.dump(pipeline_pre, './preprocess_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(clf, './clf.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('ICT_TP2': conda)",
   "metadata": {
    "interpreter": {
     "hash": "558a930abb17a758a0551b39992238abdc330d323f5c283a9ea65ae5bef6161a"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}