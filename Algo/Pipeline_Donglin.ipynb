{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.23.2\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# sklearn preprocessing for dealing with categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, StandardScaler\n",
    "from sklearn.impute import SimpleImputer as Imputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# File system manangement\n",
    "import os\n",
    "\n",
    "import joblib\n",
    "import requests\n",
    "from time import time\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['application_test.csv', 'HomeCredit_columns_description.csv', 'POS_CASH_balance.csv', 'credit_card_balance.csv', 'installments_payments.csv', 'application_train.csv', 'bureau.csv', 'previous_application.csv', 'bureau_balance.csv', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "# List files available\n",
    "print(os.listdir(\"/Users/dkoalal/Desktop/风控proj/csv/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data shape:  (307511, 122)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   SK_ID_CURR  TARGET NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR  \\\n",
       "0      100002       1         Cash loans           M            N   \n",
       "1      100003       0         Cash loans           F            N   \n",
       "2      100004       0    Revolving loans           M            Y   \n",
       "3      100006       0         Cash loans           F            N   \n",
       "4      100007       0         Cash loans           M            N   \n",
       "\n",
       "  FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
       "0               Y             0          202500.0    406597.5      24700.5   \n",
       "1               N             0          270000.0   1293502.5      35698.5   \n",
       "2               Y             0           67500.0    135000.0       6750.0   \n",
       "3               Y             0          135000.0    312682.5      29686.5   \n",
       "4               Y             0          121500.0    513000.0      21865.5   \n",
       "\n",
       "   ...  FLAG_DOCUMENT_18 FLAG_DOCUMENT_19 FLAG_DOCUMENT_20 FLAG_DOCUMENT_21  \\\n",
       "0  ...                 0                0                0                0   \n",
       "1  ...                 0                0                0                0   \n",
       "2  ...                 0                0                0                0   \n",
       "3  ...                 0                0                0                0   \n",
       "4  ...                 0                0                0                0   \n",
       "\n",
       "  AMT_REQ_CREDIT_BUREAU_HOUR AMT_REQ_CREDIT_BUREAU_DAY  \\\n",
       "0                        0.0                       0.0   \n",
       "1                        0.0                       0.0   \n",
       "2                        0.0                       0.0   \n",
       "3                        NaN                       NaN   \n",
       "4                        0.0                       0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_WEEK  AMT_REQ_CREDIT_BUREAU_MON  \\\n",
       "0                         0.0                        0.0   \n",
       "1                         0.0                        0.0   \n",
       "2                         0.0                        0.0   \n",
       "3                         NaN                        NaN   \n",
       "4                         0.0                        0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_QRT  AMT_REQ_CREDIT_BUREAU_YEAR  \n",
       "0                        0.0                         1.0  \n",
       "1                        0.0                         0.0  \n",
       "2                        0.0                         0.0  \n",
       "3                        NaN                         NaN  \n",
       "4                        0.0                         0.0  \n",
       "\n",
       "[5 rows x 122 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SK_ID_CURR</th>\n      <th>TARGET</th>\n      <th>NAME_CONTRACT_TYPE</th>\n      <th>CODE_GENDER</th>\n      <th>FLAG_OWN_CAR</th>\n      <th>FLAG_OWN_REALTY</th>\n      <th>CNT_CHILDREN</th>\n      <th>AMT_INCOME_TOTAL</th>\n      <th>AMT_CREDIT</th>\n      <th>AMT_ANNUITY</th>\n      <th>...</th>\n      <th>FLAG_DOCUMENT_18</th>\n      <th>FLAG_DOCUMENT_19</th>\n      <th>FLAG_DOCUMENT_20</th>\n      <th>FLAG_DOCUMENT_21</th>\n      <th>AMT_REQ_CREDIT_BUREAU_HOUR</th>\n      <th>AMT_REQ_CREDIT_BUREAU_DAY</th>\n      <th>AMT_REQ_CREDIT_BUREAU_WEEK</th>\n      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n      <th>AMT_REQ_CREDIT_BUREAU_YEAR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100002</td>\n      <td>1</td>\n      <td>Cash loans</td>\n      <td>M</td>\n      <td>N</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>202500.0</td>\n      <td>406597.5</td>\n      <td>24700.5</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>100003</td>\n      <td>0</td>\n      <td>Cash loans</td>\n      <td>F</td>\n      <td>N</td>\n      <td>N</td>\n      <td>0</td>\n      <td>270000.0</td>\n      <td>1293502.5</td>\n      <td>35698.5</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>100004</td>\n      <td>0</td>\n      <td>Revolving loans</td>\n      <td>M</td>\n      <td>Y</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>67500.0</td>\n      <td>135000.0</td>\n      <td>6750.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>100006</td>\n      <td>0</td>\n      <td>Cash loans</td>\n      <td>F</td>\n      <td>N</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>135000.0</td>\n      <td>312682.5</td>\n      <td>29686.5</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>100007</td>\n      <td>0</td>\n      <td>Cash loans</td>\n      <td>M</td>\n      <td>N</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>121500.0</td>\n      <td>513000.0</td>\n      <td>21865.5</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 122 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Training data\n",
    "app_train = pd.read_csv('/Users/dkoalal/Desktop/风控proj/csv/application_train.csv')\n",
    "app_test = pd.read_csv('/Users/dkoalal/Desktop/风控proj/csv/application_test.csv')\n",
    "bureau = pd.read_csv('/Users/dkoalal/Desktop/风控proj/csv/bureau.csv')\n",
    "bb = pd.read_csv('/Users/dkoalal/Desktop/风控proj/csv/bureau_balance.csv')\n",
    "prev = pd.read_csv('/Users/dkoalal/Desktop/风控proj/csv/previous_application.csv')\n",
    "cc = pd.read_csv('/Users/dkoalal/Desktop/风控proj/csv/credit_card_balance.csv')\n",
    "ins = pd.read_csv('/Users/dkoalal/Desktop/风控proj/csv/installments_payments.csv')\n",
    "pos = pd.read_csv('/Users/dkoalal/Desktop/风控proj/csv/POS_CASH_balance.csv')\n",
    "\n",
    "\n",
    "dic={}\n",
    "dic['bur'] = bureau\n",
    "dic['bb'] = bb\n",
    "dic['pre'] = prev\n",
    "dic['cc'] = cc\n",
    "dic['ins'] = ins\n",
    "dic['pos'] = pos\n",
    "\n",
    "print('Training data shape: ', app_train.shape)\n",
    "app_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Features shape:  (307511, 121)\nTesting Features shape:  (48744, 121)\napp_train_with_label shape:  (307511, 122)\n"
     ]
    }
   ],
   "source": [
    "train_labels = app_train['TARGET']\n",
    "app_train_with_label = app_train.copy()\n",
    "app_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n",
    "print('Training Features shape: ', app_train.shape)\n",
    "print('Testing Features shape: ', app_test.shape)\n",
    "print('app_train_with_label shape: ', app_train_with_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder_DL(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if ((df[col].dtype == 'object') or (df[col].dtype == 'bool'))]\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "'''\n",
    "def agg_numeric_DL(df, group_var, df_name):\n",
    "    \"\"\"Aggregates the numeric values in a dataframe. This can\n",
    "    be used to create features for each instance of the grouping variable.\n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "        df (dataframe): \n",
    "            the dataframe to calculate the statistics on\n",
    "        group_var (string): \n",
    "            the variable by which to group df\n",
    "        df_name (string): \n",
    "            the variable used to rename the columns\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "        agg (dataframe): \n",
    "            a dataframe with the statistics aggregated for \n",
    "            all numeric columns. Each instance of the grouping variable will have \n",
    "            the statistics (mean, min, max, sum; currently supported) calculated. \n",
    "            The columns are also renamed to keep track of features created.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Remove id variables other than grouping variable\n",
    "    for col in df:\n",
    "        if col != group_var and 'SK_ID' in col:\n",
    "            df = df.drop(columns = col)\n",
    "            \n",
    "    group_ids = df[group_var]\n",
    "    numeric_df = df.select_dtypes('number')\n",
    "    numeric_df[group_var] = group_ids\n",
    "\n",
    "    # Group by the specified variable and calculate the statistics\n",
    "    agg = numeric_df.groupby(group_var).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()\n",
    "\n",
    "    # Need to create new column names\n",
    "    columns = [group_var]\n",
    "\n",
    "    # Iterate through the variables names\n",
    "    for var in agg.columns.levels[0]:\n",
    "        # Skip the grouping variable\n",
    "        if var != group_var:\n",
    "            # Iterate through the stat names\n",
    "            for stat in agg.columns.levels[1][:-1]:\n",
    "                # Make a new column name for the variable and stat\n",
    "                columns.append('%s_%s_%s' % (df_name, var, stat))\n",
    "\n",
    "    agg.columns = columns\n",
    "\n",
    "     # Remove the columns with all redundant values\n",
    "    _, idx = np.unique(agg, axis = 1, return_index=True)\n",
    "    agg = agg.iloc[:, idx]\n",
    "    return agg\n",
    "'''\n",
    "\n",
    "# Function to calculate correlations with the target for a dataframe\n",
    "def target_corrs_DL(df):\n",
    "\n",
    "    # List of correlations\n",
    "    corrs = []\n",
    "\n",
    "    # Iterate through the columns \n",
    "    for col in df.columns:\n",
    "        print(col)\n",
    "        # Skip the target column\n",
    "        if col != 'TARGET':\n",
    "            # Calculate correlation with the target\n",
    "            corr = df['TARGET'].corr(df[col])\n",
    "\n",
    "            # Append the list as a tuple\n",
    "            corrs.append((col, corr))\n",
    "            \n",
    "    # Sort by absolute magnitude of correlations\n",
    "    corrs = sorted(corrs, key = lambda x: abs(x[1]), reverse = True)\n",
    "    \n",
    "    return corrs\n",
    "\n",
    "def count_categorical_DL(df, group_var, df_name):\n",
    "    \"\"\"Computes counts and normalized counts for each observation\n",
    "    of `group_var` of each unique category in every categorical variable\n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "    df : dataframe \n",
    "        The dataframe to calculate the value counts for.\n",
    "        \n",
    "    group_var : string\n",
    "        The variable by which to group the dataframe. For each unique\n",
    "        value of this variable, the final dataframe will have one row\n",
    "        \n",
    "    df_name : string\n",
    "        Variable added to the front of column names to keep track of columns\n",
    "\n",
    "    \n",
    "    Return\n",
    "    --------\n",
    "    categorical : dataframe\n",
    "        A dataframe with counts and normalized counts of each unique category in every categorical variable\n",
    "        with one row for every unique value of the `group_var`.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Select the categorical columns\n",
    "    categorical = pd.get_dummies(df.select_dtypes('object'))\n",
    "\n",
    "    # Make sure to put the identifying id on the column\n",
    "    categorical[group_var] = df[group_var]\n",
    "\n",
    "    # Groupby the group var and calculate the sum and mean\n",
    "    categorical = categorical.groupby(group_var).agg(['sum', 'mean'])\n",
    "    \n",
    "    column_names = []\n",
    "    \n",
    "    # Iterate through the columns in level 0\n",
    "    for var in categorical.columns.levels[0]:\n",
    "        # Iterate through the stats in level 1\n",
    "        for stat in ['count', 'count_norm']:\n",
    "            # Make a new column name\n",
    "            column_names.append('%s_%s_%s' % (df_name, var, stat))\n",
    "    \n",
    "    categorical.columns = column_names\n",
    "    \n",
    "    return categorical\n",
    "\n",
    "def agg_categorical_DL(df, parent_var, df_name):\n",
    "    \"\"\"\n",
    "    Aggregates the categorical features in a child dataframe\n",
    "    for each observation of the parent variable.\n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "    df : dataframe \n",
    "        The dataframe to calculate the value counts for.\n",
    "        \n",
    "    parent_var : string\n",
    "        The variable by which to group and aggregate the dataframe. For each unique\n",
    "        value of this variable, the final dataframe will have one row\n",
    "        \n",
    "    df_name : string\n",
    "        Variable added to the front of column names to keep track of columns\n",
    "\n",
    "    \n",
    "    Return\n",
    "    --------\n",
    "    categorical : dataframe\n",
    "        A dataframe with aggregated statistics for each observation of the parent_var\n",
    "        The columns are also renamed and columns with duplicate values are removed.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Select the categorical columns\n",
    "    categorical = pd.get_dummies(df.select_dtypes('object'))\n",
    "\n",
    "    # Make sure to put the identifying id on the column\n",
    "    categorical[parent_var] = df[parent_var]\n",
    "\n",
    "    # Groupby the group var and calculate the sum and mean\n",
    "    categorical = categorical.groupby(parent_var).agg(['sum', 'count', 'mean'])\n",
    "    \n",
    "    column_names = []\n",
    "    \n",
    "    # Iterate through the columns in level 0\n",
    "    for var in categorical.columns.levels[0]:\n",
    "        # Iterate through the stats in level 1\n",
    "        for stat in ['sum', 'count', 'mean']:\n",
    "            # Make a new column name\n",
    "            column_names.append('%s_%s_%s' % (df_name, var, stat))\n",
    "    \n",
    "    categorical.columns = column_names\n",
    "    \n",
    "    # Remove duplicate columns by values\n",
    "    _, idx = np.unique(categorical, axis = 1, return_index = True)\n",
    "    categorical = categorical.iloc[:, idx]\n",
    "    \n",
    "    return categorical\n",
    "\n",
    "\n",
    "def agg_numeric_DL(df, parent_var, df_name):\n",
    "    \"\"\"\n",
    "    Groups and aggregates the numeric values in a child dataframe\n",
    "    by the parent variable.\n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "        df (dataframe): \n",
    "            the child dataframe to calculate the statistics on\n",
    "        parent_var (string): \n",
    "            the parent variable used for grouping and aggregating\n",
    "        df_name (string): \n",
    "            the variable used to rename the columns\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "        agg (dataframe): \n",
    "            a dataframe with the statistics aggregated by the `parent_var` for \n",
    "            all numeric columns. Each observation of the parent variable will have \n",
    "            one row in the dataframe with the parent variable as the index. \n",
    "            The columns are also renamed using the `df_name`. Columns with all duplicate\n",
    "            values are removed. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove id variables other than grouping variable\n",
    "    for col in df:\n",
    "        if col != parent_var and 'SK_ID' in col:\n",
    "            df = df.drop(columns = col)\n",
    "            \n",
    "    # Only want the numeric variables\n",
    "    parent_ids = df[parent_var].copy()\n",
    "    numeric_df = df.select_dtypes('number').copy()\n",
    "    numeric_df[parent_var] = parent_ids\n",
    "\n",
    "    # Group by the specified variable and calculate the statistics\n",
    "    agg = numeric_df.groupby(parent_var).agg(['count', 'mean', 'max', 'min', 'sum'])\n",
    "\n",
    "    # Need to create new column names\n",
    "    columns = []\n",
    "\n",
    "    # Iterate through the variables names\n",
    "    for var in agg.columns.levels[0]:\n",
    "        if var != parent_var:\n",
    "            # Iterate through the stat names\n",
    "            for stat in agg.columns.levels[1]:\n",
    "                # Make a new column name for the variable and stat\n",
    "                columns.append('%s_%s_%s' % (df_name, var, stat))\n",
    "    \n",
    "    agg.columns = columns\n",
    "    \n",
    "    # Remove the columns with all redundant values\n",
    "    _, idx = np.unique(agg, axis = 1, return_index=True)\n",
    "    agg = agg.iloc[:, idx]\n",
    "    \n",
    "    return agg\n",
    "\n",
    "\n",
    "def aggregate_client_DL(df, group_vars, df_names):\n",
    "    \"\"\"Aggregate a dataframe with data at the loan level \n",
    "    at the client level\n",
    "    \n",
    "    Args:\n",
    "        df (dataframe): data at the loan level\n",
    "        group_vars (list of two strings): grouping variables for the loan \n",
    "        and then the client (example ['SK_ID_PREV', 'SK_ID_CURR'])\n",
    "        names (list of two strings): names to call the resulting columns\n",
    "        (example ['cash', 'client'])\n",
    "        \n",
    "    Returns:\n",
    "        df_client (dataframe): aggregated numeric stats at the client level. \n",
    "        Each client will have a single row with all the numeric data aggregated\n",
    "    \"\"\"\n",
    "    \n",
    "    # Aggregate the numeric columns\n",
    "    df_agg = agg_numeric_DL(df, parent_var = group_vars[0], df_name = df_names[0])\n",
    "    \n",
    "    # If there are categorical variables\n",
    "    if any(df.dtypes == 'object'):\n",
    "    \n",
    "        # Count the categorical columns\n",
    "        df_counts = agg_categorical_DL(df, parent_var = group_vars[0], df_name = df_names[0])\n",
    "\n",
    "        # Merge the numeric and categorical\n",
    "        df_by_loan = df_counts.merge(df_agg, on = group_vars[0], how = 'outer')\n",
    "\n",
    "        gc.enable()\n",
    "        del df_agg, df_counts\n",
    "        gc.collect()\n",
    "\n",
    "        # Merge to get the client id in dataframe\n",
    "        df_by_loan = df_by_loan.merge(df[[group_vars[0], group_vars[1]]], on = group_vars[0], how = 'left')\n",
    "\n",
    "        # Remove the loan id\n",
    "        df_by_loan = df_by_loan.drop(columns = [group_vars[0]])\n",
    "\n",
    "        # Aggregate numeric stats by column\n",
    "        df_by_client = agg_numeric_DL(df_by_loan, parent_var = group_vars[1], df_name = df_names[1])\n",
    "\n",
    "        \n",
    "    # No categorical variables\n",
    "    else:\n",
    "        # Merge to get the client id in dataframe\n",
    "        df_by_loan = df_agg.merge(df[[group_vars[0], group_vars[1]]], on = group_vars[0], how = 'left')\n",
    "        \n",
    "        gc.enable()\n",
    "        del df_agg\n",
    "        gc.collect()\n",
    "        \n",
    "        # Remove the loan id\n",
    "        df_by_loan = df_by_loan.drop(columns = [group_vars[0]])\n",
    "        \n",
    "        # Aggregate numeric stats by column\n",
    "        df_by_client = agg_numeric_DL(df_by_loan, parent_var = group_vars[1], df_name = df_names[1])\n",
    "        \n",
    "    # Memory management\n",
    "    gc.enable()\n",
    "    del df, df_by_loan\n",
    "    gc.collect()\n",
    "\n",
    "    return df_by_client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_zero_importance_features_DL(train, train_labels, iterations = 2):\n",
    "    \"\"\"\n",
    "    Identify zero importance features in a training dataset based on the \n",
    "    feature importances from a gradient boosting model. \n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "    train : dataframe\n",
    "        Training features\n",
    "        \n",
    "    train_labels : np.array\n",
    "        Labels for training data\n",
    "        \n",
    "    iterations : integer, default = 2\n",
    "        Number of cross validation splits to use for determining feature importances\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize an empty array to hold feature importances\n",
    "    feature_importances = np.zeros(train.shape[1])\n",
    "\n",
    "    # Create the model with several hyperparameters\n",
    "    model = lgb.LGBMClassifier(objective='binary', boosting_type = 'goss', n_estimators = 10000, class_weight = 'balanced')\n",
    "    \n",
    "    # Fit the model multiple times to avoid overfitting\n",
    "    for i in range(iterations):\n",
    "\n",
    "        # Split into training and validation set\n",
    "        train_features, valid_features, train_y, valid_y = train_test_split(train, train_labels, test_size = 0.25, random_state = i)\n",
    "\n",
    "        # Train using early stopping\n",
    "        model.fit(train_features, train_y, early_stopping_rounds=100, eval_set = [(valid_features, valid_y)], eval_metric = 'auc', verbose = 200)\n",
    "\n",
    "        # Record the feature importances\n",
    "        feature_importances += model.feature_importances_ / iterations\n",
    "    \n",
    "    # feature_importances = pd.DataFrame({'feature': list(train.columns), 'importance': feature_importances}).sort_values('importance', ascending = False)\n",
    "    \n",
    "    # Find the features with zero importance\n",
    "    # zero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\n",
    "    zero_features = list(np.where(feature_importances==0.0)[0])\n",
    "    print('\\nThere are %d features with 0.0 importance' % len(zero_features))\n",
    "    \n",
    "    return zero_features, feature_importances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "'''\n",
    "class appTrainTransformer_DL(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, X, y=None):\n",
    "        # DAYS_EMPLOYED_anom\n",
    "        X['DAYS_EMPLOYED_anom'] = (X['DAYS_EMPLOYED'] == 365243)\n",
    "        X['DAYS_EMPLOYED'].replace(365243, np.nan, inplace = True)\n",
    "\n",
    "        # Some simple new features (percentages)\n",
    "        X['DAYS_EMPLOYED_PERC'] = X['DAYS_EMPLOYED'] / X['DAYS_BIRTH']\n",
    "        X['INCOME_CREDIT_PERC'] = X['AMT_INCOME_TOTAL'] / X['AMT_CREDIT']\n",
    "        X['INCOME_PER_PERSON'] = X['AMT_INCOME_TOTAL'] / X['CNT_FAM_MEMBERS']\n",
    "        X['ANNUITY_INCOME_PERC'] = X['AMT_ANNUITY'] / X['AMT_INCOME_TOTAL']\n",
    "        X['PAYMENT_RATE'] = X['AMT_ANNUITY'] / X['AMT_CREDIT']\n",
    "        X.replace(np.inf, np.nan, inplace = True)\n",
    "        X.replace(-np.inf, np.nan, inplace = True)\n",
    "\n",
    "        return X\n",
    "\n",
    "'''\n",
    "\n",
    "class appTrainTransformer_DL(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, X, y=None):\n",
    "        # Anoms\n",
    "        X['DAYS_EMPLOYED_anom'] = (X['DAYS_EMPLOYED'] == 365243)\n",
    "        X['ORGANIZATION_TYPE_anom'] = (X['ORGANIZATION_TYPE'] == 'XNA')\n",
    "\n",
    "        X['CODE_GENDER'].replace('XNA', np.nan, inplace=True)\n",
    "        X['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
    "        X['NAME_FAMILY_STATUS'].replace('Unknown', np.nan, inplace=True)\n",
    "        X['ORGANIZATION_TYPE'].replace('XNA', np.nan, inplace=True)\n",
    "        X['DAYS_LAST_PHONE_CHANGE'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "        # Hand crafted features\n",
    "        X['long_employment'] = (X['DAYS_EMPLOYED'] < -2000).astype(int)\n",
    "        X['retirement_age'] = (X['DAYS_BIRTH'] < -14000).astype(int)\n",
    "\n",
    "        X['annuity_income_percentage'] = X['AMT_ANNUITY'] / X['AMT_INCOME_TOTAL']\n",
    "        X['car_to_birth_ratio'] = X['OWN_CAR_AGE'] / X['DAYS_BIRTH']\n",
    "        X['car_to_employ_ratio'] = X['OWN_CAR_AGE'] / X['DAYS_EMPLOYED']\n",
    "        X['children_ratio'] = X['CNT_CHILDREN'] / X['CNT_FAM_MEMBERS']\n",
    "        X['credit_to_annuity_ratio'] = X['AMT_CREDIT'] / X['AMT_ANNUITY']\n",
    "        X['credit_to_goods_ratio'] = X['AMT_CREDIT'] / X['AMT_GOODS_PRICE']\n",
    "        X['credit_to_income_ratio'] = X['AMT_CREDIT'] / X['AMT_INCOME_TOTAL']\n",
    "        X['days_employed_percentage'] = X['DAYS_EMPLOYED'] / X['DAYS_BIRTH']\n",
    "        X['income_credit_percentage'] = X['AMT_INCOME_TOTAL'] / X['AMT_CREDIT']\n",
    "        X['income_per_child'] = X['AMT_INCOME_TOTAL'] / (1 + X['CNT_CHILDREN'])\n",
    "        X['income_per_person'] = X['AMT_INCOME_TOTAL'] / X['CNT_FAM_MEMBERS']\n",
    "        X['payment_rate'] = X['AMT_ANNUITY'] / X['AMT_CREDIT']\n",
    "        X['phone_to_birth_ratio'] = X['DAYS_LAST_PHONE_CHANGE'] / X['DAYS_BIRTH']\n",
    "        X['phone_to_employ_ratio'] = X['DAYS_LAST_PHONE_CHANGE'] / X['DAYS_EMPLOYED']\n",
    "\n",
    "        X['cnt_non_child'] = X['CNT_FAM_MEMBERS'] - X['CNT_CHILDREN']\n",
    "        X['child_to_non_child_ratio'] = X['CNT_CHILDREN'] / X['cnt_non_child']\n",
    "        X['income_per_non_child'] = X['AMT_INCOME_TOTAL'] / X['cnt_non_child']\n",
    "        X['credit_per_person'] = X['AMT_CREDIT'] / X['CNT_FAM_MEMBERS']\n",
    "        X['credit_per_child'] = X['AMT_CREDIT'] / (1 + X['CNT_CHILDREN'])\n",
    "        X['credit_per_non_child'] = X['AMT_CREDIT'] / X['cnt_non_child']\n",
    "\n",
    "        # External sources\n",
    "        X['external_sources_weighted'] = X.EXT_SOURCE_1 * 2 + X.EXT_SOURCE_2 * 3 + X.EXT_SOURCE_3 * 4\n",
    "        for function_name in ['min', 'max', 'sum', 'mean', 'nanmedian']:\n",
    "            X['external_sources_{}'.format(function_name)] = eval('np.{}'.format(function_name))(X[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']], axis=1)\n",
    "\n",
    "        X.replace(np.inf, np.nan, inplace = True)\n",
    "        X.replace(-np.inf, np.nan, inplace = True)\n",
    "\n",
    "        return X\n",
    "\n",
    "class dropTransformer_DL(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "        self.drop = []\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        drop = []\n",
    "        for col in X:\n",
    "            if X[col].isna().sum() > self.rate * X.shape[0]:\n",
    "                drop.append(col)\n",
    "        self.drop = drop\n",
    "        return self\n",
    " \n",
    "    def transform(self, X_copy, y=None):\n",
    "        X_copy.drop(self.drop, axis = 1, inplace = True)\n",
    "        return X_copy\n",
    "\n",
    "\n",
    "\n",
    "class bureauTransformer_DL(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, bureauinfo, y=None):\n",
    "        #numeric\n",
    "        bureau_agg = agg_numeric_DL(bureauinfo.drop(columns = ['SK_ID_BUREAU']), 'SK_ID_CURR', df_name = 'bureau')\n",
    "        #categorial\n",
    "        bureau_counts = count_categorical_DL(bureauinfo, 'SK_ID_CURR', df_name = 'bureau')\n",
    "        bureau_res = bureau_agg.join(bureau_counts, how='left', on='SK_ID_CURR')\n",
    "\n",
    "        return bureau_res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class bbTransformer_DL(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, bureauinfo, bbinfo, y=None):\n",
    "        # Counts of each type of status for each previous loan\n",
    "        bureau_balance_counts = count_categorical_DL(bbinfo, 'SK_ID_BUREAU', df_name = 'bureau_balance')\n",
    "        bureau_balance_agg = agg_numeric_DL(bbinfo, 'SK_ID_BUREAU', df_name = 'bb')\n",
    "        # Dataframe grouped by the loan 把上两个结果merge\n",
    "        bureau_by_loan = bureau_balance_agg.merge(bureau_balance_counts, right_index = True, left_on = 'SK_ID_BUREAU', how = 'outer')\n",
    "\n",
    "        # Merge to include the SK_ID_CURR\n",
    "        bureau_by_loan = bureau_by_loan.merge(bureauinfo[['SK_ID_BUREAU', 'SK_ID_CURR']], on = 'SK_ID_BUREAU', how = 'left')\n",
    "        bureau_balance_by_client = agg_numeric_DL(bureau_by_loan.drop(columns = ['SK_ID_BUREAU']), 'SK_ID_CURR', df_name = 'client')\n",
    "\n",
    "        return bureau_balance_by_client \n",
    "\n",
    "#把app_train join上bureau和bb的结果共得到333features\n",
    "\n",
    "class previousTransformer_DL(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, previnfo, y=None):\n",
    "        '''\n",
    "        prev, cat_cols = one_hot_encoder(previnfo, nan_as_category= True)\n",
    "        # Days 365.243 values -> nan\n",
    "        prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "        prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "        prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "        prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "        prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "        '''\n",
    "        previous_agg = agg_numeric_DL(previnfo, 'SK_ID_CURR', 'previous')\n",
    "        previous_counts = agg_categorical_DL(previnfo, 'SK_ID_CURR', 'previous')\n",
    "        previous_res = previous_agg.join(previous_counts, how='left', on='SK_ID_CURR')\n",
    "\n",
    "        return previous_res\n",
    "\n",
    "\n",
    "class posTransformer_DL(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, posinfo, y=None):\n",
    "        cash_by_client = aggregate_client_DL(posinfo, group_vars = ['SK_ID_PREV', 'SK_ID_CURR'], df_names = ['cash', 'client'])\n",
    "\n",
    "        return cash_by_client #165features\n",
    "\n",
    "class ccTransformer_DL(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, ccinfo, y=None):\n",
    "        credit_by_client = aggregate_client_DL(ccinfo, group_vars = ['SK_ID_PREV', 'SK_ID_CURR'], df_names = ['credit', 'client'])\n",
    "        return credit_by_client #381features\n",
    "\n",
    "class installmentsTransformer_DL(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, insinfo, y=None):\n",
    "        installments_by_client = aggregate_client_DL(insinfo, group_vars = ['SK_ID_PREV', 'SK_ID_CURR'], df_names = ['installments', 'client'])\n",
    "\n",
    "        return installments_by_client #106个features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bur = bureauTransformer_DL().transform(bureau)\n",
    "bb = bbTransformer_DL().transform(bureau,bb)\n",
    "pre = previousTransformer_DL().transform(prev)\n",
    "pos = posTransformer_DL().transform(pos)\n",
    "cc = ccTransformer_DL().transform(cc)\n",
    "ins = installmentsTransformer_DL().transform(ins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_DL(X):\n",
    "    X = X.join(bur, how='left', on='SK_ID_CURR')\n",
    "    X = X.join(bb, how='left', on='SK_ID_CURR')\n",
    "    X = X.join(pre, how='left', on='SK_ID_CURR')\n",
    "    X = X.join(pos, how='left', on='SK_ID_CURR')\n",
    "    X = X.join(cc, how='left', on='SK_ID_CURR')\n",
    "    X = X.join(ins, how='left', on='SK_ID_CURR')\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/dkoalal/anaconda3/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1114: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input=overwrite_input)\n",
      "Number of categorical feature: 18\n",
      "Number of numerical feature: 866\n",
      "Feature engineering result shape (307511, 884)\n"
     ]
    }
   ],
   "source": [
    "X = app_train.copy()\n",
    "X = join_DL(X)\n",
    "X = appTrainTransformer_DL().transform(X)\n",
    "X = dropTransformer_DL(0.7).fit_transform(X)\n",
    "\n",
    "numeric_columns = []\n",
    "category_columns = []\n",
    "\n",
    "for col in X:\n",
    "    \n",
    "    if X[col].dtype == ('object') or X[col].dtype == ('bool'):\n",
    "        category_columns.append(col)\n",
    "    else:\n",
    "        numeric_columns.append(col)\n",
    "\n",
    "print(\"Number of categorical feature:\", len(category_columns))\n",
    "print(\"Number of numerical feature:\", len(numeric_columns))\n",
    "print(\"Feature engineering result shape\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer_DL = Pipeline(steps=[\n",
    "    ('impute_nan', Imputer(missing_values=np.nan,strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "category_transformer_DL = Pipeline(steps=[\n",
    "    ('impute', Imputer(strategy='constant', fill_value='missing')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocesser_DL = ColumnTransformer(transformers=[\n",
    "    ('numeric', numeric_transformer_DL, numeric_columns),\n",
    "    ('category', category_transformer_DL, category_columns)\n",
    "]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dropZeroImportanceTransformer_DL(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, zero_import):\n",
    "        self.zero_import = zero_import\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, X_copy, y=None):\n",
    "        X_copy = np.delete(X_copy,self.zero_import, axis=1)\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_features = [1, 12, 15, 17, 22, 24, 71, 74, 79, 81, 82, 83, 84, 86, 87, 88, 89, 92, 94, 96, 97, 98, 99, 119, 120, 123, 143, 160, 161, 164, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 178, 179, 184, 185, 186, 187, 188, 189, 190, 191, 192, 196, 197, 200, 201, 202, 203, 218, 224, 225, 226, 227, 229, 235, 237, 240, 241, 242, 243, 244, 245, 277, 280, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 303, 304, 305, 307, 308, 309, 310, 311, 313, 314, 315, 317, 318, 319, 320, 321, 323, 325, 326, 327, 328, 329, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 351, 352, 353, 354, 355, 357, 359, 360, 361, 362, 363, 365, 367, 368, 370, 371, 373, 375, 376, 377, 379, 381, 385, 386, 391, 392, 393, 394, 395, 397, 401, 403, 405, 406, 407, 411, 413, 418, 419, 421, 423, 429, 430, 431, 435, 442, 457, 458, 463, 464, 467, 473, 475, 479, 500, 502, 503, 508, 511, 513, 514, 519, 520, 521, 523, 551, 558, 565, 566, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 613, 614, 615, 616, 617, 618, 619, 620, 623, 627, 628, 629, 630, 631, 641, 642, 647, 650, 665, 683, 697, 700, 701, 707, 708, 712, 733, 764, 765, 767, 769, 780, 783, 788, 792, 796, 838, 839, 843, 854, 855, 867, 870, 872, 873, 874, 877, 878, 879, 881, 882, 883, 885, 888, 889, 893, 900, 901, 902, 907, 909, 910, 913, 915, 918, 920, 921, 923, 924, 925, 927, 928, 931, 932, 934, 935, 937, 940, 942, 943, 944, 945, 946, 947, 948, 949, 950, 952, 953, 955, 956, 957, 958, 959, 961, 964, 966, 967, 969, 971, 972, 974, 978, 979, 981, 982, 983, 984, 986, 987, 989, 990, 991, 992, 994, 997, 998, 1000, 1002, 1003, 1004, 1007, 1008, 1009, 1010, 1012, 1013, 1014, 1015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_pre_DL = Pipeline(steps=[\n",
    "    ('app', appTrainTransformer_DL()),\n",
    "    ('drop', dropTransformer_DL(0.7)),\n",
    "    ('preprocesser', preprocesser_DL),\n",
    "    ('feature_selection',dropZeroImportanceTransformer_DL(zero_features))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/dkoalal/anaconda3/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1114: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input=overwrite_input)\n",
      "/Users/dkoalal/anaconda3/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1114: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input=overwrite_input)\n",
      "Train shape after feature selection (307511, 655)\n",
      "Test shape after feature selection (48744, 655)\n"
     ]
    }
   ],
   "source": [
    "app_av_train = app_train.copy()\n",
    "app_av_train = join_DL(app_av_train)\n",
    "\n",
    "app_av_test = app_test.copy()\n",
    "app_av_test = join_DL(app_av_test)\n",
    "\n",
    "app_pre_train = pipeline_pre_DL.fit_transform(app_av_train) # drop 361 features with null importance, left 655 features\n",
    "app_pre_test = pipeline_pre_DL.transform(app_av_test) \n",
    "\n",
    "print(\"Train shape after feature selection\",app_pre_train.shape)\n",
    "print(\"Test shape after feature selection\",app_pre_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.784866\tvalid_0's binary_logloss: 0.505889\n",
      "Early stopping, best iteration is:\n",
      "[135]\tvalid_0's auc: 0.785743\tvalid_0's binary_logloss: 0.522555\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.785504\tvalid_0's binary_logloss: 0.505891\n",
      "Early stopping, best iteration is:\n",
      "[135]\tvalid_0's auc: 0.786607\tvalid_0's binary_logloss: 0.522705\n",
      "\n",
      "There are 361 features with 0.0 importance\n"
     ]
    }
   ],
   "source": [
    "# # Feature selection with lgbm, get 360 features with zero importance\n",
    "# zero_features, feature_importances = identify_zero_importance_features_DL(app_pre_train, train_labels)\n",
    "# app_pre_train = np.delete(app_pre_train,zero_features , axis=1) # 655 features\n",
    "# app_pre_test = np.delete(app_pre_test,zero_features , axis=1)\n",
    "\n",
    "# print(\"Final train shape (after feature selection)\",app_pre_train.shape)\n",
    "# print(\"Final test shape (after feature selection)\",app_pre_test.shape)"
   ]
  },
  {
   "source": [
    "## Train the model and predict"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kfoldlgb_DL(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.clf = []\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        kf = KFold(n_splits=self.k)\n",
    "        X_feature = np.zeros(y.shape[0])\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            gbm = lgb.LGBMClassifier(\n",
    "                nthread=4,\n",
    "                n_estimators=10000,\n",
    "                learning_rate=0.02,\n",
    "                num_leaves=34,\n",
    "                colsample_bytree=0.9497036,\n",
    "                subsample=0.8715623,\n",
    "                max_depth=8,\n",
    "                reg_alpha=0.041545473,\n",
    "                reg_lambda=0.0735294,\n",
    "                min_split_gain=0.0222415,\n",
    "                min_child_weight=39.3259775,\n",
    "                silent=-1,\n",
    "                verbose=-1, )\n",
    "            gbm.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], eval_metric= 'auc', verbose= 50, early_stopping_rounds= 200)\n",
    "            self.clf.append(gbm)\n",
    "            X_feature[test_index] = gbm.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        feature = app_train[['SK_ID_CURR']]\n",
    "        feature['feature_dl'] = X_feature\n",
    "        feature.to_csv('feature_dl.csv', index = False)\n",
    "        return self\n",
    " \n",
    "    def predict_proba(self, X, y=None):\n",
    "        for i in range(self.k):\n",
    "            tmp = self.clf[i].predict_proba(X)\n",
    "            if i!=0:\n",
    "                result = result + tmp/self.k\n",
    "            else:\n",
    "                result = tmp/self.k\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "inary_logloss: 0.205866\tvalid_1's auc: 0.788053\tvalid_1's binary_logloss: 0.238541\n",
      "[1250]\ttraining's auc: 0.869822\ttraining's binary_logloss: 0.204836\tvalid_1's auc: 0.788162\tvalid_1's binary_logloss: 0.238505\n",
      "[1300]\ttraining's auc: 0.872029\ttraining's binary_logloss: 0.203866\tvalid_1's auc: 0.788308\tvalid_1's binary_logloss: 0.238465\n",
      "[1350]\ttraining's auc: 0.87422\ttraining's binary_logloss: 0.202863\tvalid_1's auc: 0.788383\tvalid_1's binary_logloss: 0.238452\n",
      "[1400]\ttraining's auc: 0.876527\ttraining's binary_logloss: 0.201844\tvalid_1's auc: 0.788545\tvalid_1's binary_logloss: 0.238396\n",
      "[1450]\ttraining's auc: 0.878723\ttraining's binary_logloss: 0.200852\tvalid_1's auc: 0.788641\tvalid_1's binary_logloss: 0.238375\n",
      "[1500]\ttraining's auc: 0.881076\ttraining's binary_logloss: 0.199789\tvalid_1's auc: 0.788776\tvalid_1's binary_logloss: 0.238338\n",
      "[1550]\ttraining's auc: 0.883177\ttraining's binary_logloss: 0.198816\tvalid_1's auc: 0.788879\tvalid_1's binary_logloss: 0.23831\n",
      "[1600]\ttraining's auc: 0.885294\ttraining's binary_logloss: 0.197841\tvalid_1's auc: 0.788862\tvalid_1's binary_logloss: 0.23832\n",
      "[1650]\ttraining's auc: 0.887354\ttraining's binary_logloss: 0.196847\tvalid_1's auc: 0.788967\tvalid_1's binary_logloss: 0.2383\n",
      "[1700]\ttraining's auc: 0.889534\ttraining's binary_logloss: 0.195825\tvalid_1's auc: 0.789046\tvalid_1's binary_logloss: 0.238276\n",
      "[1750]\ttraining's auc: 0.891402\ttraining's binary_logloss: 0.194946\tvalid_1's auc: 0.789039\tvalid_1's binary_logloss: 0.238301\n",
      "[1800]\ttraining's auc: 0.893241\ttraining's binary_logloss: 0.194055\tvalid_1's auc: 0.789124\tvalid_1's binary_logloss: 0.238284\n",
      "[1850]\ttraining's auc: 0.895404\ttraining's binary_logloss: 0.193033\tvalid_1's auc: 0.789079\tvalid_1's binary_logloss: 0.238305\n",
      "Early stopping, best iteration is:\n",
      "[1698]\ttraining's auc: 0.889423\ttraining's binary_logloss: 0.195874\tvalid_1's auc: 0.789052\tvalid_1's binary_logloss: 0.238274\n",
      "[LightGBM] [Warning] num_threads is set with nthread=4, will be overridden by n_jobs=-1. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's auc: 0.757698\ttraining's binary_logloss: 0.251612\tvalid_1's auc: 0.74514\tvalid_1's binary_logloss: 0.258122\n",
      "[100]\ttraining's auc: 0.773047\ttraining's binary_logloss: 0.243185\tvalid_1's auc: 0.75684\tvalid_1's binary_logloss: 0.251395\n",
      "[150]\ttraining's auc: 0.785466\ttraining's binary_logloss: 0.238158\tvalid_1's auc: 0.765651\tvalid_1's binary_logloss: 0.247941\n",
      "[200]\ttraining's auc: 0.794516\ttraining's binary_logloss: 0.234559\tvalid_1's auc: 0.771458\tvalid_1's binary_logloss: 0.245826\n",
      "[250]\ttraining's auc: 0.801551\ttraining's binary_logloss: 0.231728\tvalid_1's auc: 0.775667\tvalid_1's binary_logloss: 0.24435\n",
      "[300]\ttraining's auc: 0.807636\ttraining's binary_logloss: 0.229325\tvalid_1's auc: 0.778942\tvalid_1's binary_logloss: 0.243254\n",
      "[350]\ttraining's auc: 0.812943\ttraining's binary_logloss: 0.227228\tvalid_1's auc: 0.781369\tvalid_1's binary_logloss: 0.242431\n",
      "[400]\ttraining's auc: 0.817681\ttraining's binary_logloss: 0.225367\tvalid_1's auc: 0.783041\tvalid_1's binary_logloss: 0.24188\n",
      "[450]\ttraining's auc: 0.822025\ttraining's binary_logloss: 0.223674\tvalid_1's auc: 0.784349\tvalid_1's binary_logloss: 0.241443\n",
      "[500]\ttraining's auc: 0.82612\ttraining's binary_logloss: 0.222099\tvalid_1's auc: 0.785395\tvalid_1's binary_logloss: 0.241094\n",
      "[550]\ttraining's auc: 0.830064\ttraining's binary_logloss: 0.220582\tvalid_1's auc: 0.786042\tvalid_1's binary_logloss: 0.240861\n",
      "[600]\ttraining's auc: 0.833792\ttraining's binary_logloss: 0.219148\tvalid_1's auc: 0.786646\tvalid_1's binary_logloss: 0.240653\n",
      "[650]\ttraining's auc: 0.837307\ttraining's binary_logloss: 0.217743\tvalid_1's auc: 0.787255\tvalid_1's binary_logloss: 0.240462\n",
      "[700]\ttraining's auc: 0.840559\ttraining's binary_logloss: 0.216461\tvalid_1's auc: 0.787781\tvalid_1's binary_logloss: 0.240301\n",
      "[750]\ttraining's auc: 0.843726\ttraining's binary_logloss: 0.215191\tvalid_1's auc: 0.788211\tvalid_1's binary_logloss: 0.240144\n",
      "[800]\ttraining's auc: 0.846797\ttraining's binary_logloss: 0.213948\tvalid_1's auc: 0.78863\tvalid_1's binary_logloss: 0.240005\n",
      "[850]\ttraining's auc: 0.84977\ttraining's binary_logloss: 0.212776\tvalid_1's auc: 0.789033\tvalid_1's binary_logloss: 0.239875\n",
      "[900]\ttraining's auc: 0.852764\ttraining's binary_logloss: 0.211587\tvalid_1's auc: 0.78926\tvalid_1's binary_logloss: 0.239782\n",
      "[950]\ttraining's auc: 0.855417\ttraining's binary_logloss: 0.210476\tvalid_1's auc: 0.789468\tvalid_1's binary_logloss: 0.23971\n",
      "[1000]\ttraining's auc: 0.858089\ttraining's binary_logloss: 0.209364\tvalid_1's auc: 0.789665\tvalid_1's binary_logloss: 0.239647\n",
      "[1050]\ttraining's auc: 0.860789\ttraining's binary_logloss: 0.208251\tvalid_1's auc: 0.789869\tvalid_1's binary_logloss: 0.239588\n",
      "[1100]\ttraining's auc: 0.863402\ttraining's binary_logloss: 0.207185\tvalid_1's auc: 0.789971\tvalid_1's binary_logloss: 0.239562\n",
      "[1150]\ttraining's auc: 0.865778\ttraining's binary_logloss: 0.206159\tvalid_1's auc: 0.790072\tvalid_1's binary_logloss: 0.23953\n",
      "[1200]\ttraining's auc: 0.868341\ttraining's binary_logloss: 0.205051\tvalid_1's auc: 0.790019\tvalid_1's binary_logloss: 0.239541\n",
      "[1250]\ttraining's auc: 0.870791\ttraining's binary_logloss: 0.203989\tvalid_1's auc: 0.789994\tvalid_1's binary_logloss: 0.239545\n",
      "[1300]\ttraining's auc: 0.873191\ttraining's binary_logloss: 0.202933\tvalid_1's auc: 0.789987\tvalid_1's binary_logloss: 0.239528\n",
      "Early stopping, best iteration is:\n",
      "[1143]\ttraining's auc: 0.865412\ttraining's binary_logloss: 0.206315\tvalid_1's auc: 0.79008\tvalid_1's binary_logloss: 0.239522\n",
      "[LightGBM] [Warning] num_threads is set with nthread=4, will be overridden by n_jobs=-1. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's auc: 0.759156\ttraining's binary_logloss: 0.25231\tvalid_1's auc: 0.739829\tvalid_1's binary_logloss: 0.254093\n",
      "[100]\ttraining's auc: 0.774271\ttraining's binary_logloss: 0.243782\tvalid_1's auc: 0.750914\tvalid_1's binary_logloss: 0.247798\n",
      "[150]\ttraining's auc: 0.786576\ttraining's binary_logloss: 0.238718\tvalid_1's auc: 0.759444\tvalid_1's binary_logloss: 0.244595\n",
      "[200]\ttraining's auc: 0.796179\ttraining's binary_logloss: 0.235033\tvalid_1's auc: 0.765257\tvalid_1's binary_logloss: 0.242643\n",
      "[250]\ttraining's auc: 0.803271\ttraining's binary_logloss: 0.232208\tvalid_1's auc: 0.768705\tvalid_1's binary_logloss: 0.241424\n",
      "[300]\ttraining's auc: 0.80929\ttraining's binary_logloss: 0.229819\tvalid_1's auc: 0.771284\tvalid_1's binary_logloss: 0.240557\n",
      "[350]\ttraining's auc: 0.814482\ttraining's binary_logloss: 0.227741\tvalid_1's auc: 0.773078\tvalid_1's binary_logloss: 0.239932\n",
      "[400]\ttraining's auc: 0.819238\ttraining's binary_logloss: 0.225865\tvalid_1's auc: 0.774658\tvalid_1's binary_logloss: 0.239415\n",
      "[450]\ttraining's auc: 0.823636\ttraining's binary_logloss: 0.224148\tvalid_1's auc: 0.775854\tvalid_1's binary_logloss: 0.239029\n",
      "[500]\ttraining's auc: 0.827702\ttraining's binary_logloss: 0.22256\tvalid_1's auc: 0.77691\tvalid_1's binary_logloss: 0.238708\n",
      "[550]\ttraining's auc: 0.831528\ttraining's binary_logloss: 0.221072\tvalid_1's auc: 0.777595\tvalid_1's binary_logloss: 0.238502\n",
      "[600]\ttraining's auc: 0.835068\ttraining's binary_logloss: 0.219669\tvalid_1's auc: 0.77813\tvalid_1's binary_logloss: 0.238327\n",
      "[650]\ttraining's auc: 0.838456\ttraining's binary_logloss: 0.218307\tvalid_1's auc: 0.778652\tvalid_1's binary_logloss: 0.238152\n",
      "[700]\ttraining's auc: 0.841828\ttraining's binary_logloss: 0.216961\tvalid_1's auc: 0.779121\tvalid_1's binary_logloss: 0.237981\n",
      "[750]\ttraining's auc: 0.844885\ttraining's binary_logloss: 0.215733\tvalid_1's auc: 0.779552\tvalid_1's binary_logloss: 0.237838\n",
      "[800]\ttraining's auc: 0.847937\ttraining's binary_logloss: 0.214482\tvalid_1's auc: 0.779813\tvalid_1's binary_logloss: 0.23773\n",
      "[850]\ttraining's auc: 0.850807\ttraining's binary_logloss: 0.213319\tvalid_1's auc: 0.780181\tvalid_1's binary_logloss: 0.237605\n",
      "[900]\ttraining's auc: 0.853444\ttraining's binary_logloss: 0.212223\tvalid_1's auc: 0.780283\tvalid_1's binary_logloss: 0.237542\n",
      "[950]\ttraining's auc: 0.856177\ttraining's binary_logloss: 0.211074\tvalid_1's auc: 0.780411\tvalid_1's binary_logloss: 0.237487\n",
      "[1000]\ttraining's auc: 0.858636\ttraining's binary_logloss: 0.210019\tvalid_1's auc: 0.780539\tvalid_1's binary_logloss: 0.237446\n",
      "[1050]\ttraining's auc: 0.861243\ttraining's binary_logloss: 0.208934\tvalid_1's auc: 0.780608\tvalid_1's binary_logloss: 0.23744\n",
      "[1100]\ttraining's auc: 0.863737\ttraining's binary_logloss: 0.207842\tvalid_1's auc: 0.780757\tvalid_1's binary_logloss: 0.237393\n",
      "[1150]\ttraining's auc: 0.866207\ttraining's binary_logloss: 0.206771\tvalid_1's auc: 0.780804\tvalid_1's binary_logloss: 0.237376\n",
      "[1200]\ttraining's auc: 0.868442\ttraining's binary_logloss: 0.205768\tvalid_1's auc: 0.780929\tvalid_1's binary_logloss: 0.237332\n",
      "[1250]\ttraining's auc: 0.870925\ttraining's binary_logloss: 0.204707\tvalid_1's auc: 0.78096\tvalid_1's binary_logloss: 0.237334\n",
      "[1300]\ttraining's auc: 0.873234\ttraining's binary_logloss: 0.203678\tvalid_1's auc: 0.78097\tvalid_1's binary_logloss: 0.237338\n",
      "[1350]\ttraining's auc: 0.875468\ttraining's binary_logloss: 0.202641\tvalid_1's auc: 0.780891\tvalid_1's binary_logloss: 0.23735\n",
      "[1400]\ttraining's auc: 0.877771\ttraining's binary_logloss: 0.201602\tvalid_1's auc: 0.780994\tvalid_1's binary_logloss: 0.237313\n",
      "Early stopping, best iteration is:\n",
      "[1226]\ttraining's auc: 0.86981\ttraining's binary_logloss: 0.205196\tvalid_1's auc: 0.781032\tvalid_1's binary_logloss: 0.237307\n",
      "[LightGBM] [Warning] num_threads is set with nthread=4, will be overridden by n_jobs=-1. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's auc: 0.756947\ttraining's binary_logloss: 0.252749\tvalid_1's auc: 0.752624\tvalid_1's binary_logloss: 0.252937\n",
      "[100]\ttraining's auc: 0.772487\ttraining's binary_logloss: 0.244274\tvalid_1's auc: 0.763182\tvalid_1's binary_logloss: 0.246026\n",
      "[150]\ttraining's auc: 0.784682\ttraining's binary_logloss: 0.239287\tvalid_1's auc: 0.770903\tvalid_1's binary_logloss: 0.242624\n",
      "[200]\ttraining's auc: 0.79419\ttraining's binary_logloss: 0.235602\tvalid_1's auc: 0.776157\tvalid_1's binary_logloss: 0.240517\n",
      "[250]\ttraining's auc: 0.801518\ttraining's binary_logloss: 0.232714\tvalid_1's auc: 0.779738\tvalid_1's binary_logloss: 0.239129\n",
      "[300]\ttraining's auc: 0.807608\ttraining's binary_logloss: 0.230319\tvalid_1's auc: 0.781957\tvalid_1's binary_logloss: 0.238252\n",
      "[350]\ttraining's auc: 0.812834\ttraining's binary_logloss: 0.228225\tvalid_1's auc: 0.783614\tvalid_1's binary_logloss: 0.237614\n",
      "[400]\ttraining's auc: 0.817693\ttraining's binary_logloss: 0.226328\tvalid_1's auc: 0.784886\tvalid_1's binary_logloss: 0.237142\n",
      "[450]\ttraining's auc: 0.8222\ttraining's binary_logloss: 0.224551\tvalid_1's auc: 0.78593\tvalid_1's binary_logloss: 0.236757\n",
      "[500]\ttraining's auc: 0.826287\ttraining's binary_logloss: 0.222951\tvalid_1's auc: 0.786807\tvalid_1's binary_logloss: 0.236455\n",
      "[550]\ttraining's auc: 0.829944\ttraining's binary_logloss: 0.221473\tvalid_1's auc: 0.787556\tvalid_1's binary_logloss: 0.236193\n",
      "[600]\ttraining's auc: 0.833603\ttraining's binary_logloss: 0.220056\tvalid_1's auc: 0.788039\tvalid_1's binary_logloss: 0.23601\n",
      "[650]\ttraining's auc: 0.836983\ttraining's binary_logloss: 0.21872\tvalid_1's auc: 0.788447\tvalid_1's binary_logloss: 0.235884\n",
      "[700]\ttraining's auc: 0.84029\ttraining's binary_logloss: 0.217409\tvalid_1's auc: 0.788812\tvalid_1's binary_logloss: 0.235768\n",
      "[750]\ttraining's auc: 0.843254\ttraining's binary_logloss: 0.216226\tvalid_1's auc: 0.789096\tvalid_1's binary_logloss: 0.235679\n",
      "[800]\ttraining's auc: 0.8461\ttraining's binary_logloss: 0.215031\tvalid_1's auc: 0.789409\tvalid_1's binary_logloss: 0.235572\n",
      "[850]\ttraining's auc: 0.84893\ttraining's binary_logloss: 0.213869\tvalid_1's auc: 0.789775\tvalid_1's binary_logloss: 0.235461\n",
      "[900]\ttraining's auc: 0.85152\ttraining's binary_logloss: 0.21279\tvalid_1's auc: 0.78996\tvalid_1's binary_logloss: 0.235399\n",
      "[950]\ttraining's auc: 0.85417\ttraining's binary_logloss: 0.211695\tvalid_1's auc: 0.790091\tvalid_1's binary_logloss: 0.235384\n",
      "[1000]\ttraining's auc: 0.856806\ttraining's binary_logloss: 0.21063\tvalid_1's auc: 0.790275\tvalid_1's binary_logloss: 0.235313\n",
      "[1050]\ttraining's auc: 0.859362\ttraining's binary_logloss: 0.209544\tvalid_1's auc: 0.790304\tvalid_1's binary_logloss: 0.23528\n",
      "[1100]\ttraining's auc: 0.861891\ttraining's binary_logloss: 0.208466\tvalid_1's auc: 0.790272\tvalid_1's binary_logloss: 0.235293\n",
      "[1150]\ttraining's auc: 0.864297\ttraining's binary_logloss: 0.207421\tvalid_1's auc: 0.790356\tvalid_1's binary_logloss: 0.235272\n",
      "[1200]\ttraining's auc: 0.866721\ttraining's binary_logloss: 0.206382\tvalid_1's auc: 0.790351\tvalid_1's binary_logloss: 0.235267\n",
      "[1250]\ttraining's auc: 0.86911\ttraining's binary_logloss: 0.205367\tvalid_1's auc: 0.790299\tvalid_1's binary_logloss: 0.235271\n",
      "[1300]\ttraining's auc: 0.871449\ttraining's binary_logloss: 0.204368\tvalid_1's auc: 0.790349\tvalid_1's binary_logloss: 0.23525\n",
      "[1350]\ttraining's auc: 0.873732\ttraining's binary_logloss: 0.203356\tvalid_1's auc: 0.790445\tvalid_1's binary_logloss: 0.235229\n",
      "[1400]\ttraining's auc: 0.876036\ttraining's binary_logloss: 0.202324\tvalid_1's auc: 0.790559\tvalid_1's binary_logloss: 0.235197\n",
      "[1450]\ttraining's auc: 0.87834\ttraining's binary_logloss: 0.201283\tvalid_1's auc: 0.790672\tvalid_1's binary_logloss: 0.235168\n",
      "[1500]\ttraining's auc: 0.880602\ttraining's binary_logloss: 0.200241\tvalid_1's auc: 0.790588\tvalid_1's binary_logloss: 0.235199\n",
      "[1550]\ttraining's auc: 0.882647\ttraining's binary_logloss: 0.199292\tvalid_1's auc: 0.790519\tvalid_1's binary_logloss: 0.235221\n",
      "[1600]\ttraining's auc: 0.885004\ttraining's binary_logloss: 0.19825\tvalid_1's auc: 0.790516\tvalid_1's binary_logloss: 0.235217\n",
      "[1650]\ttraining's auc: 0.887085\ttraining's binary_logloss: 0.197244\tvalid_1's auc: 0.790576\tvalid_1's binary_logloss: 0.235201\n",
      "Early stopping, best iteration is:\n",
      "[1455]\ttraining's auc: 0.878589\ttraining's binary_logloss: 0.201168\tvalid_1's auc: 0.790721\tvalid_1's binary_logloss: 0.235158\n",
      "[LightGBM] [Warning] num_threads is set with nthread=4, will be overridden by n_jobs=-1. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's auc: 0.757849\ttraining's binary_logloss: 0.252952\tvalid_1's auc: 0.750186\tvalid_1's binary_logloss: 0.251405\n",
      "[100]\ttraining's auc: 0.7731\ttraining's binary_logloss: 0.244505\tvalid_1's auc: 0.761551\tvalid_1's binary_logloss: 0.244568\n",
      "[150]\ttraining's auc: 0.785451\ttraining's binary_logloss: 0.239476\tvalid_1's auc: 0.769409\tvalid_1's binary_logloss: 0.241221\n",
      "[200]\ttraining's auc: 0.794595\ttraining's binary_logloss: 0.235867\tvalid_1's auc: 0.774528\tvalid_1's binary_logloss: 0.239195\n",
      "[250]\ttraining's auc: 0.801933\ttraining's binary_logloss: 0.232964\tvalid_1's auc: 0.778064\tvalid_1's binary_logloss: 0.237809\n",
      "[300]\ttraining's auc: 0.807985\ttraining's binary_logloss: 0.230564\tvalid_1's auc: 0.7808\tvalid_1's binary_logloss: 0.236787\n",
      "[350]\ttraining's auc: 0.813221\ttraining's binary_logloss: 0.228471\tvalid_1's auc: 0.782733\tvalid_1's binary_logloss: 0.236048\n",
      "[400]\ttraining's auc: 0.817947\ttraining's binary_logloss: 0.226613\tvalid_1's auc: 0.784253\tvalid_1's binary_logloss: 0.235472\n",
      "[450]\ttraining's auc: 0.822404\ttraining's binary_logloss: 0.224883\tvalid_1's auc: 0.785395\tvalid_1's binary_logloss: 0.235056\n",
      "[500]\ttraining's auc: 0.826422\ttraining's binary_logloss: 0.223337\tvalid_1's auc: 0.786009\tvalid_1's binary_logloss: 0.234807\n",
      "[550]\ttraining's auc: 0.830202\ttraining's binary_logloss: 0.221864\tvalid_1's auc: 0.786755\tvalid_1's binary_logloss: 0.234552\n",
      "[600]\ttraining's auc: 0.83367\ttraining's binary_logloss: 0.220513\tvalid_1's auc: 0.787435\tvalid_1's binary_logloss: 0.234309\n",
      "[650]\ttraining's auc: 0.836987\ttraining's binary_logloss: 0.219219\tvalid_1's auc: 0.787872\tvalid_1's binary_logloss: 0.234161\n",
      "[700]\ttraining's auc: 0.840401\ttraining's binary_logloss: 0.217858\tvalid_1's auc: 0.788476\tvalid_1's binary_logloss: 0.233965\n",
      "[750]\ttraining's auc: 0.843276\ttraining's binary_logloss: 0.216685\tvalid_1's auc: 0.788778\tvalid_1's binary_logloss: 0.233849\n",
      "[800]\ttraining's auc: 0.846082\ttraining's binary_logloss: 0.21556\tvalid_1's auc: 0.789041\tvalid_1's binary_logloss: 0.233724\n",
      "[850]\ttraining's auc: 0.84893\ttraining's binary_logloss: 0.214402\tvalid_1's auc: 0.789233\tvalid_1's binary_logloss: 0.233643\n",
      "[900]\ttraining's auc: 0.851701\ttraining's binary_logloss: 0.213284\tvalid_1's auc: 0.789348\tvalid_1's binary_logloss: 0.233596\n",
      "[950]\ttraining's auc: 0.854379\ttraining's binary_logloss: 0.212161\tvalid_1's auc: 0.78956\tvalid_1's binary_logloss: 0.233529\n",
      "[1000]\ttraining's auc: 0.857014\ttraining's binary_logloss: 0.211089\tvalid_1's auc: 0.78972\tvalid_1's binary_logloss: 0.233466\n",
      "[1050]\ttraining's auc: 0.859624\ttraining's binary_logloss: 0.209996\tvalid_1's auc: 0.789751\tvalid_1's binary_logloss: 0.233444\n",
      "[1100]\ttraining's auc: 0.862129\ttraining's binary_logloss: 0.208941\tvalid_1's auc: 0.789871\tvalid_1's binary_logloss: 0.233395\n",
      "[1150]\ttraining's auc: 0.864598\ttraining's binary_logloss: 0.207872\tvalid_1's auc: 0.790208\tvalid_1's binary_logloss: 0.233287\n",
      "[1200]\ttraining's auc: 0.867082\ttraining's binary_logloss: 0.206788\tvalid_1's auc: 0.790391\tvalid_1's binary_logloss: 0.233214\n",
      "[1250]\ttraining's auc: 0.869479\ttraining's binary_logloss: 0.205735\tvalid_1's auc: 0.790541\tvalid_1's binary_logloss: 0.233159\n",
      "[1300]\ttraining's auc: 0.871916\ttraining's binary_logloss: 0.204631\tvalid_1's auc: 0.790636\tvalid_1's binary_logloss: 0.233126\n",
      "[1350]\ttraining's auc: 0.874157\ttraining's binary_logloss: 0.20363\tvalid_1's auc: 0.790761\tvalid_1's binary_logloss: 0.233096\n",
      "[1400]\ttraining's auc: 0.876286\ttraining's binary_logloss: 0.202665\tvalid_1's auc: 0.790892\tvalid_1's binary_logloss: 0.233044\n",
      "[1450]\ttraining's auc: 0.878366\ttraining's binary_logloss: 0.201709\tvalid_1's auc: 0.790954\tvalid_1's binary_logloss: 0.233018\n",
      "[1500]\ttraining's auc: 0.880425\ttraining's binary_logloss: 0.200758\tvalid_1's auc: 0.79113\tvalid_1's binary_logloss: 0.232963\n",
      "[1550]\ttraining's auc: 0.882604\ttraining's binary_logloss: 0.199734\tvalid_1's auc: 0.791149\tvalid_1's binary_logloss: 0.23294\n",
      "[1600]\ttraining's auc: 0.884645\ttraining's binary_logloss: 0.198777\tvalid_1's auc: 0.791155\tvalid_1's binary_logloss: 0.232932\n",
      "[1650]\ttraining's auc: 0.886628\ttraining's binary_logloss: 0.197833\tvalid_1's auc: 0.791253\tvalid_1's binary_logloss: 0.232883\n",
      "[1700]\ttraining's auc: 0.888788\ttraining's binary_logloss: 0.19682\tvalid_1's auc: 0.791445\tvalid_1's binary_logloss: 0.232803\n",
      "[1750]\ttraining's auc: 0.890619\ttraining's binary_logloss: 0.195937\tvalid_1's auc: 0.791454\tvalid_1's binary_logloss: 0.232797\n",
      "[1800]\ttraining's auc: 0.892688\ttraining's binary_logloss: 0.194967\tvalid_1's auc: 0.79162\tvalid_1's binary_logloss: 0.232739\n",
      "[1850]\ttraining's auc: 0.894692\ttraining's binary_logloss: 0.19397\tvalid_1's auc: 0.791634\tvalid_1's binary_logloss: 0.232723\n",
      "[1900]\ttraining's auc: 0.89658\ttraining's binary_logloss: 0.19303\tvalid_1's auc: 0.791703\tvalid_1's binary_logloss: 0.232684\n",
      "[1950]\ttraining's auc: 0.898268\ttraining's binary_logloss: 0.192163\tvalid_1's auc: 0.791791\tvalid_1's binary_logloss: 0.232641\n",
      "[2000]\ttraining's auc: 0.899978\ttraining's binary_logloss: 0.19132\tvalid_1's auc: 0.791783\tvalid_1's binary_logloss: 0.232633\n",
      "[2050]\ttraining's auc: 0.901827\ttraining's binary_logloss: 0.190377\tvalid_1's auc: 0.791815\tvalid_1's binary_logloss: 0.232636\n",
      "[2100]\ttraining's auc: 0.903502\ttraining's binary_logloss: 0.189508\tvalid_1's auc: 0.791685\tvalid_1's binary_logloss: 0.232654\n",
      "[2150]\ttraining's auc: 0.905238\ttraining's binary_logloss: 0.188611\tvalid_1's auc: 0.791701\tvalid_1's binary_logloss: 0.232656\n",
      "Early stopping, best iteration is:\n",
      "[1961]\ttraining's auc: 0.898637\ttraining's binary_logloss: 0.191979\tvalid_1's auc: 0.791864\tvalid_1's binary_logloss: 0.232618\n",
      "/Users/dkoalal/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "kfoldlgb_DL(k=6)"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "clf = kfoldlgb_DL(6)\n",
    "clf.fit(app_pre_train, train_labels)"
   ]
  },
  {
   "source": [
    "## For kaggle submission"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = clf.predict_proba(app_pre_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/dkoalal/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "log_reg_pred = Y[:, 1]\n",
    "submit = app_test[['SK_ID_CURR']]\n",
    "submit['TARGET'] = log_reg_pred\n",
    "submit.to_csv('baseline8.csv', index = False)"
   ]
  },
  {
   "source": [
    "## Save the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['./preprocess_DL.joblib']"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "joblib.dump(pipeline_pre_DL, './preprocess_DL.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['./clf_DL.joblib']"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "joblib.dump(clf, './clf_DL.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
