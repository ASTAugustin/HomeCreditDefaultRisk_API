{
 "cells": [
  {
   "source": [
    "## 读入分析数据"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T09:54:03.520380Z",
     "start_time": "2020-11-02T09:54:03.516392Z"
    }
   },
   "outputs": [],
   "source": [
    "# numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# sklearn preprocessing for dealing with categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, StandardScaler\n",
    "from sklearn.impute import SimpleImputer as Imputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# File system manangement\n",
    "import os\n",
    "\n",
    "import joblib\n",
    "import requests\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T09:54:37.378241Z",
     "start_time": "2020-11-02T09:54:37.370262Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['application_test.csv', 'application_train.csv', 'bureau.csv', 'bureau_balance.csv', 'credit_card_balance.csv', 'HomeCredit_columns_description.csv', 'installments_payments.csv', 'POS_CASH_balance.csv', 'previous_application.csv', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "# List files available\n",
    "print(os.listdir(\"./input/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T09:55:00.329493Z",
     "start_time": "2020-11-02T09:54:56.606423Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data shape:  (307511, 122)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   SK_ID_CURR  TARGET NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR  \\\n",
       "0      100002       1         Cash loans           M            N   \n",
       "1      100003       0         Cash loans           F            N   \n",
       "2      100004       0    Revolving loans           M            Y   \n",
       "3      100006       0         Cash loans           F            N   \n",
       "4      100007       0         Cash loans           M            N   \n",
       "\n",
       "  FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
       "0               Y             0          202500.0    406597.5      24700.5   \n",
       "1               N             0          270000.0   1293502.5      35698.5   \n",
       "2               Y             0           67500.0    135000.0       6750.0   \n",
       "3               Y             0          135000.0    312682.5      29686.5   \n",
       "4               Y             0          121500.0    513000.0      21865.5   \n",
       "\n",
       "   ...  FLAG_DOCUMENT_18 FLAG_DOCUMENT_19 FLAG_DOCUMENT_20 FLAG_DOCUMENT_21  \\\n",
       "0  ...                 0                0                0                0   \n",
       "1  ...                 0                0                0                0   \n",
       "2  ...                 0                0                0                0   \n",
       "3  ...                 0                0                0                0   \n",
       "4  ...                 0                0                0                0   \n",
       "\n",
       "  AMT_REQ_CREDIT_BUREAU_HOUR AMT_REQ_CREDIT_BUREAU_DAY  \\\n",
       "0                        0.0                       0.0   \n",
       "1                        0.0                       0.0   \n",
       "2                        0.0                       0.0   \n",
       "3                        NaN                       NaN   \n",
       "4                        0.0                       0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_WEEK  AMT_REQ_CREDIT_BUREAU_MON  \\\n",
       "0                         0.0                        0.0   \n",
       "1                         0.0                        0.0   \n",
       "2                         0.0                        0.0   \n",
       "3                         NaN                        NaN   \n",
       "4                         0.0                        0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_QRT  AMT_REQ_CREDIT_BUREAU_YEAR  \n",
       "0                        0.0                         1.0  \n",
       "1                        0.0                         0.0  \n",
       "2                        0.0                         0.0  \n",
       "3                        NaN                         NaN  \n",
       "4                        0.0                         0.0  \n",
       "\n",
       "[5 rows x 122 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SK_ID_CURR</th>\n      <th>TARGET</th>\n      <th>NAME_CONTRACT_TYPE</th>\n      <th>CODE_GENDER</th>\n      <th>FLAG_OWN_CAR</th>\n      <th>FLAG_OWN_REALTY</th>\n      <th>CNT_CHILDREN</th>\n      <th>AMT_INCOME_TOTAL</th>\n      <th>AMT_CREDIT</th>\n      <th>AMT_ANNUITY</th>\n      <th>...</th>\n      <th>FLAG_DOCUMENT_18</th>\n      <th>FLAG_DOCUMENT_19</th>\n      <th>FLAG_DOCUMENT_20</th>\n      <th>FLAG_DOCUMENT_21</th>\n      <th>AMT_REQ_CREDIT_BUREAU_HOUR</th>\n      <th>AMT_REQ_CREDIT_BUREAU_DAY</th>\n      <th>AMT_REQ_CREDIT_BUREAU_WEEK</th>\n      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n      <th>AMT_REQ_CREDIT_BUREAU_YEAR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100002</td>\n      <td>1</td>\n      <td>Cash loans</td>\n      <td>M</td>\n      <td>N</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>202500.0</td>\n      <td>406597.5</td>\n      <td>24700.5</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>100003</td>\n      <td>0</td>\n      <td>Cash loans</td>\n      <td>F</td>\n      <td>N</td>\n      <td>N</td>\n      <td>0</td>\n      <td>270000.0</td>\n      <td>1293502.5</td>\n      <td>35698.5</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>100004</td>\n      <td>0</td>\n      <td>Revolving loans</td>\n      <td>M</td>\n      <td>Y</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>67500.0</td>\n      <td>135000.0</td>\n      <td>6750.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>100006</td>\n      <td>0</td>\n      <td>Cash loans</td>\n      <td>F</td>\n      <td>N</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>135000.0</td>\n      <td>312682.5</td>\n      <td>29686.5</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>100007</td>\n      <td>0</td>\n      <td>Cash loans</td>\n      <td>M</td>\n      <td>N</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>121500.0</td>\n      <td>513000.0</td>\n      <td>21865.5</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 122 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Training data\n",
    "app_train = pd.read_csv('./input/application_train.csv')\n",
    "app_test = pd.read_csv('./input/application_test.csv')\n",
    "bureau = pd.read_csv('./input/bureau.csv')\n",
    "bb = pd.read_csv('./input/bureau_balance.csv')\n",
    "prev = pd.read_csv('./input/previous_application.csv')\n",
    "cc = pd.read_csv('./input/credit_card_balance.csv')\n",
    "ins = pd.read_csv('./input/installments_payments.csv')\n",
    "pos = pd.read_csv('./input/POS_CASH_balance.csv')\n",
    "\n",
    "\n",
    "dic={}\n",
    "dic['bur'] = bureau\n",
    "dic['bb'] = bb\n",
    "dic['pre'] = prev\n",
    "dic['cc'] = cc\n",
    "dic['ins'] = ins\n",
    "dic['pos'] = pos\n",
    "\n",
    "print('Training data shape: ', app_train.shape)\n",
    "app_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Features shape:  (307511, 121)\nTesting Features shape:  (48744, 121)\n"
     ]
    }
   ],
   "source": [
    "train_labels = app_train['TARGET']\n",
    "\n",
    "app_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n",
    "print('Training Features shape: ', app_train.shape)\n",
    "print('Testing Features shape: ', app_test.shape)"
   ]
  },
  {
   "source": [
    "## 预处理"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if ((df[col].dtype == 'object') or (df[col].dtype == 'bool'))]\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    " \n",
    "class appTrainTransformer_Lei(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, X, y=None):\n",
    "        # DAYS_EMPLOYED_anom\n",
    "        X['DAYS_EMPLOYED_anom'] = (X['DAYS_EMPLOYED'] == 365243)\n",
    "        X['DAYS_EMPLOYED'].replace(365243, np.nan, inplace = True)\n",
    "\n",
    "        # Some simple new features (percentages)\n",
    "        X['DAYS_EMPLOYED_PERC'] = X['DAYS_EMPLOYED'] / X['DAYS_BIRTH']\n",
    "        X['INCOME_CREDIT_PERC'] = X['AMT_INCOME_TOTAL'] / X['AMT_CREDIT']\n",
    "        X['INCOME_PER_PERSON'] = X['AMT_INCOME_TOTAL'] / X['CNT_FAM_MEMBERS']\n",
    "        X['ANNUITY_INCOME_PERC'] = X['AMT_ANNUITY'] / X['AMT_INCOME_TOTAL']\n",
    "        X['PAYMENT_RATE'] = X['AMT_ANNUITY'] / X['AMT_CREDIT']\n",
    "        X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        return X\n",
    "\n",
    "class dropTransformer_Lei(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "        self.drop = []\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        drop = []\n",
    "        for col in X:\n",
    "            if X[col].isna().sum() > self.rate * X.shape[0]:\n",
    "                drop.append(col)\n",
    "        self.drop = drop\n",
    "        return self\n",
    " \n",
    "    def transform(self, X_copy, y=None):\n",
    "        X_copy.drop(self.drop, axis = 1, inplace = True)\n",
    "        X_copy.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        return X_copy\n",
    "\n",
    "class bureauTransformer_Lei(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, bureauinfo, bbinfo, y=None):\n",
    "        # app_train treatment\n",
    "        bb_copy, bb_cat = one_hot_encoder(bbinfo, True)\n",
    "        bureau_copy, bureau_cat = one_hot_encoder(bureauinfo, True)\n",
    "\n",
    "        # bb Treatment\n",
    "        bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
    "        for col in bb_cat:\n",
    "            bb_aggregations[col] = ['mean']\n",
    "        bb_agg = bb_copy.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "        bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "        bureau_copy = bureau_copy.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "        bureau_copy.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n",
    "\n",
    "        ## bureau Treatment\n",
    "        # Bureau and bureau_balance numeric features\n",
    "        num_aggregations = {\n",
    "            'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
    "            'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "            'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "            'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "            'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "            'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "            'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "            'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "            'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "            'AMT_ANNUITY': ['max', 'mean'],\n",
    "            'CNT_CREDIT_PROLONG': ['sum'],\n",
    "            'MONTHS_BALANCE_MIN': ['min'],\n",
    "            'MONTHS_BALANCE_MAX': ['max'],\n",
    "            'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
    "        }\n",
    "\n",
    "        # Bureau and bureau_balance categorical features\n",
    "        cat_aggregations = {}\n",
    "        for cat in bureau_cat: \n",
    "            cat_aggregations[cat] = ['mean']\n",
    "        for cat in bb_cat: \n",
    "            cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "\n",
    "        bureau_agg = bureau_copy.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "        bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "\n",
    "        # Bureau: Active credits - using only numerical aggregations\n",
    "        active = bureau_copy[bureau_copy['CREDIT_ACTIVE_Active'] == 1]\n",
    "        active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "        active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "        bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "\n",
    "        # Bureau: Closed credits - using only numerical aggregations\n",
    "        closed = bureau_copy[bureau_copy['CREDIT_ACTIVE_Closed'] == 1]\n",
    "        closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "        closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "        bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "        return bureau_agg\n",
    "\n",
    "class previousTransformer_Lei(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, previnfo, y=None):\n",
    "\n",
    "        prev, cat_cols = one_hot_encoder(previnfo, nan_as_category= True)\n",
    "        # Days 365.243 values -> nan\n",
    "        prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "        prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "        prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "        prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "        prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "\n",
    "        # Add feature: value ask / value received percentage\n",
    "        prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "        # Previous applications numeric features\n",
    "        num_aggregations = {\n",
    "            'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "            'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
    "            'AMT_CREDIT': ['min', 'max', 'mean'],\n",
    "            'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "            'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "            'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "            'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "            'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "            'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "            'CNT_PAYMENT': ['mean', 'sum'],\n",
    "        }\n",
    "        # Previous applications categorical features\n",
    "        cat_aggregations = {}\n",
    "        for cat in cat_cols:\n",
    "            cat_aggregations[cat] = ['mean']\n",
    "        \n",
    "        prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "        prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "        # Previous Applications: Approved Applications - only numerical features\n",
    "        approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "        approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "        approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "        prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "        # Previous Applications: Refused Applications - only numerical features\n",
    "        refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "        refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "        refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "        prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "        return prev_agg\n",
    "\n",
    "class posTransformer_Lei(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, posinfo, y=None):\n",
    "\n",
    "        pos, cat_cols = one_hot_encoder(posinfo, True)\n",
    "        # Features\n",
    "        aggregations = {\n",
    "            'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
    "            'SK_DPD': ['max', 'mean'],\n",
    "            'SK_DPD_DEF': ['max', 'mean']\n",
    "        }\n",
    "        for cat in cat_cols:\n",
    "            aggregations[cat] = ['mean']\n",
    "        \n",
    "        pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "        pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "        # Count pos cash accounts\n",
    "        pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
    "\n",
    "        return pos_agg\n",
    "    \n",
    "class installmentsTransformer_Lei(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, insinfo, y=None):\n",
    "        ins, cat_cols = one_hot_encoder(insinfo, True)\n",
    "\n",
    "        # Percentage and difference paid in each installment (amount paid and installment value)\n",
    "        ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "        ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "        # Days past due and days before due (no negative values)\n",
    "        ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "        ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "        ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "        ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "        # Features: Perform aggregations\n",
    "        aggregations = {\n",
    "            'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "            'DPD': ['max', 'mean', 'sum'],\n",
    "            'DBD': ['max', 'mean', 'sum'],\n",
    "            'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "            'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "            'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
    "            'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "            'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n",
    "        }\n",
    "        for cat in cat_cols:\n",
    "            aggregations[cat] = ['mean']\n",
    "        ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "        ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "        # Count installments accounts\n",
    "        ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "        return ins_agg\n",
    "\n",
    "class ccTransformer_Lei(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, ccinfo, y=None):\n",
    "        cc, cat_cols = one_hot_encoder(ccinfo, True)\n",
    "        # General aggregations\n",
    "        cc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n",
    "        cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "        cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "        # Count credit card lines\n",
    "        cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "        return cc_agg\n",
    "\n",
    "class dropImportanceTransformer_Lei(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, di):\n",
    "        self.di = di\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, X, y=None):\n",
    "        return np.delete(X, self.di, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bur = bureauTransformer_Lei().transform(dic['bur'], dic['bb'])\n",
    "pre = previousTransformer_Lei().transform(dic['pre'])\n",
    "pos = posTransformer_Lei().transform(dic['pos'])\n",
    "cc = ccTransformer_Lei().transform(dic['cc'])\n",
    "ins = installmentsTransformer_Lei().transform(dic['ins'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_Lei(X):\n",
    "    X = X.join(bur, how='left', on='SK_ID_CURR')\n",
    "    X = X.join(pre, how='left', on='SK_ID_CURR')\n",
    "    X = X.join(pos, how='left', on='SK_ID_CURR')\n",
    "    X = X.join(cc, how='left', on='SK_ID_CURR')\n",
    "    X = X.join(ins, how='left', on='SK_ID_CURR')\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of categorical feature: 17\nNumber of numerical feature: 484\n"
     ]
    }
   ],
   "source": [
    "X = app_train.copy()\n",
    "X = join_Lei(X)\n",
    "X = appTrainTransformer_Lei().transform(X)\n",
    "X = dropTransformer_Lei(0.7).fit_transform(X)\n",
    "\n",
    "\n",
    "numeric_columns = []\n",
    "category_columns = []\n",
    "\n",
    "for col in X:\n",
    "    \n",
    "    if X[col].dtype == ('object') or X[col].dtype == ('bool'):\n",
    "        category_columns.append(col)\n",
    "    else:\n",
    "        numeric_columns.append(col)\n",
    "\n",
    "print(\"Number of categorical feature:\", len(category_columns))\n",
    "print(\"Number of numerical feature:\", len(numeric_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer_Lei = Pipeline(steps=[\n",
    "    ('impute_nan', Imputer(missing_values=np.nan,strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "category_transformer_Lei = Pipeline(steps=[\n",
    "    ('impute', Imputer(strategy='constant', fill_value='missing')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocesser_Lei = ColumnTransformer(transformers=[\n",
    "    ('numeric', numeric_transformer_Lei, numeric_columns),\n",
    "    ('category', category_transformer_Lei, category_columns)\n",
    "]) \n",
    "\n",
    "pipeline_Lei = Pipeline(steps=[\n",
    "    ('app', appTrainTransformer_Lei()),\n",
    "    ('drop', dropTransformer_Lei(0.7)),\n",
    "    ('preprocesser', preprocesser_Lei)\n",
    "])"
   ]
  },
  {
   "source": [
    "## 建造Pipeline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def kfoldlgbfeature(X, y, k):\n",
    "#     kf = KFold(n_splits=k)\n",
    "#     X_feature = np.zeros(y.shape[0])\n",
    "#     for train_index, test_index in kf.split(X):\n",
    "#         X_train, X_test = X[train_index], X[test_index]\n",
    "#         y_train, y_test = y[train_index], y[test_index]\n",
    "#         gbm = lgb.LGBMClassifier(\n",
    "#             nthread=4,\n",
    "#             n_estimators=10000,\n",
    "#             learning_rate=0.02,\n",
    "#             num_leaves=34,\n",
    "#             colsample_bytree=0.9497036,\n",
    "#             subsample=0.8715623,\n",
    "#             max_depth=8,\n",
    "#             reg_alpha=0.041545473,\n",
    "#             reg_lambda=0.0735294,\n",
    "#             min_split_gain=0.0222415,\n",
    "#             min_child_weight=39.3259775,\n",
    "#             silent=-1,\n",
    "#             verbose=-1, )\n",
    "#         gbm.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], eval_metric= 'auc', verbose= 50, early_stopping_rounds= 200)\n",
    "#         X_feature[test_index] = gbm.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#     feature = app_train[['SK_ID_CURR']]\n",
    "#     feature['feature_lei'] = X_feature\n",
    "#     feature.to_csv('feature_lei.csv', index = False)\n",
    "#     return X_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kfoldlgb(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.clf = []\n",
    "        return None\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        kf = KFold(n_splits=self.k)\n",
    "        X_feature = np.zeros(y.shape[0])\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            gbm = lgb.LGBMClassifier(\n",
    "                nthread=4,\n",
    "                n_estimators=10000,\n",
    "                learning_rate=0.02,\n",
    "                num_leaves=34,\n",
    "                colsample_bytree=0.9497036,\n",
    "                subsample=0.8715623,\n",
    "                max_depth=8,\n",
    "                reg_alpha=0.041545473,\n",
    "                reg_lambda=0.0735294,\n",
    "                min_split_gain=0.0222415,\n",
    "                min_child_weight=39.3259775,\n",
    "                silent=-1,\n",
    "                verbose=-1, )\n",
    "            gbm.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], eval_metric= 'auc', verbose= 50, early_stopping_rounds= 200)\n",
    "            self.clf.append(gbm)\n",
    "            X_feature[test_index] = gbm.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        feature = app_train[['SK_ID_CURR']]\n",
    "        feature['feature_lei'] = X_feature\n",
    "        feature.to_csv('feature_lei.csv', index = False)\n",
    "        return self\n",
    " \n",
    "    def predict_proba(self, X, y=None):\n",
    "        for i in range(self.k):\n",
    "            tmp = self.clf[i].predict_proba(X)\n",
    "            if i!=0:\n",
    "                result = result + tmp/self.k\n",
    "            else:\n",
    "                result = tmp/self.k\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(307511, 632)\n(48744, 632)\n"
     ]
    }
   ],
   "source": [
    "app_av_train = app_train.copy()\n",
    "app_av_train = join_Lei(app_av_train)\n",
    "\n",
    "app_av_test = app_test.copy()\n",
    "app_av_test = join_Lei(app_av_test)\n",
    "\n",
    "app_pre_train = pipeline_Lei.fit_transform(app_av_train)\n",
    "app_pre_test = pipeline_Lei.transform(app_av_test)\n",
    "\n",
    "print(app_pre_train.shape)\n",
    "print(app_pre_test.shape)"
   ]
  },
  {
   "source": [
    "## Prediction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "_1's auc: 0.777041\tvalid_1's binary_logloss: 0.239113\n",
      "[750]\ttraining's auc: 0.840734\ttraining's binary_logloss: 0.217581\tvalid_1's auc: 0.777596\tvalid_1's binary_logloss: 0.238928\n",
      "[800]\ttraining's auc: 0.84357\ttraining's binary_logloss: 0.216436\tvalid_1's auc: 0.778006\tvalid_1's binary_logloss: 0.238813\n",
      "[850]\ttraining's auc: 0.846498\ttraining's binary_logloss: 0.215259\tvalid_1's auc: 0.778326\tvalid_1's binary_logloss: 0.238713\n",
      "[900]\ttraining's auc: 0.849124\ttraining's binary_logloss: 0.214169\tvalid_1's auc: 0.778396\tvalid_1's binary_logloss: 0.238669\n",
      "[950]\ttraining's auc: 0.851802\ttraining's binary_logloss: 0.213081\tvalid_1's auc: 0.778536\tvalid_1's binary_logloss: 0.238609\n",
      "[1000]\ttraining's auc: 0.854334\ttraining's binary_logloss: 0.21205\tvalid_1's auc: 0.77874\tvalid_1's binary_logloss: 0.238528\n",
      "[1050]\ttraining's auc: 0.856803\ttraining's binary_logloss: 0.210994\tvalid_1's auc: 0.778887\tvalid_1's binary_logloss: 0.238471\n",
      "[1100]\ttraining's auc: 0.859348\ttraining's binary_logloss: 0.209941\tvalid_1's auc: 0.779077\tvalid_1's binary_logloss: 0.238406\n",
      "[1150]\ttraining's auc: 0.861787\ttraining's binary_logloss: 0.208931\tvalid_1's auc: 0.779043\tvalid_1's binary_logloss: 0.238416\n",
      "[1200]\ttraining's auc: 0.86407\ttraining's binary_logloss: 0.207943\tvalid_1's auc: 0.779173\tvalid_1's binary_logloss: 0.238372\n",
      "[1250]\ttraining's auc: 0.86643\ttraining's binary_logloss: 0.206934\tvalid_1's auc: 0.779257\tvalid_1's binary_logloss: 0.238339\n",
      "[1300]\ttraining's auc: 0.868816\ttraining's binary_logloss: 0.205899\tvalid_1's auc: 0.779134\tvalid_1's binary_logloss: 0.238369\n",
      "[1350]\ttraining's auc: 0.870988\ttraining's binary_logloss: 0.204956\tvalid_1's auc: 0.779203\tvalid_1's binary_logloss: 0.238337\n",
      "[1400]\ttraining's auc: 0.873214\ttraining's binary_logloss: 0.203987\tvalid_1's auc: 0.779193\tvalid_1's binary_logloss: 0.238323\n",
      "[1450]\ttraining's auc: 0.875356\ttraining's binary_logloss: 0.203028\tvalid_1's auc: 0.779315\tvalid_1's binary_logloss: 0.23832\n",
      "[1500]\ttraining's auc: 0.877426\ttraining's binary_logloss: 0.202072\tvalid_1's auc: 0.779217\tvalid_1's binary_logloss: 0.238353\n",
      "[1550]\ttraining's auc: 0.87944\ttraining's binary_logloss: 0.201134\tvalid_1's auc: 0.779219\tvalid_1's binary_logloss: 0.238354\n",
      "[1600]\ttraining's auc: 0.881375\ttraining's binary_logloss: 0.20024\tvalid_1's auc: 0.77919\tvalid_1's binary_logloss: 0.238364\n",
      "Early stopping, best iteration is:\n",
      "[1438]\ttraining's auc: 0.87481\ttraining's binary_logloss: 0.203269\tvalid_1's auc: 0.779373\tvalid_1's binary_logloss: 0.238299\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's auc: 0.751686\ttraining's binary_logloss: 0.255074\tvalid_1's auc: 0.740553\tvalid_1's binary_logloss: 0.257663\n",
      "[100]\ttraining's auc: 0.770459\ttraining's binary_logloss: 0.246099\tvalid_1's auc: 0.754948\tvalid_1's binary_logloss: 0.250228\n",
      "[150]\ttraining's auc: 0.78358\ttraining's binary_logloss: 0.240603\tvalid_1's auc: 0.764172\tvalid_1's binary_logloss: 0.246316\n",
      "[200]\ttraining's auc: 0.792932\ttraining's binary_logloss: 0.236753\tvalid_1's auc: 0.770806\tvalid_1's binary_logloss: 0.243753\n",
      "[250]\ttraining's auc: 0.800063\ttraining's binary_logloss: 0.233786\tvalid_1's auc: 0.774879\tvalid_1's binary_logloss: 0.242082\n",
      "[300]\ttraining's auc: 0.805907\ttraining's binary_logloss: 0.231362\tvalid_1's auc: 0.777503\tvalid_1's binary_logloss: 0.240975\n",
      "[350]\ttraining's auc: 0.811037\ttraining's binary_logloss: 0.229257\tvalid_1's auc: 0.779554\tvalid_1's binary_logloss: 0.240157\n",
      "[400]\ttraining's auc: 0.81546\ttraining's binary_logloss: 0.227438\tvalid_1's auc: 0.781294\tvalid_1's binary_logloss: 0.239466\n",
      "[450]\ttraining's auc: 0.819565\ttraining's binary_logloss: 0.225787\tvalid_1's auc: 0.782583\tvalid_1's binary_logloss: 0.238971\n",
      "[500]\ttraining's auc: 0.82325\ttraining's binary_logloss: 0.2243\tvalid_1's auc: 0.783646\tvalid_1's binary_logloss: 0.238567\n",
      "[550]\ttraining's auc: 0.827033\ttraining's binary_logloss: 0.222827\tvalid_1's auc: 0.784797\tvalid_1's binary_logloss: 0.238149\n",
      "[600]\ttraining's auc: 0.830507\ttraining's binary_logloss: 0.221473\tvalid_1's auc: 0.785662\tvalid_1's binary_logloss: 0.23784\n",
      "[650]\ttraining's auc: 0.83364\ttraining's binary_logloss: 0.220239\tvalid_1's auc: 0.785971\tvalid_1's binary_logloss: 0.237658\n",
      "[700]\ttraining's auc: 0.836875\ttraining's binary_logloss: 0.218973\tvalid_1's auc: 0.786568\tvalid_1's binary_logloss: 0.237457\n",
      "[750]\ttraining's auc: 0.839893\ttraining's binary_logloss: 0.217758\tvalid_1's auc: 0.786871\tvalid_1's binary_logloss: 0.237316\n",
      "[800]\ttraining's auc: 0.84279\ttraining's binary_logloss: 0.216599\tvalid_1's auc: 0.787259\tvalid_1's binary_logloss: 0.237179\n",
      "[850]\ttraining's auc: 0.845619\ttraining's binary_logloss: 0.215458\tvalid_1's auc: 0.787626\tvalid_1's binary_logloss: 0.23705\n",
      "[900]\ttraining's auc: 0.848434\ttraining's binary_logloss: 0.2143\tvalid_1's auc: 0.787863\tvalid_1's binary_logloss: 0.23696\n",
      "[950]\ttraining's auc: 0.851057\ttraining's binary_logloss: 0.213202\tvalid_1's auc: 0.788378\tvalid_1's binary_logloss: 0.23681\n",
      "[1000]\ttraining's auc: 0.853674\ttraining's binary_logloss: 0.212131\tvalid_1's auc: 0.788587\tvalid_1's binary_logloss: 0.236743\n",
      "[1050]\ttraining's auc: 0.85624\ttraining's binary_logloss: 0.211051\tvalid_1's auc: 0.788902\tvalid_1's binary_logloss: 0.236622\n",
      "[1100]\ttraining's auc: 0.858707\ttraining's binary_logloss: 0.210028\tvalid_1's auc: 0.789143\tvalid_1's binary_logloss: 0.236554\n",
      "[1150]\ttraining's auc: 0.861137\ttraining's binary_logloss: 0.209023\tvalid_1's auc: 0.789417\tvalid_1's binary_logloss: 0.236462\n",
      "[1200]\ttraining's auc: 0.863536\ttraining's binary_logloss: 0.208012\tvalid_1's auc: 0.789712\tvalid_1's binary_logloss: 0.236404\n",
      "[1250]\ttraining's auc: 0.865689\ttraining's binary_logloss: 0.207078\tvalid_1's auc: 0.789679\tvalid_1's binary_logloss: 0.236428\n",
      "[1300]\ttraining's auc: 0.868031\ttraining's binary_logloss: 0.206053\tvalid_1's auc: 0.789777\tvalid_1's binary_logloss: 0.236399\n",
      "[1350]\ttraining's auc: 0.870188\ttraining's binary_logloss: 0.205123\tvalid_1's auc: 0.789961\tvalid_1's binary_logloss: 0.236345\n",
      "[1400]\ttraining's auc: 0.872133\ttraining's binary_logloss: 0.204239\tvalid_1's auc: 0.789975\tvalid_1's binary_logloss: 0.236342\n",
      "[1450]\ttraining's auc: 0.874304\ttraining's binary_logloss: 0.203256\tvalid_1's auc: 0.790111\tvalid_1's binary_logloss: 0.236287\n",
      "[1500]\ttraining's auc: 0.876351\ttraining's binary_logloss: 0.202329\tvalid_1's auc: 0.79026\tvalid_1's binary_logloss: 0.236218\n",
      "[1550]\ttraining's auc: 0.878264\ttraining's binary_logloss: 0.201432\tvalid_1's auc: 0.790352\tvalid_1's binary_logloss: 0.23619\n",
      "[1600]\ttraining's auc: 0.880382\ttraining's binary_logloss: 0.200473\tvalid_1's auc: 0.790407\tvalid_1's binary_logloss: 0.236173\n",
      "[1650]\ttraining's auc: 0.88238\ttraining's binary_logloss: 0.199547\tvalid_1's auc: 0.79038\tvalid_1's binary_logloss: 0.236186\n",
      "[1700]\ttraining's auc: 0.884196\ttraining's binary_logloss: 0.198669\tvalid_1's auc: 0.790396\tvalid_1's binary_logloss: 0.236171\n",
      "[1750]\ttraining's auc: 0.886047\ttraining's binary_logloss: 0.197795\tvalid_1's auc: 0.790311\tvalid_1's binary_logloss: 0.236189\n",
      "Early stopping, best iteration is:\n",
      "[1590]\ttraining's auc: 0.880026\ttraining's binary_logloss: 0.200644\tvalid_1's auc: 0.790449\tvalid_1's binary_logloss: 0.236166\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's auc: 0.750386\ttraining's binary_logloss: 0.255528\tvalid_1's auc: 0.747468\tvalid_1's binary_logloss: 0.253663\n",
      "[100]\ttraining's auc: 0.770073\ttraining's binary_logloss: 0.246474\tvalid_1's auc: 0.760921\tvalid_1's binary_logloss: 0.246344\n",
      "[150]\ttraining's auc: 0.783708\ttraining's binary_logloss: 0.240941\tvalid_1's auc: 0.769952\tvalid_1's binary_logloss: 0.242404\n",
      "[200]\ttraining's auc: 0.793073\ttraining's binary_logloss: 0.237085\tvalid_1's auc: 0.775241\tvalid_1's binary_logloss: 0.240012\n",
      "[250]\ttraining's auc: 0.800207\ttraining's binary_logloss: 0.234101\tvalid_1's auc: 0.778656\tvalid_1's binary_logloss: 0.238449\n",
      "[300]\ttraining's auc: 0.806094\ttraining's binary_logloss: 0.231638\tvalid_1's auc: 0.781257\tvalid_1's binary_logloss: 0.237359\n",
      "[350]\ttraining's auc: 0.811151\ttraining's binary_logloss: 0.229548\tvalid_1's auc: 0.782921\tvalid_1's binary_logloss: 0.236662\n",
      "[400]\ttraining's auc: 0.815563\ttraining's binary_logloss: 0.227716\tvalid_1's auc: 0.784282\tvalid_1's binary_logloss: 0.23611\n",
      "[450]\ttraining's auc: 0.819662\ttraining's binary_logloss: 0.226041\tvalid_1's auc: 0.78515\tvalid_1's binary_logloss: 0.23576\n",
      "[500]\ttraining's auc: 0.823595\ttraining's binary_logloss: 0.224467\tvalid_1's auc: 0.785639\tvalid_1's binary_logloss: 0.235546\n",
      "[550]\ttraining's auc: 0.827286\ttraining's binary_logloss: 0.223033\tvalid_1's auc: 0.786509\tvalid_1's binary_logloss: 0.235251\n",
      "[600]\ttraining's auc: 0.830768\ttraining's binary_logloss: 0.221633\tvalid_1's auc: 0.787158\tvalid_1's binary_logloss: 0.235029\n",
      "[650]\ttraining's auc: 0.834094\ttraining's binary_logloss: 0.220307\tvalid_1's auc: 0.78764\tvalid_1's binary_logloss: 0.234867\n",
      "[700]\ttraining's auc: 0.837143\ttraining's binary_logloss: 0.219068\tvalid_1's auc: 0.787946\tvalid_1's binary_logloss: 0.234745\n",
      "[750]\ttraining's auc: 0.840042\ttraining's binary_logloss: 0.217889\tvalid_1's auc: 0.788201\tvalid_1's binary_logloss: 0.234641\n",
      "[800]\ttraining's auc: 0.84278\ttraining's binary_logloss: 0.216763\tvalid_1's auc: 0.788447\tvalid_1's binary_logloss: 0.234565\n",
      "[850]\ttraining's auc: 0.845535\ttraining's binary_logloss: 0.215642\tvalid_1's auc: 0.78856\tvalid_1's binary_logloss: 0.234532\n",
      "[900]\ttraining's auc: 0.848128\ttraining's binary_logloss: 0.214583\tvalid_1's auc: 0.788877\tvalid_1's binary_logloss: 0.234442\n",
      "[950]\ttraining's auc: 0.850713\ttraining's binary_logloss: 0.21352\tvalid_1's auc: 0.78916\tvalid_1's binary_logloss: 0.234358\n",
      "[1000]\ttraining's auc: 0.853176\ttraining's binary_logloss: 0.212489\tvalid_1's auc: 0.789363\tvalid_1's binary_logloss: 0.234301\n",
      "[1050]\ttraining's auc: 0.855637\ttraining's binary_logloss: 0.211466\tvalid_1's auc: 0.789576\tvalid_1's binary_logloss: 0.234257\n",
      "[1100]\ttraining's auc: 0.85795\ttraining's binary_logloss: 0.210493\tvalid_1's auc: 0.78966\tvalid_1's binary_logloss: 0.234212\n",
      "[1150]\ttraining's auc: 0.86035\ttraining's binary_logloss: 0.209458\tvalid_1's auc: 0.789717\tvalid_1's binary_logloss: 0.234181\n",
      "[1200]\ttraining's auc: 0.862716\ttraining's binary_logloss: 0.208478\tvalid_1's auc: 0.789828\tvalid_1's binary_logloss: 0.234157\n",
      "[1250]\ttraining's auc: 0.864937\ttraining's binary_logloss: 0.207511\tvalid_1's auc: 0.78984\tvalid_1's binary_logloss: 0.234159\n",
      "[1300]\ttraining's auc: 0.867164\ttraining's binary_logloss: 0.206542\tvalid_1's auc: 0.789768\tvalid_1's binary_logloss: 0.234162\n",
      "[1350]\ttraining's auc: 0.869355\ttraining's binary_logloss: 0.205569\tvalid_1's auc: 0.789904\tvalid_1's binary_logloss: 0.234114\n",
      "[1400]\ttraining's auc: 0.871513\ttraining's binary_logloss: 0.204604\tvalid_1's auc: 0.789939\tvalid_1's binary_logloss: 0.23411\n",
      "[1450]\ttraining's auc: 0.873445\ttraining's binary_logloss: 0.203747\tvalid_1's auc: 0.789895\tvalid_1's binary_logloss: 0.234128\n",
      "[1500]\ttraining's auc: 0.875475\ttraining's binary_logloss: 0.202849\tvalid_1's auc: 0.789758\tvalid_1's binary_logloss: 0.234155\n",
      "[1550]\ttraining's auc: 0.877554\ttraining's binary_logloss: 0.201919\tvalid_1's auc: 0.789786\tvalid_1's binary_logloss: 0.23417\n",
      "Early stopping, best iteration is:\n",
      "[1382]\ttraining's auc: 0.870734\ttraining's binary_logloss: 0.204948\tvalid_1's auc: 0.789972\tvalid_1's binary_logloss: 0.234095\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's auc: 0.750398\ttraining's binary_logloss: 0.255574\tvalid_1's auc: 0.749281\tvalid_1's binary_logloss: 0.253898\n",
      "[100]\ttraining's auc: 0.769869\ttraining's binary_logloss: 0.246529\tvalid_1's auc: 0.763407\tvalid_1's binary_logloss: 0.246107\n",
      "[150]\ttraining's auc: 0.783521\ttraining's binary_logloss: 0.241016\tvalid_1's auc: 0.772678\tvalid_1's binary_logloss: 0.241917\n",
      "[200]\ttraining's auc: 0.792551\ttraining's binary_logloss: 0.237233\tvalid_1's auc: 0.777605\tvalid_1's binary_logloss: 0.239506\n",
      "[250]\ttraining's auc: 0.799628\ttraining's binary_logloss: 0.234285\tvalid_1's auc: 0.781242\tvalid_1's binary_logloss: 0.237825\n",
      "[300]\ttraining's auc: 0.805541\ttraining's binary_logloss: 0.231849\tvalid_1's auc: 0.78395\tvalid_1's binary_logloss: 0.236627\n",
      "[350]\ttraining's auc: 0.810596\ttraining's binary_logloss: 0.22975\tvalid_1's auc: 0.785906\tvalid_1's binary_logloss: 0.235741\n",
      "[400]\ttraining's auc: 0.815187\ttraining's binary_logloss: 0.227906\tvalid_1's auc: 0.787611\tvalid_1's binary_logloss: 0.235028\n",
      "[450]\ttraining's auc: 0.819254\ttraining's binary_logloss: 0.226231\tvalid_1's auc: 0.789063\tvalid_1's binary_logloss: 0.234452\n",
      "[500]\ttraining's auc: 0.82305\ttraining's binary_logloss: 0.224721\tvalid_1's auc: 0.789944\tvalid_1's binary_logloss: 0.234076\n",
      "[550]\ttraining's auc: 0.826678\ttraining's binary_logloss: 0.223298\tvalid_1's auc: 0.790786\tvalid_1's binary_logloss: 0.233737\n",
      "[600]\ttraining's auc: 0.830148\ttraining's binary_logloss: 0.22193\tvalid_1's auc: 0.791533\tvalid_1's binary_logloss: 0.23343\n",
      "[650]\ttraining's auc: 0.833437\ttraining's binary_logloss: 0.220633\tvalid_1's auc: 0.792153\tvalid_1's binary_logloss: 0.233184\n",
      "[700]\ttraining's auc: 0.836695\ttraining's binary_logloss: 0.21936\tvalid_1's auc: 0.792888\tvalid_1's binary_logloss: 0.2329\n",
      "[750]\ttraining's auc: 0.839693\ttraining's binary_logloss: 0.218182\tvalid_1's auc: 0.793327\tvalid_1's binary_logloss: 0.232729\n",
      "[800]\ttraining's auc: 0.842677\ttraining's binary_logloss: 0.217015\tvalid_1's auc: 0.793731\tvalid_1's binary_logloss: 0.232576\n",
      "[850]\ttraining's auc: 0.845311\ttraining's binary_logloss: 0.215965\tvalid_1's auc: 0.793911\tvalid_1's binary_logloss: 0.232486\n",
      "[900]\ttraining's auc: 0.847997\ttraining's binary_logloss: 0.214879\tvalid_1's auc: 0.794178\tvalid_1's binary_logloss: 0.232399\n",
      "[950]\ttraining's auc: 0.850451\ttraining's binary_logloss: 0.213865\tvalid_1's auc: 0.794416\tvalid_1's binary_logloss: 0.232309\n",
      "[1000]\ttraining's auc: 0.853224\ttraining's binary_logloss: 0.212741\tvalid_1's auc: 0.794511\tvalid_1's binary_logloss: 0.23225\n",
      "[1050]\ttraining's auc: 0.855667\ttraining's binary_logloss: 0.21171\tvalid_1's auc: 0.794626\tvalid_1's binary_logloss: 0.232191\n",
      "[1100]\ttraining's auc: 0.858016\ttraining's binary_logloss: 0.210715\tvalid_1's auc: 0.794749\tvalid_1's binary_logloss: 0.232139\n",
      "[1150]\ttraining's auc: 0.860349\ttraining's binary_logloss: 0.209723\tvalid_1's auc: 0.794786\tvalid_1's binary_logloss: 0.23211\n",
      "[1200]\ttraining's auc: 0.862541\ttraining's binary_logloss: 0.208783\tvalid_1's auc: 0.794845\tvalid_1's binary_logloss: 0.232075\n",
      "[1250]\ttraining's auc: 0.864843\ttraining's binary_logloss: 0.207805\tvalid_1's auc: 0.794776\tvalid_1's binary_logloss: 0.232058\n",
      "[1300]\ttraining's auc: 0.867252\ttraining's binary_logloss: 0.206781\tvalid_1's auc: 0.79485\tvalid_1's binary_logloss: 0.232035\n",
      "[1350]\ttraining's auc: 0.869501\ttraining's binary_logloss: 0.20581\tvalid_1's auc: 0.794806\tvalid_1's binary_logloss: 0.232048\n",
      "[1400]\ttraining's auc: 0.871629\ttraining's binary_logloss: 0.204864\tvalid_1's auc: 0.794681\tvalid_1's binary_logloss: 0.232058\n",
      "[1450]\ttraining's auc: 0.873807\ttraining's binary_logloss: 0.203882\tvalid_1's auc: 0.794691\tvalid_1's binary_logloss: 0.232016\n",
      "Early stopping, best iteration is:\n",
      "[1291]\ttraining's auc: 0.866795\ttraining's binary_logloss: 0.206963\tvalid_1's auc: 0.794906\tvalid_1's binary_logloss: 0.232014\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's auc: 0.750335\ttraining's binary_logloss: 0.255522\tvalid_1's auc: 0.738013\tvalid_1's binary_logloss: 0.254178\n",
      "[100]\ttraining's auc: 0.770618\ttraining's binary_logloss: 0.246392\tvalid_1's auc: 0.755547\tvalid_1's binary_logloss: 0.246657\n",
      "[150]\ttraining's auc: 0.784007\ttraining's binary_logloss: 0.240892\tvalid_1's auc: 0.766222\tvalid_1's binary_logloss: 0.242601\n",
      "[200]\ttraining's auc: 0.793396\ttraining's binary_logloss: 0.23701\tvalid_1's auc: 0.772854\tvalid_1's binary_logloss: 0.240093\n",
      "[250]\ttraining's auc: 0.800389\ttraining's binary_logloss: 0.234066\tvalid_1's auc: 0.776671\tvalid_1's binary_logloss: 0.238594\n",
      "[300]\ttraining's auc: 0.806256\ttraining's binary_logloss: 0.231637\tvalid_1's auc: 0.779695\tvalid_1's binary_logloss: 0.237459\n",
      "[350]\ttraining's auc: 0.81116\ttraining's binary_logloss: 0.229595\tvalid_1's auc: 0.781746\tvalid_1's binary_logloss: 0.23669\n",
      "[400]\ttraining's auc: 0.815661\ttraining's binary_logloss: 0.22774\tvalid_1's auc: 0.783417\tvalid_1's binary_logloss: 0.236074\n",
      "[450]\ttraining's auc: 0.819781\ttraining's binary_logloss: 0.226048\tvalid_1's auc: 0.784803\tvalid_1's binary_logloss: 0.235618\n",
      "[500]\ttraining's auc: 0.823668\ttraining's binary_logloss: 0.224479\tvalid_1's auc: 0.785804\tvalid_1's binary_logloss: 0.235298\n",
      "[550]\ttraining's auc: 0.827258\ttraining's binary_logloss: 0.223057\tvalid_1's auc: 0.786495\tvalid_1's binary_logloss: 0.235059\n",
      "[600]\ttraining's auc: 0.830757\ttraining's binary_logloss: 0.221662\tvalid_1's auc: 0.787011\tvalid_1's binary_logloss: 0.234876\n",
      "[650]\ttraining's auc: 0.834129\ttraining's binary_logloss: 0.220329\tvalid_1's auc: 0.787484\tvalid_1's binary_logloss: 0.234705\n",
      "[700]\ttraining's auc: 0.837182\ttraining's binary_logloss: 0.219111\tvalid_1's auc: 0.78792\tvalid_1's binary_logloss: 0.23456\n",
      "[750]\ttraining's auc: 0.840151\ttraining's binary_logloss: 0.217946\tvalid_1's auc: 0.788352\tvalid_1's binary_logloss: 0.234399\n",
      "[800]\ttraining's auc: 0.843021\ttraining's binary_logloss: 0.216821\tvalid_1's auc: 0.788684\tvalid_1's binary_logloss: 0.234289\n",
      "[850]\ttraining's auc: 0.8458\ttraining's binary_logloss: 0.215689\tvalid_1's auc: 0.788906\tvalid_1's binary_logloss: 0.234199\n",
      "[900]\ttraining's auc: 0.848478\ttraining's binary_logloss: 0.2146\tvalid_1's auc: 0.789135\tvalid_1's binary_logloss: 0.234111\n",
      "[950]\ttraining's auc: 0.851202\ttraining's binary_logloss: 0.213493\tvalid_1's auc: 0.789359\tvalid_1's binary_logloss: 0.234023\n",
      "[1000]\ttraining's auc: 0.853715\ttraining's binary_logloss: 0.212473\tvalid_1's auc: 0.789499\tvalid_1's binary_logloss: 0.233971\n",
      "[1050]\ttraining's auc: 0.856189\ttraining's binary_logloss: 0.211441\tvalid_1's auc: 0.789764\tvalid_1's binary_logloss: 0.233884\n",
      "[1100]\ttraining's auc: 0.858746\ttraining's binary_logloss: 0.210403\tvalid_1's auc: 0.789711\tvalid_1's binary_logloss: 0.233874\n",
      "[1150]\ttraining's auc: 0.86112\ttraining's binary_logloss: 0.209393\tvalid_1's auc: 0.789899\tvalid_1's binary_logloss: 0.233819\n",
      "[1200]\ttraining's auc: 0.8634\ttraining's binary_logloss: 0.208409\tvalid_1's auc: 0.790035\tvalid_1's binary_logloss: 0.2338\n",
      "[1250]\ttraining's auc: 0.86572\ttraining's binary_logloss: 0.207412\tvalid_1's auc: 0.790077\tvalid_1's binary_logloss: 0.233763\n",
      "[1300]\ttraining's auc: 0.867954\ttraining's binary_logloss: 0.206433\tvalid_1's auc: 0.790229\tvalid_1's binary_logloss: 0.233705\n",
      "[1350]\ttraining's auc: 0.870109\ttraining's binary_logloss: 0.205511\tvalid_1's auc: 0.790266\tvalid_1's binary_logloss: 0.23369\n",
      "[1400]\ttraining's auc: 0.872146\ttraining's binary_logloss: 0.204603\tvalid_1's auc: 0.790309\tvalid_1's binary_logloss: 0.233682\n",
      "[1450]\ttraining's auc: 0.874231\ttraining's binary_logloss: 0.203654\tvalid_1's auc: 0.79034\tvalid_1's binary_logloss: 0.233657\n",
      "[1500]\ttraining's auc: 0.876336\ttraining's binary_logloss: 0.202727\tvalid_1's auc: 0.790285\tvalid_1's binary_logloss: 0.23368\n",
      "[1550]\ttraining's auc: 0.878319\ttraining's binary_logloss: 0.201812\tvalid_1's auc: 0.790357\tvalid_1's binary_logloss: 0.233659\n",
      "[1600]\ttraining's auc: 0.880244\ttraining's binary_logloss: 0.200918\tvalid_1's auc: 0.7903\tvalid_1's binary_logloss: 0.23365\n",
      "[1650]\ttraining's auc: 0.882176\ttraining's binary_logloss: 0.200003\tvalid_1's auc: 0.790346\tvalid_1's binary_logloss: 0.233646\n",
      "[1700]\ttraining's auc: 0.884124\ttraining's binary_logloss: 0.199118\tvalid_1's auc: 0.790366\tvalid_1's binary_logloss: 0.233642\n",
      "[1750]\ttraining's auc: 0.885928\ttraining's binary_logloss: 0.198251\tvalid_1's auc: 0.790447\tvalid_1's binary_logloss: 0.233642\n",
      "[1800]\ttraining's auc: 0.887794\ttraining's binary_logloss: 0.197375\tvalid_1's auc: 0.790281\tvalid_1's binary_logloss: 0.233685\n",
      "[1850]\ttraining's auc: 0.889597\ttraining's binary_logloss: 0.196508\tvalid_1's auc: 0.790347\tvalid_1's binary_logloss: 0.233663\n",
      "Early stopping, best iteration is:\n",
      "[1679]\ttraining's auc: 0.883335\ttraining's binary_logloss: 0.199481\tvalid_1's auc: 0.790478\tvalid_1's binary_logloss: 0.233602\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "kfoldlgb(k=10)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "clf = kfoldlgb(10)\n",
    "clf.fit(app_pre_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y = clf.predict_proba(app_pre_test)"
   ]
  },
  {
   "source": [
    "## Kaggle 输出 （用于上传） "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_pred = Y[:, 1]\n",
    "submit = app_test[['SK_ID_CURR']]\n",
    "submit['TARGET'] = log_reg_pred\n",
    "submit.to_csv('baseline.csv', index = False)"
   ]
  },
  {
   "source": [
    "## Pipeline Output"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['./preprocess_Lei.joblib']"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "joblib.dump(pipeline_Lei, './preprocess_Lei.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['./clf.joblib']"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "joblib.dump(clf, './clf.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('creditRisk': conda)",
   "metadata": {
    "interpreter": {
     "hash": "e1c47dc2f2d2db03ac4d3dd10e2c82551458f1f333f769c0a113c39f6515fc6b"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}